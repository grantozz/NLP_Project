{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 1/930911 words are not in dictionary, thus set UNK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grant/Desktop/school/nlp/env/lib/python3.6/site-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 1/930911 words are not in dictionary, thus set UNK\n",
      "tensor([[1, 2, 3, 4, 0],\n",
      "        [5, 2, 3, 0, 0]])\n",
      "tensor([[ 0.1793, -0.1286,  0.0964,  ..., -0.3654, -0.0070, -0.0919],\n",
      "        [ 0.1372, -0.1739, -0.0526,  ..., -0.1531,  0.0139, -0.1812]],\n",
      "       grad_fn=<CatBackward>)\n",
      "torch.Size([2, 4800])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys\n",
    "sys.path.append('skip-thoughts.torch/pytorch')\n",
    "from skipthoughts import UniSkip,BiSkip\n",
    "\n",
    "dir_st = 'data/skip-thoughts'\n",
    "vocab = ['robots', 'are', 'very', 'cool', '<eos>', 'BiDiBu']\n",
    "biskip = BiSkip(dir_st, vocab)\n",
    "uniskip = UniSkip(dir_st, vocab)\n",
    "\n",
    "input = Variable(torch.LongTensor([\n",
    "    [1,2,3,4,0], # robots are very cool 0\n",
    "    [5,2,3,0,0]  # bidibu are very cool <eos>\n",
    "])) # <eos> token is optional\n",
    "\n",
    "print(input)\n",
    "def generate_embeding(batch):\n",
    "    top_half = uniskip(batch)\n",
    "    bottom_half = biskip(batch)\n",
    "    combine_skip = torch.cat([top_half,bottom_half],dim=1) \n",
    "    return combine_skip\n",
    "print(generate_embeding(input))\n",
    "print(generate_embeding(input).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Vocabulary import Vocabulary, preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab(tokens):\n",
    "    voc = Vocabulary(['UNK'])\n",
    "    voc.add_tokens(tokens)\n",
    "    print('vocab len is {}'.format(len(voc.w2idx)))\n",
    "    return voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file='story_cloze_data/cloze_test_test__spring2016 - cloze_test_ALL_test.csv'):\n",
    "    '''TODO remove 10% of data for hyper param tuning'''\n",
    "    df= pd.read_csv(file)\n",
    "    df = df.drop('InputStoryid',axis=1)\n",
    "    targets = df['AnswerRightEnding']\n",
    "    df = df.drop('AnswerRightEnding',axis=1)\n",
    "    df = df.drop('InputSentence1',axis=1)\n",
    "    df = df.drop('InputSentence2',axis=1)\n",
    "    df = df.drop('InputSentence3',axis=1)\n",
    "    \n",
    "    voc_str= ''\n",
    "    for index, row in df.iterrows():\n",
    "        voc_str+=' '.join(list(row)) + ' '\n",
    "        \n",
    "    df['AnswerRightEnding'] = targets\n",
    "    return df, make_vocab(preprocess(voc_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len is 5225\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "df, voc = load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InputSentence4</th>\n",
       "      <th>RandomFifthSentenceQuiz1</th>\n",
       "      <th>RandomFifthSentenceQuiz2</th>\n",
       "      <th>AnswerRightEnding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I danced terribly and broke a friend's toe.</td>\n",
       "      <td>My friends decided to keep inviting me out as ...</td>\n",
       "      <td>The next weekend, I was asked to please stay h...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My eyes were watery and it was hard to breathe.</td>\n",
       "      <td>My allergies were too bad and I had to go back...</td>\n",
       "      <td>It reminded me of how much I loved spring flow...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>She made poor decisions that night and was unf...</td>\n",
       "      <td>Avery thought her children would be happy with...</td>\n",
       "      <td>Avery regretted what she did the next day.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The eggs his mom used must have been bad though.</td>\n",
       "      <td>Josh thought that the pie was delicious.</td>\n",
       "      <td>Josh got sick.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>He took a walk, hung out with some friends, an...</td>\n",
       "      <td>He felt inspiration and then went back home to...</td>\n",
       "      <td>John then got an idea for his painting.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      InputSentence4  \\\n",
       "0        I danced terribly and broke a friend's toe.   \n",
       "1    My eyes were watery and it was hard to breathe.   \n",
       "2  She made poor decisions that night and was unf...   \n",
       "3   The eggs his mom used must have been bad though.   \n",
       "4  He took a walk, hung out with some friends, an...   \n",
       "\n",
       "                            RandomFifthSentenceQuiz1  \\\n",
       "0  My friends decided to keep inviting me out as ...   \n",
       "1  My allergies were too bad and I had to go back...   \n",
       "2  Avery thought her children would be happy with...   \n",
       "3           Josh thought that the pie was delicious.   \n",
       "4  He felt inspiration and then went back home to...   \n",
       "\n",
       "                            RandomFifthSentenceQuiz2  AnswerRightEnding  \n",
       "0  The next weekend, I was asked to please stay h...                  2  \n",
       "1  It reminded me of how much I loved spring flow...                  1  \n",
       "2         Avery regretted what she did the next day.                  2  \n",
       "3                                     Josh got sick.                  2  \n",
       "4            John then got an idea for his painting.                  1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('My friends decided to keep inviting me out as I am so much fun.',)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['RandomFifthSentenceQuiz1'][0],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 11, 12, 13, 14, 15, 16, 17, 18, 1, 19, 20, 21, 22]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc.get_sentence(preprocess(df['RandomFifthSentenceQuiz1'][0]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "class LastSentenceDataset(Dataset):\n",
    "    '''currently implements no context model. will add in last sentence later'''\n",
    "    def __init__(self,file='story_cloze_data/cloze_test_test__spring2016 - cloze_test_ALL_test.csv',vocab=None):\n",
    "\n",
    "        super().__init__()\n",
    "        df, created_vocab = load_data(file)\n",
    "        \n",
    "        if vocab:\n",
    "            self.vocab = vocab\n",
    "        else:\n",
    "            self.vocab = created_vocab\n",
    "        self.df = df\n",
    "      \n",
    "        \n",
    "        self.dir_st = 'data/skip-thoughts'\n",
    "        self.biskip = BiSkip(self.dir_st, self.vocab.convert_to_list())\n",
    "        \n",
    "        self.uniskip = UniSkip(self.dir_st, self.vocab.convert_to_list())\n",
    "        \n",
    "       \n",
    "        self.data = self.make_data()\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx\n",
    "        Returns: skip thought embedding of ending and 0/1 if it is the right ending \n",
    "\n",
    "        \"\"\"\n",
    "        return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns len of the dataset\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "       \n",
    "    def make_data(self):\n",
    "        data = []\n",
    "        total = self.df.index\n",
    "        print('skip thought encoding dataset')\n",
    "        for i in total:\n",
    "            #print(row['RandomFifthSentenceQuiz1'],row['RandomFifthSentenceQuiz2'])\n",
    "           \n",
    "            progress(i,len(total))\n",
    "            endings =  self.gen_embbeding(self.df.at[i,'RandomFifthSentenceQuiz1'], self.df.at[i,'RandomFifthSentenceQuiz2'])\n",
    "            if self.df.at[i,'AnswerRightEnding'] == 1:\n",
    "                data.append((endings[0],1))\n",
    "                data.append((endings[1],0))\n",
    "            else:\n",
    "                data.append((endings[0],0))\n",
    "                data.append((endings[1],1))\n",
    "        return data\n",
    "    \n",
    "\n",
    "    def zero_pad(self,l,n):\n",
    "        l = (l + n * [0])[:n]\n",
    "        return l\n",
    "    \n",
    "    def pad_input(self,a,b):\n",
    "        ed = sorted([a,b],key=len)\n",
    "        longer = ed[1]\n",
    "        shorter = ed[0]\n",
    "        padded = self.zero_pad( shorter,len(longer))\n",
    "        if shorter == a:\n",
    "            return padded,b\n",
    "        else: return a,padded\n",
    "        \n",
    "    def gen_embbeding(self,sent1,sent2):\n",
    "        try:\n",
    "            sent1 = preprocess(sent1)\n",
    "            sent2 = preprocess(sent2)\n",
    "            #remove random n token that is in one sentence\n",
    "            if 'n' in sent2:\n",
    "                sent2.remove('n')\n",
    "            encoded_end1 = self.vocab.get_sentence(sent1)\n",
    "            encoded_end2 = self.vocab.get_sentence(sent2)\n",
    "        except KeyError as e:\n",
    "            print('key error',e)\n",
    "            print(sent1,sent2)\n",
    "        except TypeError as exc:\n",
    "            print('TYPE ERROR',exc)\n",
    "            print(sent1,sent2)\n",
    "        a,b = self.pad_input(encoded_end1,encoded_end2)\n",
    "        \n",
    "        batch = Variable(torch.LongTensor([a,b])) \n",
    "        top_half = self.uniskip(batch)\n",
    "        bottom_half = self.biskip(batch)\n",
    "        combine_skip = torch.cat([top_half,bottom_half],dim=1) \n",
    "        return combine_skip\n",
    "    \n",
    "def progress(count, total, status=''):\n",
    "    bar_len = 60\n",
    "    filled_len = int(round(bar_len * count / float(total)))\n",
    "\n",
    "    percents = round(100.0 * count / float(total), 1)\n",
    "    bar = '=' * filled_len + '-' * (bar_len - filled_len)\n",
    "\n",
    "    sys.stdout.write('[%s] %s%s ...%s\\r' % (bar, percents, '%', status))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len is 5225\n",
      "Warning: 47/930911 words are not in dictionary, thus set UNK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grant/Desktop/school/nlp/env/lib/python3.6/site-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 47/930911 words are not in dictionary, thus set UNK\n",
      "skip thought encoding dataset\n",
      "[============================================================] 99.9% ...\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([-0.1340, -0.2412,  0.1731,  ...,  0.0278, -0.0469,  0.0330],\n",
       "        grad_fn=<SelectBackward>), 0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the Dataset constructor or something has a memory leak \n",
    "#so repeated execution of this cell can use lots of memory fast. be carefull\n",
    "l = LastSentenceDataset()\n",
    "tensor =  l.gen_embbeding('My friends decided to have more fun on time','My friends decided to keep inviting me out as I am so much fun',)\n",
    "l[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0605, -0.1360,  0.0627,  ...,  0.0759,  0.0663,  0.0043],\n",
       "        [-0.1340, -0.2412,  0.1731,  ...,  0.0278, -0.0469,  0.0330]],\n",
       "       grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor =  l.gen_embbeding('My friends ate','My friends decided to keep inviting me out as I am so much fun')\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3742"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len is 5225\n",
      "Warning: 47/930911 words are not in dictionary, thus set UNK\n",
      "Warning: 47/930911 words are not in dictionary, thus set UNK\n",
      "skip thought encoding dataset\n",
      "[==================------------------------------------------] 30.0% ...\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-92e3b047fb2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_data_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLastSentenceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'story_cloze_data/cloze_test_test__spring2016 - cloze_test_ALL_test.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-9dedc95a40f2>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, vocab)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-9dedc95a40f2>\u001b[0m in \u001b[0;36mmake_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mprogress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mendings\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen_embbeding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'RandomFifthSentenceQuiz1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'RandomFifthSentenceQuiz2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'AnswerRightEnding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-9dedc95a40f2>\u001b[0m in \u001b[0;36mgen_embbeding\u001b[0;34m(self, sent1, sent2)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mtop_half\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniskip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mbottom_half\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiskip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mcombine_skip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtop_half\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbottom_half\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/school/nlp/env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/school/nlp/env/lib/python3.6/site-packages/skipthoughts/skipthoughts.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, lengths)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_lengths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# seq2seq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_select_last\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/school/nlp/env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/school/nlp/env/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             result = _impl(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 179\u001b[0;31m                            self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_data_set = LastSentenceDataset(file='story_cloze_data/cloze_test_test__spring2016 - cloze_test_ALL_test.csv',vocab=l.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
