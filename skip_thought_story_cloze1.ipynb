{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 1/930911 words are not in dictionary, thus set UNK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/cs/undergrad/2019/gnosborn/nlp/env/lib/python3.6/site-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 1/930911 words are not in dictionary, thus set UNK\n",
      "tensor([[1, 2, 3, 4, 0],\n",
      "        [5, 2, 3, 0, 0]])\n",
      "tensor([[ 0.1793, -0.1286,  0.0964,  ..., -0.3654, -0.0070, -0.0919],\n",
      "        [ 0.1372, -0.1739, -0.0526,  ..., -0.1531,  0.0139, -0.1812]],\n",
      "       grad_fn=<CatBackward>)\n",
      "torch.Size([2, 4800])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys\n",
    "sys.path.append('skip-thoughts.torch/pytorch')\n",
    "from skipthoughts import UniSkip,BiSkip\n",
    "\n",
    "dir_st = 'data/skip-thoughts'\n",
    "vocab = ['robots', 'are', 'very', 'cool', '<eos>', 'BiDiBu']\n",
    "biskip = BiSkip(dir_st, vocab)\n",
    "uniskip = UniSkip(dir_st, vocab)\n",
    "\n",
    "input = Variable(torch.LongTensor([\n",
    "    [1,2,3,4,0], # robots are very cool 0\n",
    "    [5,2,3,0,0]  # bidibu are very cool <eos>\n",
    "])) # <eos> token is optional\n",
    "\n",
    "print(input)\n",
    "def generate_embeding(batch):\n",
    "    top_half = uniskip(batch)\n",
    "    bottom_half = biskip(batch)\n",
    "    combine_skip = torch.cat([top_half,bottom_half],dim=1) \n",
    "    return combine_skip\n",
    "\n",
    "print(generate_embeding(input))\n",
    "print(generate_embeding(input).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Vocabulary import Vocabulary, preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab(tokens):\n",
    "    voc = Vocabulary(['<PAD>','<UNK>'])\n",
    "    voc.add_tokens(tokens)\n",
    "    print('vocab len is {}'.format(len(voc.w2idx)))\n",
    "    return voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file='story_cloze_data/cloze_test_test__spring2016 - cloze_test_ALL_test.csv'):\n",
    "    '''TODO remove 10% of data for hyper param tuning'''\n",
    "    df= pd.read_csv(file)\n",
    "    df = df.drop('InputStoryid',axis=1)\n",
    "    targets = df['AnswerRightEnding']\n",
    "    df = df.drop('AnswerRightEnding',axis=1)\n",
    "    df = df.drop('InputSentence1',axis=1)\n",
    "    df = df.drop('InputSentence2',axis=1)\n",
    "    df = df.drop('InputSentence3',axis=1)\n",
    "    \n",
    "    voc_str= ''\n",
    "    for index, row in df.iterrows():\n",
    "        voc_str+=' '.join(list(row)) + ' '\n",
    "        \n",
    "    df['AnswerRightEnding'] = targets\n",
    "    return df, make_vocab(preprocess(voc_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len is 5226\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "df, voc = load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InputSentence4</th>\n",
       "      <th>RandomFifthSentenceQuiz1</th>\n",
       "      <th>RandomFifthSentenceQuiz2</th>\n",
       "      <th>AnswerRightEnding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I danced terribly and broke a friend's toe.</td>\n",
       "      <td>My friends decided to keep inviting me out as ...</td>\n",
       "      <td>The next weekend, I was asked to please stay h...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My eyes were watery and it was hard to breathe.</td>\n",
       "      <td>My allergies were too bad and I had to go back...</td>\n",
       "      <td>It reminded me of how much I loved spring flow...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>She made poor decisions that night and was unf...</td>\n",
       "      <td>Avery thought her children would be happy with...</td>\n",
       "      <td>Avery regretted what she did the next day.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The eggs his mom used must have been bad though.</td>\n",
       "      <td>Josh thought that the pie was delicious.</td>\n",
       "      <td>Josh got sick.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>He took a walk, hung out with some friends, an...</td>\n",
       "      <td>He felt inspiration and then went back home to...</td>\n",
       "      <td>John then got an idea for his painting.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      InputSentence4  \\\n",
       "0        I danced terribly and broke a friend's toe.   \n",
       "1    My eyes were watery and it was hard to breathe.   \n",
       "2  She made poor decisions that night and was unf...   \n",
       "3   The eggs his mom used must have been bad though.   \n",
       "4  He took a walk, hung out with some friends, an...   \n",
       "\n",
       "                            RandomFifthSentenceQuiz1  \\\n",
       "0  My friends decided to keep inviting me out as ...   \n",
       "1  My allergies were too bad and I had to go back...   \n",
       "2  Avery thought her children would be happy with...   \n",
       "3           Josh thought that the pie was delicious.   \n",
       "4  He felt inspiration and then went back home to...   \n",
       "\n",
       "                            RandomFifthSentenceQuiz2  AnswerRightEnding  \n",
       "0  The next weekend, I was asked to please stay h...                  2  \n",
       "1  It reminded me of how much I loved spring flow...                  1  \n",
       "2         Avery regretted what she did the next day.                  2  \n",
       "3                                     Josh got sick.                  2  \n",
       "4            John then got an idea for his painting.                  1  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('My friends decided to keep inviting me out as I am so much fun.',)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['RandomFifthSentenceQuiz1'][0],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 12, 13, 14, 15, 16, 17, 18, 19, 2, 20, 21, 22, 23]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc.get_sentence(preprocess(df['RandomFifthSentenceQuiz1'][0]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "class LastSentenceDataset(Dataset):\n",
    "    '''currently implements no context model. will add in last sentence later'''\n",
    "    def __init__(self,file='story_cloze_data/cloze_test_test__spring2016 - cloze_test_ALL_test.csv',vocab=None):\n",
    "\n",
    "        super().__init__()\n",
    "        df, created_vocab = load_data(file)\n",
    "        \n",
    "        if vocab:\n",
    "            self.vocab = vocab\n",
    "        else:\n",
    "            self.vocab = created_vocab\n",
    "        #self.df = df\n",
    "      \n",
    "        \n",
    "        self.dir_st = 'data/skip-thoughts'\n",
    "        self.biskip = BiSkip(self.dir_st, self.vocab.convert_to_list())\n",
    "        \n",
    "        self.uniskip = UniSkip(self.dir_st, self.vocab.convert_to_list())\n",
    "        \n",
    "       \n",
    "        self.data = self.make_data(df)\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx\n",
    "        Returns: skip thought embedding of ending and 0/1 if it is the right ending \n",
    "\n",
    "        \"\"\"\n",
    "        return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns len of the dataset\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "       \n",
    "    def make_data(self, df):\n",
    "        data = []\n",
    "        total = df.index[:1000]\n",
    "        print('skip thought encoding dataset')\n",
    "        for i in total:\n",
    "            #print(row['RandomFifthSentenceQuiz1'],row['RandomFifthSentenceQuiz2'])\n",
    "           \n",
    "            progress(i,len(total))\n",
    "            endings =  self.gen_embbeding(df.at[i,'RandomFifthSentenceQuiz1'], df.at[i,'RandomFifthSentenceQuiz2'])\n",
    "            if df.at[i,'AnswerRightEnding'] == 1:\n",
    "                data.append((endings[0],1))\n",
    "                data.append((endings[1],0))\n",
    "            else:\n",
    "                data.append((endings[0],0))\n",
    "                data.append((endings[1],1))\n",
    "        return data\n",
    "    \n",
    "\n",
    "    def zero_pad(self,l,n):\n",
    "        l = (l + n * [0])[:n]\n",
    "        return l\n",
    "    \n",
    "    def pad_input(self,a,b):\n",
    "        ed = sorted([a,b],key=len)\n",
    "        longer = ed[1]\n",
    "        shorter = ed[0]\n",
    "        padded = self.zero_pad( shorter,len(longer))\n",
    "        if shorter == a:\n",
    "            return padded,b\n",
    "        else: return a,padded\n",
    "        \n",
    "    def gen_embbeding(self,sent1,sent2):\n",
    "        sent1 = preprocess(sent1)\n",
    "        sent2 = preprocess(sent2)\n",
    "        #remove random n token that is in one sentence\n",
    "        if 'n' in sent2:\n",
    "            sent2.remove('n')\n",
    "        encoded_end1 = self.vocab.get_sentence(sent1)\n",
    "        encoded_end2 = self.vocab.get_sentence(sent2)\n",
    "        a,b = self.pad_input(encoded_end1,encoded_end2)\n",
    "        \n",
    "        batch = torch.LongTensor([a,b]) \n",
    "        top_half = self.uniskip(batch)\n",
    "        bottom_half = self.biskip(batch)\n",
    "        combine_skip = torch.cat([top_half,bottom_half],dim=1) \n",
    "        return combine_skip    \n",
    "    \n",
    "def progress(count, total, status=''):\n",
    "    bar_len = 60\n",
    "    filled_len = int(round(bar_len * count / float(total)))\n",
    "\n",
    "    percents = round(100.0 * count / float(total), 1)\n",
    "    bar = '=' * filled_len + '-' * (bar_len - filled_len)\n",
    "\n",
    "    sys.stdout.write('[%s] %s%s ...%s\\r' % (bar, percents, '%', status))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoContextModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input = torch.nn.Linear(4800,256)\n",
    "        self.hidden= torch.nn.Linear(256,64)\n",
    "        self.output = torch.nn.Linear(64,2)\n",
    "    def forward(self, inputs):\n",
    "        hidden = torch.nn.functional.relu(self.input(inputs))\n",
    "        hidden1 = torch.nn.functional.relu(self.hidden(hidden))\n",
    "        output = self.output(hidden1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.hd1 = torch.nn.Linear(4800, 2400)\n",
    "        self.hd2 = torch.nn.Linear(2400, 1200)\n",
    "        self.hd3 = torch.nn.Linear(1200, 600)\n",
    "        self.output = torch.nn.Linear(600, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.hd1(x))\n",
    "        x = torch.nn.functional.relu(self.hd2(x))\n",
    "        x = torch.nn.functional.relu(self.hd3(x))\n",
    "        x = self.output(x)\n",
    "        #print('output',x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-paper')\n",
    "batch_size = 64\n",
    "lr = 0.01\n",
    "num_epochs = 5\n",
    "report_every = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len is 5226\n",
      "Warning: 49/930911 words are not in dictionary, thus set UNK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/cs/undergrad/2019/gnosborn/nlp/env/lib/python3.6/site-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 49/930911 words are not in dictionary, thus set UNK\n",
      "skip thought encoding dataset\n",
      "[============================================================] 99.8% ...\r"
     ]
    }
   ],
   "source": [
    "dataset = LastSentenceDataset()\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False,num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NoContextModel()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Loss 0.6920. Elapsed 26 seconds\n",
      "Epoch 1. Loss 0.8636. Elapsed 52 seconds\n",
      "Epoch 1. Loss 2.1122. Elapsed 77 seconds\n",
      "Epoch 1. Loss 1.7683. Elapsed 102 seconds\n",
      "Epoch 1. Loss 1.5619. Elapsed 126 seconds\n",
      "Epoch 1. Loss 1.4208. Elapsed 149 seconds\n",
      "Epoch 1. Loss 1.3193. Elapsed 173 seconds\n",
      "Epoch 1. Loss 1.2443. Elapsed 198 seconds\n",
      "Epoch 1. Loss 1.1813. Elapsed 222 seconds\n",
      "Epoch 1. Loss 1.1337. Elapsed 246 seconds\n",
      "Epoch 1. Loss 1.0929. Elapsed 269 seconds\n",
      "Epoch 1. Loss 1.0586. Elapsed 293 seconds\n",
      "Epoch 1. Loss 1.0299. Elapsed 317 seconds\n",
      "Epoch 1. Loss 1.0065. Elapsed 342 seconds\n",
      "Epoch 1. Loss 0.9864. Elapsed 367 seconds\n",
      "Epoch 1. Loss 0.9686. Elapsed 391 seconds\n",
      "Epoch 1. Loss 0.9536. Elapsed 417 seconds\n",
      "Epoch 1. Loss 0.9380. Elapsed 442 seconds\n",
      "Epoch 1. Loss 0.9260. Elapsed 466 seconds\n",
      "Epoch 1. Loss 0.9138. Elapsed 491 seconds\n",
      "Epoch 1. Loss 0.9038. Elapsed 515 seconds\n",
      "Epoch 1. Loss 0.8935. Elapsed 540 seconds\n",
      "Epoch 1. Loss 0.8852. Elapsed 566 seconds\n",
      "Epoch 1. Loss 0.8769. Elapsed 589 seconds\n",
      "Epoch 1. Loss 0.8701. Elapsed 615 seconds\n",
      "Epoch 1. Loss 0.8626. Elapsed 639 seconds\n",
      "Epoch 1. Loss 0.8566. Elapsed 662 seconds\n",
      "Epoch 1. Loss 0.8506. Elapsed 685 seconds\n",
      "Epoch 1. Loss 0.8449. Elapsed 709 seconds\n",
      "Epoch 1. Loss 0.8405. Elapsed 733 seconds\n",
      "Epoch 1. Loss 0.8362. Elapsed 758 seconds\n",
      "Epoch 1. Loss 0.8310. Elapsed 764 seconds\n",
      "Epoch 2. Loss 0.6609. Elapsed 786 seconds\n",
      "Epoch 2. Loss 0.6507. Elapsed 809 seconds\n",
      "Epoch 2. Loss 0.6462. Elapsed 835 seconds\n",
      "Epoch 2. Loss 0.6557. Elapsed 859 seconds\n",
      "Epoch 2. Loss 0.6518. Elapsed 883 seconds\n",
      "Epoch 2. Loss 0.6523. Elapsed 907 seconds\n",
      "Epoch 2. Loss 0.6522. Elapsed 929 seconds\n",
      "Epoch 2. Loss 0.6531. Elapsed 953 seconds\n",
      "Epoch 2. Loss 0.6525. Elapsed 978 seconds\n",
      "Epoch 2. Loss 0.6460. Elapsed 1000 seconds\n",
      "Epoch 2. Loss 0.6465. Elapsed 1025 seconds\n",
      "Epoch 2. Loss 0.6455. Elapsed 1049 seconds\n",
      "Epoch 2. Loss 0.6475. Elapsed 1073 seconds\n",
      "Epoch 2. Loss 0.6470. Elapsed 1098 seconds\n",
      "Epoch 2. Loss 0.6484. Elapsed 1124 seconds\n",
      "Epoch 2. Loss 0.6472. Elapsed 1149 seconds\n",
      "Epoch 2. Loss 0.6476. Elapsed 1174 seconds\n",
      "Epoch 2. Loss 0.6488. Elapsed 1199 seconds\n",
      "Epoch 2. Loss 0.6494. Elapsed 1224 seconds\n",
      "Epoch 2. Loss 0.6503. Elapsed 1248 seconds\n",
      "Epoch 2. Loss 0.6490. Elapsed 1274 seconds\n",
      "Epoch 2. Loss 0.6490. Elapsed 1300 seconds\n",
      "Epoch 2. Loss 0.6500. Elapsed 1325 seconds\n",
      "Epoch 2. Loss 0.6502. Elapsed 1349 seconds\n",
      "Epoch 2. Loss 0.6502. Elapsed 1373 seconds\n",
      "Epoch 2. Loss 0.6490. Elapsed 1399 seconds\n",
      "Epoch 2. Loss 0.6497. Elapsed 1424 seconds\n",
      "Epoch 2. Loss 0.6493. Elapsed 1448 seconds\n",
      "Epoch 2. Loss 0.6504. Elapsed 1472 seconds\n",
      "Epoch 2. Loss 0.6511. Elapsed 1496 seconds\n",
      "Epoch 2. Loss 0.6499. Elapsed 1519 seconds\n",
      "Epoch 2. Loss 0.6481. Elapsed 1526 seconds\n",
      "Epoch 3. Loss 0.5775. Elapsed 1551 seconds\n",
      "Epoch 3. Loss 0.5428. Elapsed 1576 seconds\n",
      "Epoch 3. Loss 0.5621. Elapsed 1600 seconds\n",
      "Epoch 3. Loss 0.5760. Elapsed 1625 seconds\n",
      "Epoch 3. Loss 0.5659. Elapsed 1650 seconds\n",
      "Epoch 3. Loss 0.5824. Elapsed 1674 seconds\n",
      "Epoch 3. Loss 0.5938. Elapsed 1699 seconds\n",
      "Epoch 3. Loss 0.6002. Elapsed 1725 seconds\n",
      "Epoch 3. Loss 0.5915. Elapsed 1750 seconds\n",
      "Epoch 3. Loss 0.5881. Elapsed 1774 seconds\n",
      "Epoch 3. Loss 0.5904. Elapsed 1798 seconds\n",
      "Epoch 3. Loss 0.5863. Elapsed 1822 seconds\n",
      "Epoch 3. Loss 0.5860. Elapsed 1846 seconds\n",
      "Epoch 3. Loss 0.5880. Elapsed 1870 seconds\n",
      "Epoch 3. Loss 0.5845. Elapsed 1895 seconds\n",
      "Epoch 3. Loss 0.5841. Elapsed 1919 seconds\n",
      "Epoch 3. Loss 0.5817. Elapsed 1942 seconds\n",
      "Epoch 3. Loss 0.5806. Elapsed 1967 seconds\n",
      "Epoch 3. Loss 0.5799. Elapsed 1990 seconds\n",
      "Epoch 3. Loss 0.5776. Elapsed 2013 seconds\n",
      "Epoch 3. Loss 0.5803. Elapsed 2038 seconds\n",
      "Epoch 3. Loss 0.5799. Elapsed 2061 seconds\n",
      "Epoch 3. Loss 0.5769. Elapsed 2086 seconds\n",
      "Epoch 3. Loss 0.5755. Elapsed 2110 seconds\n",
      "Epoch 3. Loss 0.5744. Elapsed 2136 seconds\n",
      "Epoch 3. Loss 0.5732. Elapsed 2161 seconds\n",
      "Epoch 3. Loss 0.5743. Elapsed 2185 seconds\n",
      "Epoch 3. Loss 0.5802. Elapsed 2209 seconds\n",
      "Epoch 3. Loss 0.5800. Elapsed 2235 seconds\n",
      "Epoch 3. Loss 0.5788. Elapsed 2260 seconds\n",
      "Epoch 3. Loss 0.5777. Elapsed 2285 seconds\n",
      "Epoch 3. Loss 0.5761. Elapsed 2290 seconds\n",
      "Epoch 4. Loss 0.4065. Elapsed 2315 seconds\n",
      "Epoch 4. Loss 0.4378. Elapsed 2341 seconds\n",
      "Epoch 4. Loss 0.4377. Elapsed 2365 seconds\n",
      "Epoch 4. Loss 0.4792. Elapsed 2391 seconds\n",
      "Epoch 4. Loss 0.4979. Elapsed 2415 seconds\n",
      "Epoch 4. Loss 0.4991. Elapsed 2440 seconds\n",
      "Epoch 4. Loss 0.4841. Elapsed 2463 seconds\n",
      "Epoch 4. Loss 0.4928. Elapsed 2487 seconds\n",
      "Epoch 4. Loss 0.4838. Elapsed 2512 seconds\n",
      "Epoch 4. Loss 0.4747. Elapsed 2536 seconds\n",
      "Epoch 4. Loss 0.4715. Elapsed 2562 seconds\n",
      "Epoch 4. Loss 0.4747. Elapsed 2587 seconds\n",
      "Epoch 4. Loss 0.4738. Elapsed 2611 seconds\n",
      "Epoch 4. Loss 0.4683. Elapsed 2636 seconds\n",
      "Epoch 4. Loss 0.4758. Elapsed 2660 seconds\n",
      "Epoch 4. Loss 0.4712. Elapsed 2683 seconds\n",
      "Epoch 4. Loss 0.4685. Elapsed 2709 seconds\n",
      "Epoch 4. Loss 0.4621. Elapsed 2732 seconds\n",
      "Epoch 4. Loss 0.4633. Elapsed 2755 seconds\n",
      "Epoch 4. Loss 0.4642. Elapsed 2780 seconds\n",
      "Epoch 4. Loss 0.4641. Elapsed 2803 seconds\n",
      "Epoch 4. Loss 0.4627. Elapsed 2827 seconds\n",
      "Epoch 4. Loss 0.4634. Elapsed 2853 seconds\n",
      "Epoch 4. Loss 0.4668. Elapsed 2877 seconds\n",
      "Epoch 4. Loss 0.4703. Elapsed 2901 seconds\n",
      "Epoch 4. Loss 0.4695. Elapsed 2925 seconds\n",
      "Epoch 4. Loss 0.4645. Elapsed 2951 seconds\n",
      "Epoch 4. Loss 0.4629. Elapsed 2975 seconds\n",
      "Epoch 4. Loss 0.4619. Elapsed 2999 seconds\n",
      "Epoch 4. Loss 0.4607. Elapsed 3024 seconds\n",
      "Epoch 4. Loss 0.4648. Elapsed 3050 seconds\n",
      "Epoch 4. Loss 0.4767. Elapsed 3056 seconds\n",
      "Epoch 5. Loss 1.1229. Elapsed 3081 seconds\n",
      "Epoch 5. Loss 0.7812. Elapsed 3106 seconds\n",
      "Epoch 5. Loss 0.6602. Elapsed 3129 seconds\n",
      "Epoch 5. Loss 0.5919. Elapsed 3152 seconds\n",
      "Epoch 5. Loss 0.5641. Elapsed 3176 seconds\n",
      "Epoch 5. Loss 0.5287. Elapsed 3201 seconds\n",
      "Epoch 5. Loss 0.5072. Elapsed 3225 seconds\n",
      "Epoch 5. Loss 0.4882. Elapsed 3250 seconds\n",
      "Epoch 5. Loss 0.4697. Elapsed 3273 seconds\n",
      "Epoch 5. Loss 0.4612. Elapsed 3297 seconds\n",
      "Epoch 5. Loss 0.4600. Elapsed 3321 seconds\n",
      "Epoch 5. Loss 0.4509. Elapsed 3345 seconds\n",
      "Epoch 5. Loss 0.4482. Elapsed 3369 seconds\n",
      "Epoch 5. Loss 0.4422. Elapsed 3394 seconds\n",
      "Epoch 5. Loss 0.4319. Elapsed 3419 seconds\n",
      "Epoch 5. Loss 0.4253. Elapsed 3444 seconds\n",
      "Epoch 5. Loss 0.4341. Elapsed 3469 seconds\n",
      "Epoch 5. Loss 0.4425. Elapsed 3493 seconds\n",
      "Epoch 5. Loss 0.4377. Elapsed 3519 seconds\n",
      "Epoch 5. Loss 0.4311. Elapsed 3543 seconds\n",
      "Epoch 5. Loss 0.4258. Elapsed 3567 seconds\n",
      "Epoch 5. Loss 0.4216. Elapsed 3593 seconds\n",
      "Epoch 5. Loss 0.4183. Elapsed 3618 seconds\n",
      "Epoch 5. Loss 0.4148. Elapsed 3643 seconds\n",
      "Epoch 5. Loss 0.4135. Elapsed 3668 seconds\n",
      "Epoch 5. Loss 0.4088. Elapsed 3694 seconds\n",
      "Epoch 5. Loss 0.4047. Elapsed 3716 seconds\n",
      "Epoch 5. Loss 0.3997. Elapsed 3740 seconds\n",
      "Epoch 5. Loss 0.4011. Elapsed 3765 seconds\n",
      "Epoch 5. Loss 0.3991. Elapsed 3788 seconds\n",
      "Epoch 5. Loss 0.3968. Elapsed 3813 seconds\n",
      "Epoch 5. Loss 0.3915. Elapsed 3819 seconds\n",
      "Epoch 6. Loss 0.3279. Elapsed 3843 seconds\n",
      "Epoch 6. Loss 0.2766. Elapsed 3866 seconds\n",
      "Epoch 6. Loss 0.2353. Elapsed 3891 seconds\n",
      "Epoch 6. Loss 0.2269. Elapsed 3914 seconds\n",
      "Epoch 6. Loss 0.2154. Elapsed 3939 seconds\n",
      "Epoch 6. Loss 0.2118. Elapsed 3963 seconds\n",
      "Epoch 6. Loss 0.2157. Elapsed 3987 seconds\n",
      "Epoch 6. Loss 0.2170. Elapsed 4009 seconds\n",
      "Epoch 6. Loss 0.2140. Elapsed 4035 seconds\n",
      "Epoch 6. Loss 0.2206. Elapsed 4058 seconds\n",
      "Epoch 6. Loss 0.2223. Elapsed 4081 seconds\n",
      "Epoch 6. Loss 0.2221. Elapsed 4106 seconds\n",
      "Epoch 6. Loss 0.2231. Elapsed 4131 seconds\n",
      "Epoch 6. Loss 0.2243. Elapsed 4154 seconds\n",
      "Epoch 6. Loss 0.2280. Elapsed 4179 seconds\n",
      "Epoch 6. Loss 0.2287. Elapsed 4203 seconds\n",
      "Epoch 6. Loss 0.2247. Elapsed 4228 seconds\n",
      "Epoch 6. Loss 0.2269. Elapsed 4254 seconds\n",
      "Epoch 6. Loss 0.2253. Elapsed 4278 seconds\n",
      "Epoch 6. Loss 0.2258. Elapsed 4304 seconds\n",
      "Epoch 6. Loss 0.2265. Elapsed 4328 seconds\n",
      "Epoch 6. Loss 0.2251. Elapsed 4353 seconds\n",
      "Epoch 6. Loss 0.2223. Elapsed 4377 seconds\n",
      "Epoch 6. Loss 0.2211. Elapsed 4401 seconds\n",
      "Epoch 6. Loss 0.2214. Elapsed 4425 seconds\n",
      "Epoch 6. Loss 0.2200. Elapsed 4451 seconds\n",
      "Epoch 6. Loss 0.2216. Elapsed 4475 seconds\n",
      "Epoch 6. Loss 0.2250. Elapsed 4501 seconds\n",
      "Epoch 6. Loss 0.2268. Elapsed 4525 seconds\n",
      "Epoch 6. Loss 0.2363. Elapsed 4550 seconds\n",
      "Epoch 6. Loss 0.2558. Elapsed 4574 seconds\n",
      "Epoch 6. Loss 0.2623. Elapsed 4580 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7. Loss 0.2619. Elapsed 4604 seconds\n",
      "Epoch 7. Loss 0.2598. Elapsed 4630 seconds\n",
      "Epoch 7. Loss 0.2367. Elapsed 4655 seconds\n",
      "Epoch 7. Loss 0.2232. Elapsed 4679 seconds\n",
      "Epoch 7. Loss 0.1988. Elapsed 4703 seconds\n",
      "Epoch 7. Loss 0.1917. Elapsed 4728 seconds\n",
      "Epoch 7. Loss 0.1883. Elapsed 4754 seconds\n",
      "Epoch 7. Loss 0.1840. Elapsed 4779 seconds\n",
      "Epoch 7. Loss 0.1774. Elapsed 4803 seconds\n",
      "Epoch 7. Loss 0.1788. Elapsed 4828 seconds\n",
      "Epoch 7. Loss 0.1815. Elapsed 4853 seconds\n",
      "Epoch 7. Loss 0.1803. Elapsed 4878 seconds\n",
      "Epoch 7. Loss 0.1759. Elapsed 4902 seconds\n",
      "Epoch 7. Loss 0.1740. Elapsed 4927 seconds\n",
      "Epoch 7. Loss 0.1682. Elapsed 4952 seconds\n",
      "Epoch 7. Loss 0.1624. Elapsed 4976 seconds\n",
      "Epoch 7. Loss 0.1609. Elapsed 5000 seconds\n",
      "Epoch 7. Loss 0.1589. Elapsed 5025 seconds\n",
      "Epoch 7. Loss 0.1567. Elapsed 5049 seconds\n",
      "Epoch 7. Loss 0.1537. Elapsed 5073 seconds\n",
      "Epoch 7. Loss 0.1515. Elapsed 5098 seconds\n",
      "Epoch 7. Loss 0.1507. Elapsed 5123 seconds\n",
      "Epoch 7. Loss 0.1508. Elapsed 5145 seconds\n",
      "Epoch 7. Loss 0.1489. Elapsed 5170 seconds\n",
      "Epoch 7. Loss 0.1501. Elapsed 5193 seconds\n",
      "Epoch 7. Loss 0.1485. Elapsed 5218 seconds\n",
      "Epoch 7. Loss 0.1472. Elapsed 5241 seconds\n",
      "Epoch 7. Loss 0.1465. Elapsed 5266 seconds\n",
      "Epoch 7. Loss 0.1477. Elapsed 5290 seconds\n",
      "Epoch 7. Loss 0.1469. Elapsed 5314 seconds\n",
      "Epoch 7. Loss 0.1465. Elapsed 5337 seconds\n",
      "Epoch 7. Loss 0.1450. Elapsed 5344 seconds\n",
      "Epoch 8. Loss 0.3134. Elapsed 5368 seconds\n",
      "Epoch 8. Loss 0.3710. Elapsed 5393 seconds\n",
      "Epoch 8. Loss 0.4408. Elapsed 5417 seconds\n",
      "Epoch 8. Loss 0.3684. Elapsed 5443 seconds\n",
      "Epoch 8. Loss 0.3164. Elapsed 5467 seconds\n",
      "Epoch 8. Loss 0.2799. Elapsed 5491 seconds\n",
      "Epoch 8. Loss 0.2496. Elapsed 5515 seconds\n",
      "Epoch 8. Loss 0.2285. Elapsed 5539 seconds\n",
      "Epoch 8. Loss 0.2128. Elapsed 5565 seconds\n",
      "Epoch 8. Loss 0.1983. Elapsed 5590 seconds\n",
      "Epoch 8. Loss 0.1862. Elapsed 5615 seconds\n",
      "Epoch 8. Loss 0.1743. Elapsed 5639 seconds\n",
      "Epoch 8. Loss 0.1727. Elapsed 5663 seconds\n",
      "Epoch 8. Loss 0.1667. Elapsed 5686 seconds\n",
      "Epoch 8. Loss 0.1641. Elapsed 5710 seconds\n",
      "Epoch 8. Loss 0.1601. Elapsed 5735 seconds\n",
      "Epoch 8. Loss 0.1556. Elapsed 5760 seconds\n",
      "Epoch 8. Loss 0.1525. Elapsed 5783 seconds\n",
      "Epoch 8. Loss 0.1474. Elapsed 5805 seconds\n",
      "Epoch 8. Loss 0.1452. Elapsed 5828 seconds\n",
      "Epoch 8. Loss 0.1414. Elapsed 5852 seconds\n",
      "Epoch 8. Loss 0.1380. Elapsed 5878 seconds\n",
      "Epoch 8. Loss 0.1402. Elapsed 5902 seconds\n",
      "Epoch 8. Loss 0.1378. Elapsed 5926 seconds\n",
      "Epoch 8. Loss 0.1354. Elapsed 5951 seconds\n",
      "Epoch 8. Loss 0.1340. Elapsed 5976 seconds\n",
      "Epoch 8. Loss 0.1318. Elapsed 6000 seconds\n",
      "Epoch 8. Loss 0.1294. Elapsed 6024 seconds\n",
      "Epoch 8. Loss 0.1274. Elapsed 6047 seconds\n",
      "Epoch 8. Loss 0.1272. Elapsed 6071 seconds\n",
      "Epoch 8. Loss 0.1249. Elapsed 6097 seconds\n",
      "Epoch 8. Loss 0.1226. Elapsed 6103 seconds\n",
      "Epoch 9. Loss 0.0369. Elapsed 6127 seconds\n",
      "Epoch 9. Loss 0.0480. Elapsed 6153 seconds\n",
      "Epoch 9. Loss 0.0501. Elapsed 6177 seconds\n",
      "Epoch 9. Loss 0.0533. Elapsed 6203 seconds\n",
      "Epoch 9. Loss 0.0520. Elapsed 6228 seconds\n",
      "Epoch 9. Loss 0.0494. Elapsed 6252 seconds\n",
      "Epoch 9. Loss 0.0476. Elapsed 6277 seconds\n",
      "Epoch 9. Loss 0.0460. Elapsed 6301 seconds\n",
      "Epoch 9. Loss 0.0473. Elapsed 6326 seconds\n",
      "Epoch 9. Loss 0.0461. Elapsed 6348 seconds\n",
      "Epoch 9. Loss 0.0460. Elapsed 6373 seconds\n",
      "Epoch 9. Loss 0.0450. Elapsed 6396 seconds\n",
      "Epoch 9. Loss 0.0430. Elapsed 6420 seconds\n",
      "Epoch 9. Loss 0.0432. Elapsed 6444 seconds\n",
      "Epoch 9. Loss 0.0421. Elapsed 6469 seconds\n",
      "Epoch 9. Loss 0.0428. Elapsed 6494 seconds\n",
      "Epoch 9. Loss 0.0445. Elapsed 6519 seconds\n",
      "Epoch 9. Loss 0.0452. Elapsed 6543 seconds\n",
      "Epoch 9. Loss 0.0445. Elapsed 6568 seconds\n",
      "Epoch 9. Loss 0.0436. Elapsed 6590 seconds\n",
      "Epoch 9. Loss 0.0433. Elapsed 6615 seconds\n",
      "Epoch 9. Loss 0.0429. Elapsed 6641 seconds\n",
      "Epoch 9. Loss 0.0443. Elapsed 6665 seconds\n",
      "Epoch 9. Loss 0.0452. Elapsed 6690 seconds\n",
      "Epoch 9. Loss 0.0467. Elapsed 6714 seconds\n",
      "Epoch 9. Loss 0.0483. Elapsed 6740 seconds\n",
      "Epoch 9. Loss 0.0479. Elapsed 6763 seconds\n",
      "Epoch 9. Loss 0.0474. Elapsed 6787 seconds\n",
      "Epoch 9. Loss 0.0468. Elapsed 6812 seconds\n",
      "Epoch 9. Loss 0.0478. Elapsed 6836 seconds\n",
      "Epoch 9. Loss 0.0515. Elapsed 6859 seconds\n",
      "Epoch 9. Loss 0.0522. Elapsed 6865 seconds\n",
      "Epoch 10. Loss 0.0291. Elapsed 6890 seconds\n",
      "Epoch 10. Loss 0.0302. Elapsed 6912 seconds\n",
      "Epoch 10. Loss 0.0290. Elapsed 6938 seconds\n",
      "Epoch 10. Loss 0.0397. Elapsed 6964 seconds\n",
      "Epoch 10. Loss 0.0399. Elapsed 6988 seconds\n",
      "Epoch 10. Loss 0.0382. Elapsed 7012 seconds\n",
      "Epoch 10. Loss 0.0356. Elapsed 7036 seconds\n",
      "Epoch 10. Loss 0.0341. Elapsed 7059 seconds\n",
      "Epoch 10. Loss 0.0333. Elapsed 7084 seconds\n",
      "Epoch 10. Loss 0.0321. Elapsed 7109 seconds\n",
      "Epoch 10. Loss 0.0312. Elapsed 7132 seconds\n",
      "Epoch 10. Loss 0.0308. Elapsed 7157 seconds\n",
      "Epoch 10. Loss 0.0298. Elapsed 7182 seconds\n",
      "Epoch 10. Loss 0.0297. Elapsed 7208 seconds\n",
      "Epoch 10. Loss 0.0325. Elapsed 7233 seconds\n",
      "Epoch 10. Loss 0.0322. Elapsed 7257 seconds\n",
      "Epoch 10. Loss 0.0318. Elapsed 7282 seconds\n",
      "Epoch 10. Loss 0.0329. Elapsed 7306 seconds\n",
      "Epoch 10. Loss 0.0328. Elapsed 7329 seconds\n",
      "Epoch 10. Loss 0.0322. Elapsed 7353 seconds\n",
      "Epoch 10. Loss 0.0316. Elapsed 7377 seconds\n",
      "Epoch 10. Loss 0.0311. Elapsed 7402 seconds\n",
      "Epoch 10. Loss 0.0311. Elapsed 7427 seconds\n",
      "Epoch 10. Loss 0.0308. Elapsed 7451 seconds\n",
      "Epoch 10. Loss 0.0321. Elapsed 7474 seconds\n",
      "Epoch 10. Loss 0.0331. Elapsed 7499 seconds\n",
      "Epoch 10. Loss 0.0331. Elapsed 7522 seconds\n",
      "Epoch 10. Loss 0.0335. Elapsed 7546 seconds\n",
      "Epoch 10. Loss 0.0336. Elapsed 7571 seconds\n",
      "Epoch 10. Loss 0.0335. Elapsed 7595 seconds\n",
      "Epoch 10. Loss 0.0332. Elapsed 7621 seconds\n",
      "Epoch 10. Loss 0.0325. Elapsed 7627 seconds\n",
      "Total time elapsed: 127 minutes\n"
     ]
    }
   ],
   "source": [
    "tick = time.time()\n",
    "epoch_losses = []\n",
    "for epoch_num in range(1, num_epochs + 1):\n",
    "    batch_losses = []\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        ### YOUR CODE BELOW ###\n",
    "        # Zero the gradients\n",
    "        #print('Zero Grad')\n",
    "        optimizer.zero_grad()\n",
    "        # Extract the inputs and the targets\n",
    "        #print('Extract')\n",
    "        inputs, targets = batch\n",
    "        #print(inputs.shape)\n",
    "        #print(inputs)\n",
    "        # Transfer the inputs and the targets to GPUs, if available\n",
    "        #print('2cuda start')\n",
    "        if torch.cuda.is_available():\n",
    "            #print(\"transfering to Cuda\")\n",
    "            inputs = torch.FloatTensor(inputs.float()).cuda()\n",
    "            targets = torch.LongTensor(targets).cuda()\n",
    "        #print('2cuda end')\n",
    "        #print(inputs.size())\n",
    "        # Run the model\n",
    "        #print('model')\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        #print('loss')\n",
    "        loss = criterion(outputs,targets)\n",
    "        \n",
    "        \n",
    "        # Backpropagate the error\n",
    "        #print('backprop')\n",
    "        loss.backward(retain_graph=True)\n",
    "        # Update the parameters\n",
    "        #print('step')\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Append the loss\n",
    "        #print('append loss')\n",
    "        batch_losses.append(float (loss))\n",
    "        ### YOUR CODE ABOVE ###\n",
    "        epoch_loss = np.mean(np.array(batch_losses))\n",
    "        epoch_losses.append(epoch_loss)\n",
    "\n",
    "        if epoch_num % report_every == 0:\n",
    "            tock = time.time()\n",
    "            print(\"Epoch {}. Loss {:.4f}. Elapsed {:.0f} seconds\".format(epoch_num, epoch_loss, tock-tick))\n",
    "\n",
    "print(\"Total time elapsed: {:.0f} minutes\".format((tock-tick)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(name):\n",
    "    torch.save(model.state_dict(), 'saved_models/{}.save'.format(name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(name):\n",
    "    l_model = NoContextModel()\n",
    "    l_model.load_state_dict(torch.load('saved_models/{}.save'.format(name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(pair):\n",
    "    '''true if model predicts right'''\n",
    "    inpt, target = pair\n",
    "    softm = torch.nn.Softmax(dim=0)\n",
    "    dist = softm(model(inpt.cuda()))\n",
    "    pred = dist.argmax()\n",
    "    #print(dist,target)\n",
    "    return pred.item() == target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len is 5303\n",
      "Warning: 49/930911 words are not in dictionary, thus set UNK\n",
      "Warning: 49/930911 words are not in dictionary, thus set UNK\n",
      "skip thought encoding dataset\n",
      "[============================================================] 99.8% ...\r"
     ]
    }
   ],
   "source": [
    "vocab = dataset.vocab\n",
    "val_data_set = LastSentenceDataset(file='story_cloze_data/cloze_test_val__spring2016 - cloze_test_ALL_val.csv',vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.575"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_right = 0\n",
    "for i in val_data_set:\n",
    "    if score(i):\n",
    "        num_right+=1\n",
    "num_right/len(val_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
