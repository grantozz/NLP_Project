{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys\n",
    "import numpy\n",
    "sys.path.append('skip-thoughts.torch/pytorch')\n",
    "from skipthoughts import UniSkip,BiSkip\n",
    "import pandas as pd\n",
    "from Vocabulary import Vocabulary, preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab(tokens):\n",
    "    voc = Vocabulary(['<PAD>','<UNK>'])\n",
    "    voc.add_tokens(tokens)\n",
    "    print('vocab len is {}'.format(len(voc.w2idx)))\n",
    "    return voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file='story_cloze_data/cloze_test_val__spring2016 - cloze_test_ALL_val.csv'):\n",
    "    df= pd.read_csv(file)\n",
    "    df=df.head(500)\n",
    "    df = df.drop('InputStoryid',axis=1)\n",
    "    targets = df['AnswerRightEnding']\n",
    "    df = df.drop('AnswerRightEnding',axis=1)\n",
    "    df = df.drop('InputSentence1',axis=1)\n",
    "    df = df.drop('InputSentence2',axis=1)\n",
    "    df = df.drop('InputSentence3',axis=1)\n",
    "    \n",
    "    voc_str= ''\n",
    "    for index, row in df.iterrows():\n",
    "        voc_str+=' '.join(list(row)) + ' '\n",
    "        \n",
    "    df['AnswerRightEnding'] = targets\n",
    "    return df,make_vocab(preprocess(voc_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import OrderedDict \n",
    "class LastSentenceDataset(Dataset):\n",
    "    def __init__(self,file='story_cloze_data/cloze_test_val__spring2016 - cloze_test_ALL_val.csv',vocab=None,df=None):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        created_df, created_vocab = load_data(file)\n",
    "        if df is None:\n",
    "            df = created_df\n",
    "        if vocab:\n",
    "            self.vocab = vocab\n",
    "        else:\n",
    "            self.vocab = created_vocab\n",
    "      \n",
    "        \n",
    "        self.dir_st = 'data/skip-thoughts'\n",
    "        self.biskip = BiSkip(self.dir_st, self.vocab.convert_to_list())\n",
    "        \n",
    "        self.uniskip = UniSkip(self.dir_st, self.vocab.convert_to_list())\n",
    "        \n",
    "        \n",
    "        self.data = self.make_data(df)\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx\n",
    "        Returns: skip thought embedding of ending and 0/1 if it is the right ending \n",
    "\n",
    "        \"\"\"\n",
    "        return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns len of the dataset\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "       \n",
    "    def make_data(self, df):\n",
    "        data = []\n",
    "        total = df.index\n",
    "        print('skip thought encoding dataset')\n",
    "        for i in total:\n",
    "            #print(row['RandomFifthSentenceQuiz1'],row['RandomFifthSentenceQuiz2'])\n",
    "            progress(i,len(total))\n",
    "            endings =  self.gen_embbeding(df.at[i,'RandomFifthSentenceQuiz1'], \n",
    "                                          df.at[i,'RandomFifthSentenceQuiz2'],\n",
    "                                          df.at[i,'InputSentence4'])\n",
    "            if df.at[i,'AnswerRightEnding'] == 1:\n",
    "                data.append((endings[0].detach().numpy(),1))\n",
    "                data.append((endings[1].detach().numpy(),0))\n",
    "            else:\n",
    "                data.append((endings[0].detach().numpy(),0))\n",
    "                data.append((endings[1].detach().numpy(),1))\n",
    "        return data\n",
    "    \n",
    "\n",
    "    def zero_pad(self,l,n):\n",
    "        l = (l + n * [0])[:n]\n",
    "        return l\n",
    "    \n",
    "    def pad_input(self,d):\n",
    "        d = OrderedDict(sorted(d.items(), key=lambda s: len(s[1])))\n",
    "        for k,v in d.items():\n",
    "            d[k]= self.zero_pad(v,len(list(d.items())[-1][1]))\n",
    "        return d\n",
    "        \n",
    "    def gen_embbeding(self,sent1,sent2,last_sent):\n",
    "        d = dict()\n",
    "        sent1 = preprocess(sent1)\n",
    "        sent2 = preprocess(sent2)\n",
    "        ls = preprocess(last_sent)\n",
    "        #remove random n token that is in one sentence\n",
    "        if 'n' in sent2:\n",
    "            sent2.remove('n')\n",
    "        d['sent1'] = self.vocab.get_sentence(sent1)\n",
    "        d['sent2'] = self.vocab.get_sentence(sent2)\n",
    "        d['ls'] = self.vocab.get_sentence(ls)\n",
    "        d = self.pad_input(d)\n",
    "        \n",
    "        batch = torch.LongTensor([d['sent1'],d['sent2'], d['ls']]) \n",
    "        top_half = self.uniskip(batch)\n",
    "        bottom_half = self.biskip(batch)\n",
    "        combine_skip = torch.cat([top_half,bottom_half],dim=1)\n",
    "        end1 = combine_skip[0]\n",
    "        end2 = combine_skip[1]\n",
    "        ls = combine_skip[2]\n",
    "        \n",
    "        #print(end1[:20],end2[:20])\n",
    "        end1.add_(ls)\n",
    "        end2.add_(ls)\n",
    "        #print('ls',ls[:20])\n",
    "        #print('after',end1[:20])\n",
    "        return end1,end2    \n",
    "    \n",
    "def progress(count, total, status=''):\n",
    "    bar_len = 60\n",
    "    filled_len = int(round(bar_len * count / float(total)))\n",
    "\n",
    "    percents = round(100.0 * count / float(total), 1)\n",
    "    bar = '=' * filled_len + '-' * (bar_len - filled_len)\n",
    "\n",
    "    sys.stdout.write('[%s] %s%s ...%s\\r' % (bar, percents, '%', status))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hd1 = torch.nn.Linear(4800, 2400)\n",
    "        self.hd2 = torch.nn.Linear(2400, 1200)\n",
    "        self.hd3 = torch.nn.Linear(1200, 600)\n",
    "        self.output = torch.nn.Linear(600, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.hd1(x))\n",
    "        x = torch.nn.functional.relu(self.hd2(x))\n",
    "        x = torch.nn.functional.relu(self.hd3(x))\n",
    "        x = self.output(x)\n",
    "        #print('output',x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(pair,model):\n",
    "    '''true if model predicts right'''\n",
    "    ending1, ending2 = pair\n",
    "    if ending1[1] == 1:\n",
    "        target = 1\n",
    "    else:\n",
    "        target =  2 \n",
    "        \n",
    "    ending1 = torch.tensor(ending1[0]).cuda()\n",
    "    ending2 = torch.tensor(ending2[0]).cuda()\n",
    "    res1 = model(ending1)\n",
    "    res2 = model(ending2)\n",
    "    softm = torch.nn.Softmax(dim=0)\n",
    "    prob_end1_right = softm(res1)[1].item() \n",
    "    prob_end2_right = softm(res2)[1].item()\n",
    "    \n",
    "    if prob_end1_right > prob_end2_right:\n",
    "        pred = 1\n",
    "    else:\n",
    "        pred = 2\n",
    "    \n",
    "    if pred == target:\n",
    "        return True\n",
    "    else: \n",
    "        #print(prob_end1_right,prob_end2_right,pred,target)\n",
    "        return False\n",
    "def score2(pair,clf):\n",
    "    '''true if model predicts right'''\n",
    "    ending1, ending2 = pair\n",
    "    if ending1[1] == 1:\n",
    "        target = 1\n",
    "    else:\n",
    "        target =  2 \n",
    "        \n",
    "    ending1 = ending1[0]\n",
    "    ending2 = ending2[0]\n",
    "    prob_end1_right = clf.predict_proba([ending1.detach().numpy()])[0][1]\n",
    "    prob_end2_right = clf.predict_proba([ending2.detach().numpy()])[0][1]\n",
    "    \n",
    "    if prob_end1_right > prob_end2_right:\n",
    "        pred = 1\n",
    "    else:\n",
    "        pred = 2\n",
    "    \n",
    "    if pred == target:\n",
    "        return True\n",
    "    else: \n",
    "        #print(prob_end1_right,prob_end2_right,pred,target)\n",
    "        return False\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(data_set,model):\n",
    "    num_right = 0\n",
    "    for i in range(0,len(data_set),2):\n",
    "        if score((data_set[i],data_set[i+1]),model):\n",
    "            num_right+=1\n",
    "    return num_right / (len(data_set)/2)\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(name):\n",
    "    torch.save(model.state_dict(), 'saved_models/{}.save'.format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len is 2522\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df, voc = load_data()\n",
    "train, val = train_test_split(df, test_size=0.1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-paper')\n",
    "batch_size = 128\n",
    "lr = 0.01\n",
    "num_epochs = 100\n",
    "report_every = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len is 2522\n",
      "Warning: 8/930911 words are not in dictionary, thus set UNK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/cs/undergrad/2019/gnosborn/nlp/env/lib/python3.6/site-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 8/930911 words are not in dictionary, thus set UNK\n",
      "skip thought encoding dataset\n",
      "[============================================================] 99.8% ...\r"
     ]
    }
   ],
   "source": [
    "train_data_set = LastSentenceDataset(df=train,vocab=voc)\n",
    "data_loader = torch.utils.data.DataLoader(train_data_set, batch_size=batch_size, shuffle=True,num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len is 2522\n",
      "Warning: 8/930911 words are not in dictionary, thus set UNK\n",
      "Warning: 8/930911 words are not in dictionary, thus set UNK\n",
      "skip thought encoding dataset\n",
      "[=======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] 998.0% ...\r"
     ]
    }
   ],
   "source": [
    "val_data_set = LastSentenceDataset(file='story_cloze_data/cloze_test_val__spring2016 - cloze_test_ALL_val.csv',df=val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3366, 4800)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.5, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSModel()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Loss 0.6908. accuracy 0.4200. Elapsed 0 seconds\n",
      "Epoch 1. Loss 126.6485. accuracy 0.4200. Elapsed 0 seconds\n",
      "Epoch 1. Loss 115.3008. accuracy 0.4200. Elapsed 0 seconds\n",
      "Epoch 1. Loss 90.0425. accuracy 0.4800. Elapsed 0 seconds\n",
      "new best saving model\n",
      "Epoch 1. Loss 72.1765. accuracy 0.5200. Elapsed 3 seconds\n",
      "Epoch 1. Loss 61.1259. accuracy 0.5000. Elapsed 3 seconds\n",
      "new best saving model\n",
      "Epoch 1. Loss 52.6199. accuracy 0.6000. Elapsed 6 seconds\n",
      "Epoch 1. Loss 46.1079. accuracy 0.5200. Elapsed 6 seconds\n",
      "saving model\n",
      "Epoch 2. Loss 1.4966. accuracy 0.4600. Elapsed 9 seconds\n",
      "Epoch 2. Loss 1.1161. accuracy 0.5400. Elapsed 9 seconds\n",
      "Epoch 2. Loss 1.1062. accuracy 0.5000. Elapsed 9 seconds\n",
      "Epoch 2. Loss 1.0116. accuracy 0.4600. Elapsed 10 seconds\n",
      "Epoch 2. Loss 0.9504. accuracy 0.4800. Elapsed 10 seconds\n",
      "Epoch 2. Loss 0.9612. accuracy 0.6000. Elapsed 10 seconds\n",
      "Epoch 2. Loss 0.9313. accuracy 0.5800. Elapsed 10 seconds\n",
      "Epoch 2. Loss 0.8972. accuracy 0.5200. Elapsed 10 seconds\n",
      "saving model\n",
      "Epoch 3. Loss 0.8739. accuracy 0.5600. Elapsed 13 seconds\n",
      "Epoch 3. Loss 0.7824. accuracy 0.4600. Elapsed 13 seconds\n",
      "Epoch 3. Loss 0.7505. accuracy 0.4800. Elapsed 13 seconds\n",
      "Epoch 3. Loss 0.7453. accuracy 0.5200. Elapsed 13 seconds\n",
      "Epoch 3. Loss 0.7524. accuracy 0.4800. Elapsed 13 seconds\n",
      "Epoch 3. Loss 0.7424. accuracy 0.4800. Elapsed 13 seconds\n",
      "Epoch 3. Loss 0.7355. accuracy 0.4400. Elapsed 13 seconds\n",
      "Epoch 3. Loss 0.7386. accuracy 0.5400. Elapsed 13 seconds\n",
      "saving model\n",
      "new best saving model\n",
      "Epoch 4. Loss 0.6881. accuracy 0.6200. Elapsed 19 seconds\n",
      "Epoch 4. Loss 0.6869. accuracy 0.5600. Elapsed 19 seconds\n",
      "Epoch 4. Loss 0.6885. accuracy 0.4800. Elapsed 19 seconds\n",
      "Epoch 4. Loss 0.6883. accuracy 0.5400. Elapsed 19 seconds\n",
      "Epoch 4. Loss 0.6893. accuracy 0.6200. Elapsed 19 seconds\n",
      "Epoch 4. Loss 0.6878. accuracy 0.4600. Elapsed 19 seconds\n",
      "Epoch 4. Loss 0.6867. accuracy 0.4800. Elapsed 19 seconds\n",
      "Epoch 4. Loss 0.6953. accuracy 0.4600. Elapsed 19 seconds\n",
      "saving model\n",
      "Epoch 5. Loss 0.6731. accuracy 0.5600. Elapsed 22 seconds\n",
      "Epoch 5. Loss 0.6844. accuracy 0.5600. Elapsed 22 seconds\n",
      "Epoch 5. Loss 0.6848. accuracy 0.5600. Elapsed 22 seconds\n",
      "Epoch 5. Loss 0.6814. accuracy 0.5400. Elapsed 22 seconds\n",
      "Epoch 5. Loss 0.6849. accuracy 0.5600. Elapsed 22 seconds\n",
      "Epoch 5. Loss 0.6837. accuracy 0.5600. Elapsed 22 seconds\n",
      "Epoch 5. Loss 0.6826. accuracy 0.5800. Elapsed 22 seconds\n",
      "Epoch 5. Loss 0.6974. accuracy 0.4800. Elapsed 22 seconds\n",
      "saving model\n",
      "Epoch 6. Loss 0.7184. accuracy 0.4600. Elapsed 25 seconds\n",
      "Epoch 6. Loss 0.6971. accuracy 0.5200. Elapsed 25 seconds\n",
      "Epoch 6. Loss 0.6926. accuracy 0.5200. Elapsed 25 seconds\n",
      "Epoch 6. Loss 0.6897. accuracy 0.5200. Elapsed 25 seconds\n",
      "Epoch 6. Loss 0.6865. accuracy 0.5600. Elapsed 25 seconds\n",
      "Epoch 6. Loss 0.6855. accuracy 0.5600. Elapsed 25 seconds\n",
      "Epoch 6. Loss 0.6850. accuracy 0.5000. Elapsed 25 seconds\n",
      "Epoch 6. Loss 0.6836. accuracy 0.4800. Elapsed 25 seconds\n",
      "saving model\n",
      "Epoch 7. Loss 0.6898. accuracy 0.6000. Elapsed 28 seconds\n",
      "Epoch 7. Loss 0.6778. accuracy 0.5800. Elapsed 28 seconds\n",
      "new best saving model\n",
      "Epoch 7. Loss 0.6643. accuracy 0.6400. Elapsed 31 seconds\n",
      "Epoch 7. Loss 0.6647. accuracy 0.4800. Elapsed 31 seconds\n",
      "Epoch 7. Loss 0.6608. accuracy 0.5400. Elapsed 31 seconds\n",
      "Epoch 7. Loss 0.6614. accuracy 0.5200. Elapsed 31 seconds\n",
      "Epoch 7. Loss 0.6614. accuracy 0.5800. Elapsed 31 seconds\n",
      "Epoch 7. Loss 0.6701. accuracy 0.5600. Elapsed 31 seconds\n",
      "saving model\n",
      "Epoch 8. Loss 0.7683. accuracy 0.5800. Elapsed 34 seconds\n",
      "Epoch 8. Loss 0.7130. accuracy 0.5200. Elapsed 34 seconds\n",
      "Epoch 8. Loss 0.6931. accuracy 0.5400. Elapsed 34 seconds\n",
      "Epoch 8. Loss 0.6816. accuracy 0.5200. Elapsed 34 seconds\n",
      "Epoch 8. Loss 0.6795. accuracy 0.5600. Elapsed 35 seconds\n",
      "Epoch 8. Loss 0.6735. accuracy 0.5400. Elapsed 35 seconds\n",
      "Epoch 8. Loss 0.6719. accuracy 0.5200. Elapsed 35 seconds\n",
      "Epoch 8. Loss 0.6711. accuracy 0.5600. Elapsed 35 seconds\n",
      "saving model\n",
      "Epoch 9. Loss 0.6179. accuracy 0.4600. Elapsed 38 seconds\n",
      "Epoch 9. Loss 0.6173. accuracy 0.6000. Elapsed 38 seconds\n",
      "Epoch 9. Loss 0.6131. accuracy 0.6000. Elapsed 38 seconds\n",
      "Epoch 9. Loss 0.6133. accuracy 0.6400. Elapsed 38 seconds\n",
      "Epoch 9. Loss 0.6124. accuracy 0.5400. Elapsed 38 seconds\n",
      "Epoch 9. Loss 0.6029. accuracy 0.5000. Elapsed 38 seconds\n",
      "Epoch 9. Loss 0.6076. accuracy 0.6200. Elapsed 38 seconds\n",
      "Epoch 9. Loss 0.6294. accuracy 0.5000. Elapsed 38 seconds\n",
      "saving model\n",
      "Epoch 10. Loss 0.8196. accuracy 0.5200. Elapsed 41 seconds\n",
      "Epoch 10. Loss 0.7325. accuracy 0.5200. Elapsed 41 seconds\n",
      "Epoch 10. Loss 0.6919. accuracy 0.4600. Elapsed 41 seconds\n",
      "Epoch 10. Loss 0.6770. accuracy 0.5600. Elapsed 41 seconds\n",
      "Epoch 10. Loss 0.6631. accuracy 0.5000. Elapsed 41 seconds\n",
      "Epoch 10. Loss 0.6553. accuracy 0.5600. Elapsed 41 seconds\n",
      "Epoch 10. Loss 0.6430. accuracy 0.5800. Elapsed 41 seconds\n",
      "Epoch 10. Loss 0.6362. accuracy 0.5000. Elapsed 41 seconds\n",
      "saving model\n",
      "Epoch 11. Loss 0.7855. accuracy 0.5000. Elapsed 44 seconds\n",
      "Epoch 11. Loss 0.6808. accuracy 0.4800. Elapsed 44 seconds\n",
      "Epoch 11. Loss 0.6428. accuracy 0.5400. Elapsed 44 seconds\n",
      "Epoch 11. Loss 0.6314. accuracy 0.5600. Elapsed 44 seconds\n",
      "Epoch 11. Loss 0.6079. accuracy 0.5600. Elapsed 44 seconds\n",
      "Epoch 11. Loss 0.5923. accuracy 0.5400. Elapsed 44 seconds\n",
      "Epoch 11. Loss 0.5886. accuracy 0.5800. Elapsed 44 seconds\n",
      "Epoch 11. Loss 0.6110. accuracy 0.4800. Elapsed 44 seconds\n",
      "saving model\n",
      "Epoch 12. Loss 0.6833. accuracy 0.5200. Elapsed 47 seconds\n",
      "Epoch 12. Loss 0.6173. accuracy 0.5200. Elapsed 47 seconds\n",
      "Epoch 12. Loss 0.5946. accuracy 0.5400. Elapsed 47 seconds\n",
      "Epoch 12. Loss 0.5690. accuracy 0.5600. Elapsed 47 seconds\n",
      "Epoch 12. Loss 0.5516. accuracy 0.4400. Elapsed 47 seconds\n",
      "Epoch 12. Loss 0.5651. accuracy 0.5400. Elapsed 47 seconds\n",
      "Epoch 12. Loss 0.5605. accuracy 0.5400. Elapsed 47 seconds\n",
      "Epoch 12. Loss 0.5743. accuracy 0.4800. Elapsed 47 seconds\n",
      "saving model\n",
      "Epoch 13. Loss 0.4842. accuracy 0.4800. Elapsed 50 seconds\n",
      "Epoch 13. Loss 0.4844. accuracy 0.5400. Elapsed 50 seconds\n",
      "Epoch 13. Loss 0.5001. accuracy 0.4600. Elapsed 50 seconds\n",
      "Epoch 13. Loss 0.4974. accuracy 0.5400. Elapsed 50 seconds\n",
      "Epoch 13. Loss 0.4944. accuracy 0.5200. Elapsed 50 seconds\n",
      "Epoch 13. Loss 0.4887. accuracy 0.4800. Elapsed 50 seconds\n",
      "Epoch 13. Loss 0.4922. accuracy 0.5600. Elapsed 50 seconds\n",
      "Epoch 13. Loss 0.5136. accuracy 0.4800. Elapsed 51 seconds\n",
      "saving model\n",
      "Epoch 14. Loss 0.9004. accuracy 0.5000. Elapsed 53 seconds\n",
      "Epoch 14. Loss 0.8827. accuracy 0.5400. Elapsed 53 seconds\n",
      "Epoch 14. Loss 0.7756. accuracy 0.5000. Elapsed 53 seconds\n",
      "Epoch 14. Loss 0.7174. accuracy 0.5000. Elapsed 53 seconds\n",
      "Epoch 14. Loss 0.6600. accuracy 0.4800. Elapsed 53 seconds\n",
      "Epoch 14. Loss 0.6342. accuracy 0.5000. Elapsed 54 seconds\n",
      "Epoch 14. Loss 0.6023. accuracy 0.5400. Elapsed 54 seconds\n",
      "Epoch 14. Loss 0.5531. accuracy 0.5400. Elapsed 54 seconds\n",
      "saving model\n",
      "Epoch 15. Loss 0.4407. accuracy 0.6200. Elapsed 57 seconds\n",
      "Epoch 15. Loss 0.5291. accuracy 0.5200. Elapsed 57 seconds\n",
      "Epoch 15. Loss 0.6675. accuracy 0.5000. Elapsed 57 seconds\n",
      "Epoch 15. Loss 0.6365. accuracy 0.5400. Elapsed 57 seconds\n",
      "Epoch 15. Loss 0.6088. accuracy 0.5800. Elapsed 57 seconds\n",
      "Epoch 15. Loss 0.5885. accuracy 0.6000. Elapsed 57 seconds\n",
      "Epoch 15. Loss 0.5677. accuracy 0.5400. Elapsed 57 seconds\n",
      "Epoch 15. Loss 0.5612. accuracy 0.5800. Elapsed 57 seconds\n",
      "saving model\n",
      "Epoch 16. Loss 0.3998. accuracy 0.5800. Elapsed 60 seconds\n",
      "Epoch 16. Loss 0.4411. accuracy 0.5400. Elapsed 60 seconds\n",
      "Epoch 16. Loss 0.5181. accuracy 0.6000. Elapsed 60 seconds\n",
      "Epoch 16. Loss 0.4871. accuracy 0.5600. Elapsed 60 seconds\n",
      "Epoch 16. Loss 0.4670. accuracy 0.5600. Elapsed 60 seconds\n",
      "Epoch 16. Loss 0.4500. accuracy 0.5400. Elapsed 60 seconds\n",
      "Epoch 16. Loss 0.4417. accuracy 0.5000. Elapsed 60 seconds\n",
      "Epoch 16. Loss 0.3955. accuracy 0.5400. Elapsed 60 seconds\n",
      "saving model\n",
      "Epoch 17. Loss 0.4295. accuracy 0.6200. Elapsed 63 seconds\n",
      "Epoch 17. Loss 0.5231. accuracy 0.6000. Elapsed 63 seconds\n",
      "Epoch 17. Loss 0.4655. accuracy 0.5600. Elapsed 63 seconds\n",
      "Epoch 17. Loss 0.4356. accuracy 0.5400. Elapsed 63 seconds\n",
      "Epoch 17. Loss 0.4009. accuracy 0.5600. Elapsed 63 seconds\n",
      "Epoch 17. Loss 0.3982. accuracy 0.5400. Elapsed 63 seconds\n",
      "Epoch 17. Loss 0.4093. accuracy 0.5400. Elapsed 63 seconds\n",
      "Epoch 17. Loss 0.4188. accuracy 0.6000. Elapsed 63 seconds\n",
      "saving model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18. Loss 0.3758. accuracy 0.5600. Elapsed 66 seconds\n",
      "Epoch 18. Loss 0.4184. accuracy 0.5600. Elapsed 66 seconds\n",
      "Epoch 18. Loss 0.4281. accuracy 0.6000. Elapsed 66 seconds\n",
      "Epoch 18. Loss 0.3917. accuracy 0.6000. Elapsed 66 seconds\n",
      "Epoch 18. Loss 0.3632. accuracy 0.6000. Elapsed 67 seconds\n",
      "Epoch 18. Loss 0.3374. accuracy 0.6000. Elapsed 67 seconds\n",
      "Epoch 18. Loss 0.3314. accuracy 0.5400. Elapsed 67 seconds\n",
      "Epoch 18. Loss 0.3296. accuracy 0.5600. Elapsed 67 seconds\n",
      "saving model\n",
      "Epoch 19. Loss 0.3014. accuracy 0.5800. Elapsed 70 seconds\n",
      "Epoch 19. Loss 0.3415. accuracy 0.6400. Elapsed 70 seconds\n",
      "Epoch 19. Loss 0.3293. accuracy 0.6200. Elapsed 70 seconds\n",
      "Epoch 19. Loss 0.3084. accuracy 0.6200. Elapsed 70 seconds\n",
      "Epoch 19. Loss 0.2889. accuracy 0.6400. Elapsed 70 seconds\n",
      "Epoch 19. Loss 0.2764. accuracy 0.6400. Elapsed 70 seconds\n",
      "Epoch 19. Loss 0.2712. accuracy 0.5800. Elapsed 70 seconds\n",
      "Epoch 19. Loss 0.2790. accuracy 0.5600. Elapsed 70 seconds\n",
      "saving model\n",
      "Epoch 20. Loss 0.3060. accuracy 0.5400. Elapsed 73 seconds\n",
      "Epoch 20. Loss 0.3525. accuracy 0.6000. Elapsed 73 seconds\n",
      "Epoch 20. Loss 0.3411. accuracy 0.6200. Elapsed 73 seconds\n",
      "Epoch 20. Loss 0.3171. accuracy 0.6000. Elapsed 73 seconds\n",
      "Epoch 20. Loss 0.2911. accuracy 0.5800. Elapsed 73 seconds\n",
      "Epoch 20. Loss 0.2694. accuracy 0.5800. Elapsed 73 seconds\n",
      "Epoch 20. Loss 0.2520. accuracy 0.5400. Elapsed 73 seconds\n",
      "Epoch 20. Loss 0.2281. accuracy 0.5000. Elapsed 73 seconds\n",
      "saving model\n",
      "Epoch 21. Loss 0.1322. accuracy 0.5200. Elapsed 76 seconds\n",
      "Epoch 21. Loss 0.1094. accuracy 0.5400. Elapsed 76 seconds\n",
      "Epoch 21. Loss 0.1064. accuracy 0.5000. Elapsed 76 seconds\n",
      "Epoch 21. Loss 0.1080. accuracy 0.4800. Elapsed 76 seconds\n",
      "Epoch 21. Loss 0.1022. accuracy 0.5200. Elapsed 76 seconds\n",
      "Epoch 21. Loss 0.1029. accuracy 0.5000. Elapsed 76 seconds\n",
      "Epoch 21. Loss 0.1072. accuracy 0.5400. Elapsed 76 seconds\n",
      "Epoch 21. Loss 0.0966. accuracy 0.5400. Elapsed 76 seconds\n",
      "saving model\n",
      "Epoch 22. Loss 0.0509. accuracy 0.5400. Elapsed 79 seconds\n",
      "Epoch 22. Loss 0.0530. accuracy 0.5600. Elapsed 79 seconds\n",
      "Epoch 22. Loss 0.0642. accuracy 0.5600. Elapsed 79 seconds\n",
      "Epoch 22. Loss 0.0665. accuracy 0.5800. Elapsed 79 seconds\n",
      "Epoch 22. Loss 0.0770. accuracy 0.5400. Elapsed 79 seconds\n",
      "Epoch 22. Loss 0.0738. accuracy 0.5400. Elapsed 80 seconds\n",
      "Epoch 22. Loss 0.0733. accuracy 0.5800. Elapsed 80 seconds\n",
      "Epoch 22. Loss 0.0642. accuracy 0.5800. Elapsed 80 seconds\n",
      "saving model\n",
      "Epoch 23. Loss 0.0200. accuracy 0.5600. Elapsed 82 seconds\n",
      "Epoch 23. Loss 0.0333. accuracy 0.5400. Elapsed 83 seconds\n",
      "Epoch 23. Loss 0.0327. accuracy 0.5600. Elapsed 83 seconds\n",
      "Epoch 23. Loss 0.0328. accuracy 0.5400. Elapsed 83 seconds\n",
      "Epoch 23. Loss 0.0375. accuracy 0.5400. Elapsed 83 seconds\n",
      "Epoch 23. Loss 0.0496. accuracy 0.6000. Elapsed 83 seconds\n",
      "Epoch 23. Loss 0.0562. accuracy 0.5400. Elapsed 83 seconds\n",
      "Epoch 23. Loss 0.0522. accuracy 0.5600. Elapsed 83 seconds\n",
      "saving model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-be5ab2f77dd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch {}. Loss {:.4f}. accuracy {:.4f}. Elapsed {:.0f} seconds\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtock\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtick\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'saving model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total time elapsed: {:.0f} minutes\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtock\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtick\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-8f166a72c2fd>\u001b[0m in \u001b[0;36msave\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'saved_models/{}.save'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/nlp/env/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \"\"\"\n\u001b[0;32m--> 219\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nlp/env/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[0;34m(f, mode, body)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nlp/env/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \"\"\"\n\u001b[0;32m--> 219\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nlp/env/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mserialized_storage_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mserialized_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_should_read_directly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tick = time.time()\n",
    "name='LS_adagrad_no_retrain'\n",
    "epoch_losses = []\n",
    "best_score= 0.5\n",
    "for epoch_num in range(1, num_epochs + 1):\n",
    "    batch_losses = []\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        #print(batch)\n",
    "        ### YOUR CODE BELOW ###\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Extract the inputs and the targets\n",
    "        inputs, targets = batch\n",
    "        # Transfer the inputs and the targets to GPUs, if available\n",
    "        if torch.cuda.is_available():\n",
    "            inputs =inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "        # Run the model\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs,targets)\n",
    "        \n",
    "        \n",
    "        # Backpropagate the error\n",
    "        loss.backward()\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Append the loss\n",
    "        batch_losses.append(float (loss))\n",
    "        ### YOUR CODE ABOVE ###\n",
    "        epoch_loss = np.mean(np.array(batch_losses))\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        \n",
    "        acc = compute_accuracy(val_data_set,model)\n",
    "        if acc > best_score:\n",
    "            best_score = acc\n",
    "            print('new best saving model')\n",
    "            save('{}{}_best_acc_{:.2f}'.format(name,epoch_num,acc))\n",
    "\n",
    "        if epoch_num % report_every == 0:\n",
    "            tock = time.time()\n",
    "            print(\"Epoch {}. Loss {:.4f}. accuracy {:.4f}. Elapsed {:.0f} seconds\".format(epoch_num, epoch_loss,acc, tock-tick))\n",
    "    print('saving model')\n",
    "    save(name + str(epoch_num))\n",
    "\n",
    "print(\"Total time elapsed: {:.0f} minutes\".format((tock-tick)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len is 5226\n",
      "Warning: 47/930911 words are not in dictionary, thus set UNK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/cs/undergrad/2019/gnosborn/nlp/env/lib/python3.6/site-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 47/930911 words are not in dictionary, thus set UNK\n",
      "skip thought encoding dataset\n",
      "[============================================================] 99.9% ...\r"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data_set = None\n",
    "test_data_set = LastSentenceDataset(file='story_cloze_data/cloze_test_test__spring2016 - cloze_test_ALL_test.csv',vocab=voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(name):\n",
    "    l_model = LSModel()\n",
    "    l_model.load_state_dict(torch.load('saved_models/{}.save'.format(name)))\n",
    "    return l_model\n",
    "#model = load('LS_adagrad5')\n",
    "#print('model',model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5793693212185996"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy(test_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
