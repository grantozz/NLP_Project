{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys\n",
    "import numpy\n",
    "sys.path.append('skip-thoughts.torch/pytorch')\n",
    "from skipthoughts import UniSkip,BiSkip\n",
    "import pandas as pd\n",
    "from Vocabulary import Vocabulary, preprocess\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab(tokens):\n",
    "    voc = Vocabulary(['<PAD>','<UNK>'])\n",
    "    voc.add_tokens(tokens)\n",
    "    print('vocab len is {}'.format(len(voc.w2idx)))\n",
    "    return voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file='story_cloze_data/cloze_test_val__spring2016 - cloze_test_ALL_val.csv'):\n",
    "    df= pd.read_csv(file)\n",
    "    df = df.drop('InputStoryid',axis=1)\n",
    "    targets = df['AnswerRightEnding']\n",
    "    df = df.drop('AnswerRightEnding',axis=1)\n",
    "    df = df.drop('InputSentence1',axis=1)\n",
    "    df = df.drop('InputSentence2',axis=1)\n",
    "    df = df.drop('InputSentence3',axis=1)\n",
    "    \n",
    "    voc_str= ''\n",
    "    for index, row in df.iterrows():\n",
    "        voc_str+=' '.join(list(row)) + ' '\n",
    "        \n",
    "    df['AnswerRightEnding'] = targets\n",
    "    return df,make_vocab(preprocess(voc_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_train(file='story_cloze_data/ROCStories__spring2016 - ROCStories_spring2016.csv',random_seed=None):\n",
    "    np.random.RandomState(random_seed)\n",
    "    df = pd.read_csv(file)\n",
    "    df = df.drop('storyid',axis=1)\n",
    "    df = df.drop('storytitle',axis=1)\n",
    "    df = df.drop('sentence1',axis=1)\n",
    "    df = df.drop('sentence2',axis=1)\n",
    "    df = df.drop('sentence3',axis=1)\n",
    "    target = df['sentence5']\n",
    "    df = df.drop('sentence5',axis=1)\n",
    "    \n",
    "    df = df.rename({'sentence4':'InputSentence4'},axis=1)\n",
    "    answer1=[]\n",
    "    answer2=[]\n",
    "    trueanswer=[]\n",
    "    for i in range(len(target)):\n",
    "        k= np.random.randint(0,len(target))\n",
    "        while k == i:\n",
    "            k= np.random.randint(0,len(target))\n",
    "        if np.random.random()<.5:\n",
    "            answer1.append(target[i])\n",
    "            answer2.append(target[k])\n",
    "            trueanswer.append(1)\n",
    "        else:\n",
    "            answer1.append(target[k])\n",
    "            answer2.append(target[i])\n",
    "            trueanswer.append(2)\n",
    "    \n",
    "    df1 = pd.DataFrame({'RandomFifthSentenceQuiz1': answer1,'RandomFifthSentenceQuiz2': answer2})\n",
    "    df = df.join(df1)\n",
    "    voc_str= ''\n",
    "    for index, row in df.iterrows():\n",
    "        voc_str+=' '.join(list(row)) + ' '\n",
    "    df['AnswerRightEnding'] = trueanswer\n",
    "    return df,make_vocab(preprocess(voc_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import OrderedDict \n",
    "class LastSentenceDataset(Dataset):\n",
    "    def __init__(self,file='story_cloze_data/cloze_test_val__spring2016 - cloze_test_ALL_val.csv',vocab=None,df=None,train=None):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        created_df, created_vocab = load_data(file)\n",
    "        if df is None:\n",
    "            df = created_df\n",
    "        if vocab:\n",
    "            self.vocab = vocab\n",
    "        else:\n",
    "            self.vocab = created_vocab\n",
    "      \n",
    "        \n",
    "        self.dir_st = 'data/skip-thoughts'\n",
    "        self.biskip = BiSkip(self.dir_st, self.vocab.convert_to_list()[1:])\n",
    "        \n",
    "        self.uniskip = UniSkip(self.dir_st, self.vocab.convert_to_list()[1:])\n",
    "        \n",
    "        \n",
    "        self.data = self.make_data(df)\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx\n",
    "        Returns: skip thought embedding of ending and 0/1 if it is the right ending \n",
    "\n",
    "        \"\"\"\n",
    "        return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns len of the dataset\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "       \n",
    "    def make_data(self, df):\n",
    "        data = []\n",
    "        total = df.index\n",
    "        print('skip thought encoding dataset')\n",
    "        for i in total:\n",
    "            #print(row['RandomFifthSentenceQuiz1'],row['RandomFifthSentenceQuiz2'])\n",
    "            progress(i,len(total))\n",
    "            endings =  self.gen_embbeding(df.at[i,'RandomFifthSentenceQuiz1'], \n",
    "                                          df.at[i,'RandomFifthSentenceQuiz2'],\n",
    "                                          df.at[i,'InputSentence4'])\n",
    "            if df.at[i,'AnswerRightEnding'] == 1:\n",
    "                data.append((endings[0].detach().numpy(),1))\n",
    "                data.append((endings[1].detach().numpy(),0))\n",
    "            else:\n",
    "                data.append((endings[0].detach().numpy(),0))\n",
    "                data.append((endings[1].detach().numpy(),1))\n",
    "        return data\n",
    "    \n",
    "\n",
    "    def zero_pad(self,l,n):\n",
    "        l = (l + n * [0])[:n]\n",
    "        return l\n",
    "    \n",
    "    def pad_input(self,d):\n",
    "        d = OrderedDict(sorted(d.items(), key=lambda s: len(s[1])))\n",
    "        for k,v in d.items():\n",
    "            d[k]= self.zero_pad(v,len(list(d.items())[-1][1]))\n",
    "        return d\n",
    "        \n",
    "    def gen_embbeding(self,sent1,sent2,last_sent):\n",
    "        d = dict()\n",
    "        sent1 = preprocess(sent1)\n",
    "        sent2 = preprocess(sent2)\n",
    "        ls = preprocess(last_sent)\n",
    "        #remove random n token that is in one sentence\n",
    "        if 'n' in sent2:\n",
    "            sent2.remove('n')\n",
    "        d['sent1'] = self.vocab.get_sentence(sent1)\n",
    "        d['sent2'] = self.vocab.get_sentence(sent2)\n",
    "        d['ls'] = self.vocab.get_sentence(ls)\n",
    "        d = self.pad_input(d)\n",
    "        \n",
    "        batch = torch.LongTensor([d['sent1'],d['sent2'], d['ls']]) \n",
    "        top_half = self.uniskip(batch)\n",
    "        bottom_half = self.biskip(batch)\n",
    "        combine_skip = torch.cat([top_half,bottom_half],dim=1)\n",
    "        end1 = combine_skip[0]\n",
    "        end2 = combine_skip[1]\n",
    "        ls = combine_skip[2]\n",
    "        \n",
    "        #print(end1[:20],end2[:20])\n",
    "        end1.add_(ls)\n",
    "        end2.add_(ls)\n",
    "        #print('ls',ls[:20])\n",
    "        #print('after',end1[:20])\n",
    "        return end1,end2    \n",
    "    \n",
    "def progress(count, total, status=''):\n",
    "    bar_len = 60\n",
    "    filled_len = int(round(bar_len * count / float(total)))\n",
    "\n",
    "    percents = round(100.0 * count / float(total), 1)\n",
    "    bar = '=' * filled_len + '-' * (bar_len - filled_len)\n",
    "\n",
    "    sys.stdout.write('[%s] %s%s ...%s\\r' % (bar, percents, '%', status))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hd1 = torch.nn.Linear(4800, 2400)\n",
    "        self.hd2 = torch.nn.Linear(2400, 1200)\n",
    "        self.hd3 = torch.nn.Linear(1200, 600)\n",
    "        self.output = torch.nn.Linear(600, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.hd1(x))\n",
    "        x = torch.nn.functional.relu(self.hd2(x))\n",
    "        x = torch.nn.functional.relu(self.hd3(x))\n",
    "        x = self.output(x)\n",
    "        #print('output',x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSShortModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input = torch.nn.Linear(4800,256)\n",
    "        self.hidden= torch.nn.Linear(256,64)\n",
    "        self.output = torch.nn.Linear(64,2)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        hidden = torch.nn.functional.relu(self.input(inputs))\n",
    "        hidden1 = torch.nn.functional.relu(self.hidden(hidden))\n",
    "        output = self.output(hidden1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(pair,model):\n",
    "    '''true if model predicts right'''\n",
    "    ending1, ending2 = pair\n",
    "    if ending1[1] == 1:\n",
    "        target = 1\n",
    "    else:\n",
    "        target =  2 \n",
    "    if torch.cuda.is_available():\n",
    "        ending1 = torch.tensor(ending1[0]).cuda()\n",
    "        ending2 = torch.tensor(ending2[0]).cuda()\n",
    "    else:\n",
    "        ending1 = torch.tensor(ending1[0])\n",
    "        ending2 = torch.tensor(ending2[0])\n",
    "    res1 = model(ending1)\n",
    "    res2 = model(ending2)\n",
    "    softm = torch.nn.Softmax(dim=0)\n",
    "    prob_end1_right = softm(res1)[1].item() \n",
    "    prob_end2_right = softm(res2)[1].item()\n",
    "    \n",
    "    if prob_end1_right > prob_end2_right:\n",
    "        pred = 1\n",
    "    else:\n",
    "        pred = 2\n",
    "    \n",
    "    if pred == target:\n",
    "        return True\n",
    "    else: \n",
    "        #print(prob_end1_right,prob_end2_right,pred,target)\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(data_set,model):\n",
    "    num_right = 0\n",
    "    for i in range(0,len(data_set),2):\n",
    "        if score((data_set[i],data_set[i+1]),model):\n",
    "            num_right+=1\n",
    "    return num_right / (len(data_set)/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(name):\n",
    "    torch.save(model.state_dict(), 'saved_models/{}.save'.format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len is 4851\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df, voc = load_data(file='story_cloze_data/cloze_test_val__winter2018-cloze_test_ALL_val - 1 - 1.csv')\n",
    "train, val = train_test_split(df, test_size=0.1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment to load training data\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#df, voc = load_data_train()\n",
    "#train, val = train_test_split(df, test_size=0.1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "lr = 0.01\n",
    "num_epochs = 7\n",
    "report_every = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len is 5303\n",
      "Warning: 38/930911 words are not in dictionary, thus set UNK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grant/Desktop/school/nlp/env/lib/python3.6/site-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 38/930911 words are not in dictionary, thus set UNK\n",
      "skip thought encoding dataset\n",
      "[============================================================] 99.9% ...\r"
     ]
    }
   ],
   "source": [
    "train_data_set = LastSentenceDataset(df=train, vocab=voc)\n",
    "data_loader = torch.utils.data.DataLoader(train_data_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2  out of 33769 ids generated were for unk(0.006 %)'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc.unk_ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len is 5303\n",
      "Warning: 38/930911 words are not in dictionary, thus set UNK\n",
      "Warning: 38/930911 words are not in dictionary, thus set UNK\n",
      "skip thought encoding dataset\n",
      "[====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] 993.7% ...\r"
     ]
    }
   ],
   "source": [
    "val_data_set = LastSentenceDataset(df=val,vocab=voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSModel()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Loss 0.6930. accuracy 0.5823. Elapsed 1 seconds\n",
      "Epoch 1. Loss 21.8699. accuracy 0.5063. Elapsed 3 seconds\n",
      "Epoch 1. Loss 104.6657. accuracy 0.4620. Elapsed 4 seconds\n",
      "Epoch 1. Loss 80.8051. accuracy 0.5886. Elapsed 5 seconds\n",
      "Epoch 1. Loss 65.0139. accuracy 0.3924. Elapsed 6 seconds\n",
      "Epoch 1. Loss 54.3510. accuracy 0.5063. Elapsed 7 seconds\n",
      "Epoch 1. Loss 46.7884. accuracy 0.5316. Elapsed 9 seconds\n",
      "Epoch 1. Loss 41.0415. accuracy 0.4430. Elapsed 10 seconds\n",
      "Epoch 1. Loss 36.5722. accuracy 0.4304. Elapsed 11 seconds\n",
      "Epoch 1. Loss 33.0038. accuracy 0.5127. Elapsed 12 seconds\n",
      "Epoch 1. Loss 30.0670. accuracy 0.5443. Elapsed 13 seconds\n",
      "Epoch 1. Loss 27.6195. accuracy 0.5253. Elapsed 14 seconds\n",
      "Epoch 1. Loss 25.5475. accuracy 0.4873. Elapsed 16 seconds\n",
      "Epoch 1. Loss 23.7790. accuracy 0.5633. Elapsed 17 seconds\n",
      "Epoch 1. Loss 22.2457. accuracy 0.6266. Elapsed 18 seconds\n",
      "Epoch 1. Loss 20.8984. accuracy 0.5886. Elapsed 19 seconds\n",
      "Epoch 1. Loss 19.7088. accuracy 0.5380. Elapsed 20 seconds\n",
      "Epoch 1. Loss 18.6570. accuracy 0.6519. Elapsed 22 seconds\n",
      "Epoch 1. Loss 17.7143. accuracy 0.6456. Elapsed 23 seconds\n",
      "Epoch 1. Loss 16.8624. accuracy 0.6013. Elapsed 24 seconds\n",
      "Epoch 1. Loss 16.0939. accuracy 0.6013. Elapsed 25 seconds\n",
      "Epoch 1. Loss 15.3938. accuracy 0.5823. Elapsed 26 seconds\n",
      "Epoch 1. Loss 14.7541. accuracy 0.5759. Elapsed 27 seconds\n",
      "ep acc 0.5429279031370392\n",
      "new best saving model\n",
      "Epoch 2. Loss 0.6936. accuracy 0.6076. Elapsed 29 seconds\n",
      "Epoch 2. Loss 0.6945. accuracy 0.6582. Elapsed 30 seconds\n",
      "Epoch 2. Loss 0.6955. accuracy 0.6392. Elapsed 31 seconds\n",
      "Epoch 2. Loss 0.6977. accuracy 0.6203. Elapsed 32 seconds\n",
      "Epoch 2. Loss 0.6927. accuracy 0.5949. Elapsed 33 seconds\n",
      "Epoch 2. Loss 0.6948. accuracy 0.5949. Elapsed 34 seconds\n",
      "Epoch 2. Loss 0.7010. accuracy 0.6392. Elapsed 35 seconds\n",
      "Epoch 2. Loss 0.7452. accuracy 0.6519. Elapsed 37 seconds\n",
      "Epoch 2. Loss 0.7369. accuracy 0.6203. Elapsed 38 seconds\n",
      "Epoch 2. Loss 0.7382. accuracy 0.5823. Elapsed 39 seconds\n",
      "Epoch 2. Loss 0.7449. accuracy 0.6013. Elapsed 40 seconds\n",
      "Epoch 2. Loss 0.7380. accuracy 0.6076. Elapsed 41 seconds\n",
      "Epoch 2. Loss 0.7391. accuracy 0.5949. Elapsed 42 seconds\n",
      "Epoch 2. Loss 0.7387. accuracy 0.6266. Elapsed 44 seconds\n",
      "Epoch 2. Loss 0.7341. accuracy 0.6899. Elapsed 45 seconds\n",
      "Epoch 2. Loss 0.7322. accuracy 0.6392. Elapsed 46 seconds\n",
      "Epoch 2. Loss 0.7315. accuracy 0.6709. Elapsed 47 seconds\n",
      "Epoch 2. Loss 0.7301. accuracy 0.6392. Elapsed 48 seconds\n",
      "Epoch 2. Loss 0.7302. accuracy 0.6139. Elapsed 49 seconds\n",
      "Epoch 2. Loss 0.7292. accuracy 0.6266. Elapsed 50 seconds\n",
      "Epoch 2. Loss 0.7331. accuracy 0.6519. Elapsed 52 seconds\n",
      "Epoch 2. Loss 0.7304. accuracy 0.6772. Elapsed 53 seconds\n",
      "Epoch 2. Loss 0.7310. accuracy 0.7342. Elapsed 54 seconds\n",
      "ep acc 0.5884700055035773\n",
      "new best saving model\n",
      "Epoch 3. Loss 0.7049. accuracy 0.7152. Elapsed 55 seconds\n",
      "Epoch 3. Loss 0.6892. accuracy 0.7278. Elapsed 56 seconds\n",
      "Epoch 3. Loss 0.6842. accuracy 0.7595. Elapsed 57 seconds\n",
      "Epoch 3. Loss 0.6718. accuracy 0.7152. Elapsed 59 seconds\n",
      "Epoch 3. Loss 0.6671. accuracy 0.7089. Elapsed 60 seconds\n",
      "Epoch 3. Loss 0.6693. accuracy 0.7152. Elapsed 61 seconds\n",
      "Epoch 3. Loss 0.6648. accuracy 0.6772. Elapsed 62 seconds\n",
      "Epoch 3. Loss 0.6619. accuracy 0.7025. Elapsed 63 seconds\n",
      "Epoch 3. Loss 0.6625. accuracy 0.7215. Elapsed 65 seconds\n",
      "Epoch 3. Loss 0.6654. accuracy 0.6835. Elapsed 66 seconds\n",
      "Epoch 3. Loss 0.6640. accuracy 0.6772. Elapsed 67 seconds\n",
      "Epoch 3. Loss 0.6670. accuracy 0.6646. Elapsed 68 seconds\n",
      "Epoch 3. Loss 0.6707. accuracy 0.6709. Elapsed 69 seconds\n",
      "Epoch 3. Loss 0.6709. accuracy 0.6772. Elapsed 70 seconds\n",
      "Epoch 3. Loss 0.6711. accuracy 0.7152. Elapsed 71 seconds\n",
      "Epoch 3. Loss 0.6721. accuracy 0.6962. Elapsed 73 seconds\n",
      "Epoch 3. Loss 0.6731. accuracy 0.7025. Elapsed 74 seconds\n",
      "Epoch 3. Loss 0.6770. accuracy 0.7215. Elapsed 75 seconds\n",
      "Epoch 3. Loss 0.6761. accuracy 0.7025. Elapsed 76 seconds\n",
      "Epoch 3. Loss 0.6767. accuracy 0.7342. Elapsed 77 seconds\n",
      "Epoch 3. Loss 0.6756. accuracy 0.7468. Elapsed 78 seconds\n",
      "Epoch 3. Loss 0.6746. accuracy 0.7468. Elapsed 80 seconds\n",
      "Epoch 3. Loss 0.6696. accuracy 0.7342. Elapsed 81 seconds\n",
      "ep acc 0.6287837094111173\n",
      "new best saving model\n",
      "Epoch 4. Loss 0.6339. accuracy 0.7278. Elapsed 82 seconds\n",
      "Epoch 4. Loss 0.6359. accuracy 0.7152. Elapsed 83 seconds\n",
      "Epoch 4. Loss 0.6410. accuracy 0.7152. Elapsed 84 seconds\n",
      "Epoch 4. Loss 0.6402. accuracy 0.7405. Elapsed 85 seconds\n",
      "Epoch 4. Loss 0.6382. accuracy 0.7468. Elapsed 87 seconds\n",
      "Epoch 4. Loss 0.6353. accuracy 0.7532. Elapsed 88 seconds\n",
      "Epoch 4. Loss 0.6384. accuracy 0.7658. Elapsed 89 seconds\n",
      "Epoch 4. Loss 0.6325. accuracy 0.7405. Elapsed 90 seconds\n",
      "Epoch 4. Loss 0.6287. accuracy 0.7215. Elapsed 91 seconds\n",
      "Epoch 4. Loss 0.6264. accuracy 0.7532. Elapsed 92 seconds\n",
      "Epoch 4. Loss 0.6268. accuracy 0.7278. Elapsed 94 seconds\n",
      "Epoch 4. Loss 0.6278. accuracy 0.7405. Elapsed 95 seconds\n",
      "Epoch 4. Loss 0.6262. accuracy 0.7532. Elapsed 96 seconds\n",
      "Epoch 4. Loss 0.6236. accuracy 0.7468. Elapsed 97 seconds\n",
      "Epoch 4. Loss 0.6232. accuracy 0.7342. Elapsed 99 seconds\n",
      "Epoch 4. Loss 0.6229. accuracy 0.7278. Elapsed 100 seconds\n",
      "Epoch 4. Loss 0.6237. accuracy 0.6709. Elapsed 101 seconds\n",
      "Epoch 4. Loss 0.6237. accuracy 0.6646. Elapsed 102 seconds\n",
      "Epoch 4. Loss 0.6218. accuracy 0.6456. Elapsed 103 seconds\n",
      "Epoch 4. Loss 0.6219. accuracy 0.7278. Elapsed 104 seconds\n",
      "Epoch 4. Loss 0.6245. accuracy 0.7089. Elapsed 106 seconds\n",
      "Epoch 4. Loss 0.6280. accuracy 0.7215. Elapsed 107 seconds\n",
      "Epoch 4. Loss 0.6191. accuracy 0.7595. Elapsed 108 seconds\n",
      "ep acc 0.6532058337919647\n",
      "new best saving model\n",
      "Epoch 5. Loss 0.5848. accuracy 0.7342. Elapsed 109 seconds\n",
      "Epoch 5. Loss 0.5816. accuracy 0.7468. Elapsed 110 seconds\n",
      "Epoch 5. Loss 0.5800. accuracy 0.7342. Elapsed 111 seconds\n",
      "Epoch 5. Loss 0.5794. accuracy 0.7278. Elapsed 113 seconds\n",
      "Epoch 5. Loss 0.5774. accuracy 0.7025. Elapsed 114 seconds\n",
      "Epoch 5. Loss 0.5681. accuracy 0.7278. Elapsed 115 seconds\n",
      "Epoch 5. Loss 0.5682. accuracy 0.6962. Elapsed 116 seconds\n",
      "Epoch 5. Loss 0.5700. accuracy 0.6962. Elapsed 117 seconds\n",
      "Epoch 5. Loss 0.5753. accuracy 0.6709. Elapsed 118 seconds\n",
      "Epoch 5. Loss 0.5827. accuracy 0.6772. Elapsed 120 seconds\n",
      "Epoch 5. Loss 0.5807. accuracy 0.7089. Elapsed 121 seconds\n",
      "Epoch 5. Loss 0.5799. accuracy 0.7278. Elapsed 122 seconds\n",
      "Epoch 5. Loss 0.5878. accuracy 0.7595. Elapsed 123 seconds\n",
      "Epoch 5. Loss 0.5913. accuracy 0.7785. Elapsed 124 seconds\n",
      "Epoch 5. Loss 0.5890. accuracy 0.7848. Elapsed 125 seconds\n",
      "Epoch 5. Loss 0.5860. accuracy 0.7911. Elapsed 127 seconds\n",
      "Epoch 5. Loss 0.5852. accuracy 0.7911. Elapsed 128 seconds\n",
      "Epoch 5. Loss 0.5878. accuracy 0.7722. Elapsed 129 seconds\n",
      "Epoch 5. Loss 0.5846. accuracy 0.7595. Elapsed 130 seconds\n",
      "Epoch 5. Loss 0.5839. accuracy 0.7532. Elapsed 131 seconds\n",
      "Epoch 5. Loss 0.5870. accuracy 0.7595. Elapsed 132 seconds\n",
      "Epoch 5. Loss 0.5883. accuracy 0.7468. Elapsed 133 seconds\n",
      "Epoch 5. Loss 0.5938. accuracy 0.7405. Elapsed 135 seconds\n",
      "ep acc 0.6702806824435884\n",
      "new best saving model\n",
      "Epoch 6. Loss 0.6039. accuracy 0.7468. Elapsed 136 seconds\n",
      "Epoch 6. Loss 0.5919. accuracy 0.7722. Elapsed 137 seconds\n",
      "Epoch 6. Loss 0.5762. accuracy 0.7215. Elapsed 138 seconds\n",
      "Epoch 6. Loss 0.5799. accuracy 0.7278. Elapsed 139 seconds\n",
      "Epoch 6. Loss 0.5777. accuracy 0.7468. Elapsed 140 seconds\n",
      "Epoch 6. Loss 0.5813. accuracy 0.7532. Elapsed 142 seconds\n",
      "Epoch 6. Loss 0.5792. accuracy 0.7532. Elapsed 143 seconds\n",
      "Epoch 6. Loss 0.5781. accuracy 0.7722. Elapsed 144 seconds\n",
      "Epoch 6. Loss 0.5675. accuracy 0.7468. Elapsed 145 seconds\n",
      "Epoch 6. Loss 0.5609. accuracy 0.7468. Elapsed 146 seconds\n",
      "Epoch 6. Loss 0.5542. accuracy 0.7532. Elapsed 147 seconds\n",
      "Epoch 6. Loss 0.5546. accuracy 0.7025. Elapsed 148 seconds\n",
      "Epoch 6. Loss 0.5590. accuracy 0.7025. Elapsed 150 seconds\n",
      "Epoch 6. Loss 0.5604. accuracy 0.7152. Elapsed 151 seconds\n",
      "Epoch 6. Loss 0.5624. accuracy 0.7342. Elapsed 152 seconds\n",
      "Epoch 6. Loss 0.5638. accuracy 0.7468. Elapsed 153 seconds\n",
      "Epoch 6. Loss 0.5568. accuracy 0.7405. Elapsed 154 seconds\n",
      "Epoch 6. Loss 0.5574. accuracy 0.7468. Elapsed 155 seconds\n",
      "Epoch 6. Loss 0.5562. accuracy 0.7658. Elapsed 156 seconds\n",
      "Epoch 6. Loss 0.5617. accuracy 0.7405. Elapsed 158 seconds\n",
      "Epoch 6. Loss 0.5629. accuracy 0.7405. Elapsed 159 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6. Loss 0.5607. accuracy 0.7405. Elapsed 160 seconds\n",
      "Epoch 6. Loss 0.5612. accuracy 0.6962. Elapsed 161 seconds\n",
      "ep acc 0.6818473674555128\n",
      "new best saving model\n",
      "Epoch 7. Loss 0.5263. accuracy 0.7025. Elapsed 162 seconds\n",
      "Epoch 7. Loss 0.5380. accuracy 0.7089. Elapsed 163 seconds\n",
      "Epoch 7. Loss 0.5334. accuracy 0.7342. Elapsed 165 seconds\n",
      "Epoch 7. Loss 0.5256. accuracy 0.7152. Elapsed 166 seconds\n",
      "Epoch 7. Loss 0.5480. accuracy 0.7025. Elapsed 167 seconds\n",
      "Epoch 7. Loss 0.5476. accuracy 0.6962. Elapsed 168 seconds\n",
      "Epoch 7. Loss 0.5538. accuracy 0.7342. Elapsed 169 seconds\n",
      "Epoch 7. Loss 0.5460. accuracy 0.7342. Elapsed 170 seconds\n",
      "Epoch 7. Loss 0.5389. accuracy 0.7722. Elapsed 172 seconds\n",
      "Epoch 7. Loss 0.5425. accuracy 0.7785. Elapsed 173 seconds\n",
      "Epoch 7. Loss 0.5384. accuracy 0.7658. Elapsed 174 seconds\n",
      "Epoch 7. Loss 0.5394. accuracy 0.7658. Elapsed 175 seconds\n",
      "Epoch 7. Loss 0.5384. accuracy 0.7342. Elapsed 176 seconds\n",
      "Epoch 7. Loss 0.5376. accuracy 0.6962. Elapsed 177 seconds\n",
      "Epoch 7. Loss 0.5444. accuracy 0.7405. Elapsed 178 seconds\n",
      "Epoch 7. Loss 0.5452. accuracy 0.7848. Elapsed 180 seconds\n",
      "Epoch 7. Loss 0.5456. accuracy 0.7785. Elapsed 181 seconds\n",
      "Epoch 7. Loss 0.5426. accuracy 0.7722. Elapsed 182 seconds\n",
      "Epoch 7. Loss 0.5469. accuracy 0.7785. Elapsed 183 seconds\n",
      "Epoch 7. Loss 0.5464. accuracy 0.7595. Elapsed 184 seconds\n",
      "Epoch 7. Loss 0.5428. accuracy 0.7785. Elapsed 185 seconds\n",
      "Epoch 7. Loss 0.5400. accuracy 0.7785. Elapsed 187 seconds\n",
      "Epoch 7. Loss 0.5361. accuracy 0.6772. Elapsed 188 seconds\n",
      "ep acc 0.6905810205204812\n",
      "new best saving model\n",
      "Total time elapsed: 3 minutes\n"
     ]
    }
   ],
   "source": [
    "tick = time.time()\n",
    "name='LS_new_4_26_train_ep'\n",
    "epoch_losses = []\n",
    "epoch_accs = []\n",
    "best_score= 0.5\n",
    "for epoch_num in range(1, num_epochs + 1):\n",
    "    batch_losses = []\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        #print(batch)\n",
    "        ### YOUR CODE BELOW ###\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Extract the inputs and the targets\n",
    "        inputs, targets = batch\n",
    "        # Transfer the inputs and the targets to GPUs, if available\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "        # Run the model\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs,targets)\n",
    "        \n",
    "        \n",
    "        # Backpropagate the error\n",
    "        loss.backward()\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Append the loss\n",
    "        batch_losses.append(float (loss))\n",
    "        ### YOUR CODE ABOVE ###\n",
    "        epoch_loss = np.mean(np.array(batch_losses))\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        \n",
    "        acc = compute_accuracy(val_data_set,model)\n",
    "        epoch_accs.append(acc)\n",
    "        \n",
    "\n",
    "        if epoch_num % report_every == 0:\n",
    "            tock = time.time()\n",
    "            print(\"Epoch {}. Loss {:.4f}. accuracy {:.4f}. Elapsed {:.0f} seconds\".format(epoch_num, epoch_loss,acc, tock-tick))\n",
    "    epoch_acc = np.mean(np.array(epoch_accs))\n",
    "    print('ep acc',epoch_acc)\n",
    "    if epoch_acc > best_score:\n",
    "            best_score = epoch_acc\n",
    "            print('new best saving model')\n",
    "            save('{}{}_best_acc_{:.2f}'.format(name,epoch_num,epoch_acc))\n",
    "     \n",
    "    #print('saving model')\n",
    "    #save(name + str(epoch_num))\n",
    "\n",
    "print(\"Total time elapsed: {:.0f} minutes\".format((tock-tick)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#train_data_set = None\n",
    "test_data_set = LastSentenceDataset(file='story_cloze_data/cloze_test_test__spring2016 - cloze_test_ALL_test.csv',vocab=voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(name):\n",
    "    l_model = LSModel()\n",
    "    l_model.load_state_dict(torch.load('saved_models/{}.save'.format(name)))\n",
    "    return l_model\n",
    "#model = load('LS_new_4_25_voc_fix_2_ep7_best_acc_0.70')\n",
    "#print('model',model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_accuracy(train_data_set,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_accuracy(test_data_set,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
