{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys\n",
    "import numpy\n",
    "sys.path.append('skip-thoughts.torch/pytorch')\n",
    "from skipthoughts import UniSkip,BiSkip\n",
    "import pandas as pd\n",
    "from Vocabulary import Vocabulary, preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab(tokens):\n",
    "    voc = Vocabulary(['<PAD>','<UNK>'])\n",
    "    voc.add_tokens(tokens)\n",
    "    print('vocab len is {}'.format(len(voc.w2idx)))\n",
    "    return voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file='story_cloze_data/cloze_test_val__spring2016 - cloze_test_ALL_val.csv'):\n",
    "    df= pd.read_csv(file)\n",
    "    df = df.drop('InputStoryid',axis=1)\n",
    "    targets = df['AnswerRightEnding']\n",
    "    df = df.drop('AnswerRightEnding',axis=1)\n",
    "    df = df.drop('InputSentence1',axis=1)\n",
    "    df = df.drop('InputSentence2',axis=1)\n",
    "    df = df.drop('InputSentence3',axis=1)\n",
    "    \n",
    "    voc_str= ''\n",
    "    for index, row in df.iterrows():\n",
    "        voc_str+=' '.join(list(row)) + ' '\n",
    "        \n",
    "    df['AnswerRightEnding'] = targets\n",
    "    return df,make_vocab(preprocess(voc_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import OrderedDict \n",
    "class LastSentenceDataset(Dataset):\n",
    "    def __init__(self,file='story_cloze_data/cloze_test_val__spring2016 - cloze_test_ALL_val.csv',vocab=None,df=None):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        created_df, created_vocab = load_data(file)\n",
    "        if df is None:\n",
    "            df = created_df\n",
    "        if vocab:\n",
    "            self.vocab = vocab\n",
    "        else:\n",
    "            self.vocab = created_vocab\n",
    "      \n",
    "        \n",
    "        self.dir_st = 'data/skip-thoughts'\n",
    "        self.biskip = BiSkip(self.dir_st, self.vocab.convert_to_list())\n",
    "        \n",
    "        self.uniskip = UniSkip(self.dir_st, self.vocab.convert_to_list())\n",
    "        \n",
    "        \n",
    "        self.data = self.make_data(df)\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx\n",
    "        Returns: skip thought embedding of ending and 0/1 if it is the right ending \n",
    "\n",
    "        \"\"\"\n",
    "        return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns len of the dataset\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "       \n",
    "    def make_data(self, df):\n",
    "        data = []\n",
    "        total = df.index\n",
    "        print('skip thought encoding dataset')\n",
    "        for i in total:\n",
    "            #print(row['RandomFifthSentenceQuiz1'],row['RandomFifthSentenceQuiz2'])\n",
    "            progress(i,len(total))\n",
    "            endings =  self.gen_embbeding(df.at[i,'RandomFifthSentenceQuiz1'], \n",
    "                                          df.at[i,'RandomFifthSentenceQuiz2'],\n",
    "                                          df.at[i,'InputSentence4'])\n",
    "            if df.at[i,'AnswerRightEnding'] == 1:\n",
    "                data.append((endings[0].detach().numpy(),1))\n",
    "                data.append((endings[1].detach().numpy(),0))\n",
    "            else:\n",
    "                data.append((endings[0].detach().numpy(),0))\n",
    "                data.append((endings[1].detach().numpy(),1))\n",
    "        return data\n",
    "    \n",
    "\n",
    "    def zero_pad(self,l,n):\n",
    "        l = (l + n * [0])[:n]\n",
    "        return l\n",
    "    \n",
    "    def pad_input(self,d):\n",
    "        d = OrderedDict(sorted(d.items(), key=lambda s: len(s[1])))\n",
    "        for k,v in d.items():\n",
    "            d[k]= self.zero_pad(v,len(list(d.items())[-1][1]))\n",
    "        return d\n",
    "        \n",
    "    def gen_embbeding(self,sent1,sent2,last_sent):\n",
    "        d = dict()\n",
    "        sent1 = preprocess(sent1)\n",
    "        sent2 = preprocess(sent2)\n",
    "        ls = preprocess(last_sent)\n",
    "        #remove random n token that is in one sentence\n",
    "        if 'n' in sent2:\n",
    "            sent2.remove('n')\n",
    "        d['sent1'] = self.vocab.get_sentence(sent1)\n",
    "        d['sent2'] = self.vocab.get_sentence(sent2)\n",
    "        d['ls'] = self.vocab.get_sentence(ls)\n",
    "        d = self.pad_input(d)\n",
    "        \n",
    "        batch = torch.LongTensor([d['sent1'],d['sent2'], d['ls']]) \n",
    "        top_half = self.uniskip(batch)\n",
    "        bottom_half = self.biskip(batch)\n",
    "        combine_skip = torch.cat([top_half,bottom_half],dim=1)\n",
    "        end1 = combine_skip[0]\n",
    "        end2 = combine_skip[1]\n",
    "        ls = combine_skip[2]\n",
    "        \n",
    "        #print(end1[:20],end2[:20])\n",
    "        end1.add_(ls)\n",
    "        end2.add_(ls)\n",
    "        #print('ls',ls[:20])\n",
    "        #print('after',end1[:20])\n",
    "        return end1,end2    \n",
    "    \n",
    "def progress(count, total, status=''):\n",
    "    bar_len = 60\n",
    "    filled_len = int(round(bar_len * count / float(total)))\n",
    "\n",
    "    percents = round(100.0 * count / float(total), 1)\n",
    "    bar = '=' * filled_len + '-' * (bar_len - filled_len)\n",
    "\n",
    "    sys.stdout.write('[%s] %s%s ...%s\\r' % (bar, percents, '%', status))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hd1 = torch.nn.Linear(4800, 2400)\n",
    "        self.hd2 = torch.nn.Linear(2400, 1200)\n",
    "        self.hd3 = torch.nn.Linear(1200, 600)\n",
    "        self.output = torch.nn.Linear(600, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.hd1(x))\n",
    "        x = torch.nn.functional.relu(self.hd2(x))\n",
    "        x = torch.nn.functional.relu(self.hd3(x))\n",
    "        x = self.output(x)\n",
    "        #print('output',x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(pair,model):\n",
    "    '''true if model predicts right'''\n",
    "    ending1, ending2 = pair\n",
    "    if ending1[1] == 1:\n",
    "        target = 1\n",
    "    else:\n",
    "        target =  2 \n",
    "        \n",
    "    ending1 = torch.tensor(ending1[0]).cuda()\n",
    "    ending2 = torch.tensor(ending2[0]).cuda()\n",
    "    res1 = model(ending1)\n",
    "    res2 = model(ending2)\n",
    "    softm = torch.nn.Softmax(dim=0)\n",
    "    prob_end1_right = softm(res1)[1].item() \n",
    "    prob_end2_right = softm(res2)[1].item()\n",
    "    \n",
    "    if prob_end1_right > prob_end2_right:\n",
    "        pred = 1\n",
    "    else:\n",
    "        pred = 2\n",
    "    \n",
    "    if pred == target:\n",
    "        return True\n",
    "    else: \n",
    "        #print(prob_end1_right,prob_end2_right,pred,target)\n",
    "        return False\n",
    "def score2(pair,clf):\n",
    "    '''true if model predicts right'''\n",
    "    ending1, ending2 = pair\n",
    "    if ending1[1] == 1:\n",
    "        target = 1\n",
    "    else:\n",
    "        target =  2 \n",
    "        \n",
    "    ending1 = ending1[0]\n",
    "    ending2 = ending2[0]\n",
    "    prob_end1_right = clf.predict_proba([ending1.detach().numpy()])[0][1]\n",
    "    prob_end2_right = clf.predict_proba([ending2.detach().numpy()])[0][1]\n",
    "    \n",
    "    if prob_end1_right > prob_end2_right:\n",
    "        pred = 1\n",
    "    else:\n",
    "        pred = 2\n",
    "    \n",
    "    if pred == target:\n",
    "        return True\n",
    "    else: \n",
    "        #print(prob_end1_right,prob_end2_right,pred,target)\n",
    "        return False\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(data_set,model):\n",
    "    num_right = 0\n",
    "    for i in range(0,len(data_set),2):\n",
    "        if score((data_set[i],data_set[i+1]),model):\n",
    "            num_right+=1\n",
    "    return num_right / (len(data_set)/2)\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(name):\n",
    "    torch.save(model.state_dict(), 'saved_models/{}.save'.format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len is 5303\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df, voc = load_data()\n",
    "train, val = train_test_split(df, test_size=0.1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-paper')\n",
    "batch_size = 128\n",
    "lr = 0.01\n",
    "num_epochs = 10\n",
    "report_every = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len is 5303\n",
      "Warning: 47/930911 words are not in dictionary, thus set UNK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/cs/undergrad/2019/gnosborn/nlp/env/lib/python3.6/site-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 47/930911 words are not in dictionary, thus set UNK\n",
      "skip thought encoding dataset\n",
      "[============================================================] 99.9% ...\r"
     ]
    }
   ],
   "source": [
    "train_data_set = LastSentenceDataset(df=train,vocab=voc)\n",
    "data_loader = torch.utils.data.DataLoader(train_data_set, batch_size=batch_size, shuffle=True,num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len is 5303\n",
      "Warning: 47/930911 words are not in dictionary, thus set UNK\n",
      "Warning: 47/930911 words are not in dictionary, thus set UNK\n",
      "skip thought encoding dataset\n",
      "[=====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] 994.7% ...\r"
     ]
    }
   ],
   "source": [
    "val_data_set = LastSentenceDataset(file='story_cloze_data/cloze_test_val__spring2016 - cloze_test_ALL_val.csv',df=val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSModel()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Loss 0.6930. accuracy 0.4521. Elapsed 0 seconds\n",
      "Epoch 1. Loss 118.9712. accuracy 0.4468. Elapsed 0 seconds\n",
      "Epoch 1. Loss 100.9401. accuracy 0.4521. Elapsed 1 seconds\n",
      "Epoch 1. Loss 81.6606. accuracy 0.4734. Elapsed 1 seconds\n",
      "Epoch 1. Loss 65.4659. accuracy 0.5266. Elapsed 1 seconds\n",
      "Epoch 1. Loss 54.7566. accuracy 0.5000. Elapsed 1 seconds\n",
      "Epoch 1. Loss 47.0888. accuracy 0.5213. Elapsed 1 seconds\n",
      "Epoch 1. Loss 41.2918. accuracy 0.4734. Elapsed 1 seconds\n",
      "Epoch 1. Loss 36.7850. accuracy 0.4787. Elapsed 1 seconds\n",
      "Epoch 1. Loss 33.1788. accuracy 0.4947. Elapsed 2 seconds\n",
      "Epoch 1. Loss 30.2281. accuracy 0.5426. Elapsed 2 seconds\n",
      "Epoch 1. Loss 27.7666. accuracy 0.5053. Elapsed 2 seconds\n",
      "Epoch 1. Loss 25.6858. accuracy 0.5000. Elapsed 2 seconds\n",
      "Epoch 1. Loss 23.9017. accuracy 0.4628. Elapsed 2 seconds\n",
      "Epoch 1. Loss 22.3564. accuracy 0.5160. Elapsed 2 seconds\n",
      "Epoch 1. Loss 21.0029. accuracy 0.4894. Elapsed 3 seconds\n",
      "Epoch 1. Loss 19.8081. accuracy 0.5266. Elapsed 3 seconds\n",
      "Epoch 1. Loss 18.7462. accuracy 0.5479. Elapsed 3 seconds\n",
      "Epoch 1. Loss 17.7959. accuracy 0.5266. Elapsed 3 seconds\n",
      "Epoch 1. Loss 16.9421. accuracy 0.5426. Elapsed 3 seconds\n",
      "Epoch 1. Loss 16.1685. accuracy 0.5426. Elapsed 3 seconds\n",
      "Epoch 1. Loss 15.4649. accuracy 0.5053. Elapsed 4 seconds\n",
      "Epoch 1. Loss 14.8228. accuracy 0.5000. Elapsed 4 seconds\n",
      "Epoch 1. Loss 14.2344. accuracy 0.5319. Elapsed 4 seconds\n",
      "Epoch 1. Loss 13.6918. accuracy 0.5691. Elapsed 4 seconds\n",
      "Epoch 1. Loss 13.1929. accuracy 0.5638. Elapsed 4 seconds\n",
      "Epoch 1. Loss 12.7299. accuracy 0.5266. Elapsed 4 seconds\n",
      "ep acc 0.5080772261623325\n",
      "new best saving model\n",
      "saving model\n",
      "Epoch 2. Loss 0.7175. accuracy 0.5372. Elapsed 7 seconds\n",
      "Epoch 2. Loss 0.7208. accuracy 0.5479. Elapsed 7 seconds\n",
      "Epoch 2. Loss 0.7172. accuracy 0.5106. Elapsed 8 seconds\n",
      "Epoch 2. Loss 0.7105. accuracy 0.5319. Elapsed 8 seconds\n",
      "Epoch 2. Loss 0.7067. accuracy 0.5213. Elapsed 8 seconds\n",
      "Epoch 2. Loss 0.7040. accuracy 0.5000. Elapsed 8 seconds\n",
      "Epoch 2. Loss 0.7026. accuracy 0.5266. Elapsed 8 seconds\n",
      "Epoch 2. Loss 0.7014. accuracy 0.5160. Elapsed 8 seconds\n",
      "Epoch 2. Loss 0.7017. accuracy 0.4628. Elapsed 9 seconds\n",
      "Epoch 2. Loss 0.7001. accuracy 0.5106. Elapsed 9 seconds\n",
      "Epoch 2. Loss 0.6990. accuracy 0.5479. Elapsed 9 seconds\n",
      "Epoch 2. Loss 0.7055. accuracy 0.5106. Elapsed 9 seconds\n",
      "Epoch 2. Loss 0.7047. accuracy 0.6170. Elapsed 9 seconds\n",
      "Epoch 2. Loss 0.7040. accuracy 0.5479. Elapsed 9 seconds\n",
      "Epoch 2. Loss 0.7031. accuracy 0.6011. Elapsed 10 seconds\n",
      "Epoch 2. Loss 0.7044. accuracy 0.5106. Elapsed 10 seconds\n",
      "Epoch 2. Loss 0.7102. accuracy 0.5319. Elapsed 10 seconds\n",
      "Epoch 2. Loss 0.7081. accuracy 0.5213. Elapsed 10 seconds\n",
      "Epoch 2. Loss 0.7124. accuracy 0.6011. Elapsed 10 seconds\n",
      "Epoch 2. Loss 0.7115. accuracy 0.5798. Elapsed 10 seconds\n",
      "Epoch 2. Loss 0.7105. accuracy 0.5851. Elapsed 10 seconds\n",
      "Epoch 2. Loss 0.7096. accuracy 0.5213. Elapsed 11 seconds\n",
      "Epoch 2. Loss 0.7090. accuracy 0.5798. Elapsed 11 seconds\n",
      "Epoch 2. Loss 0.7082. accuracy 0.5585. Elapsed 11 seconds\n",
      "Epoch 2. Loss 0.7083. accuracy 0.5798. Elapsed 11 seconds\n",
      "Epoch 2. Loss 0.7079. accuracy 0.5532. Elapsed 11 seconds\n",
      "Epoch 2. Loss 0.7078. accuracy 0.5532. Elapsed 11 seconds\n",
      "ep acc 0.5256107171000787\n",
      "new best saving model\n",
      "saving model\n",
      "Epoch 3. Loss 0.6914. accuracy 0.5426. Elapsed 14 seconds\n",
      "Epoch 3. Loss 0.6904. accuracy 0.5479. Elapsed 15 seconds\n",
      "Epoch 3. Loss 0.6900. accuracy 0.5479. Elapsed 15 seconds\n",
      "Epoch 3. Loss 0.6894. accuracy 0.5266. Elapsed 15 seconds\n",
      "Epoch 3. Loss 0.6898. accuracy 0.5426. Elapsed 15 seconds\n",
      "Epoch 3. Loss 0.6900. accuracy 0.5585. Elapsed 15 seconds\n",
      "Epoch 3. Loss 0.6896. accuracy 0.5266. Elapsed 15 seconds\n",
      "Epoch 3. Loss 0.6888. accuracy 0.5372. Elapsed 16 seconds\n",
      "Epoch 3. Loss 0.6882. accuracy 0.5745. Elapsed 16 seconds\n",
      "Epoch 3. Loss 0.6882. accuracy 0.5160. Elapsed 16 seconds\n",
      "Epoch 3. Loss 0.6947. accuracy 0.5319. Elapsed 16 seconds\n",
      "Epoch 3. Loss 0.6943. accuracy 0.5319. Elapsed 16 seconds\n",
      "Epoch 3. Loss 0.6934. accuracy 0.5638. Elapsed 16 seconds\n",
      "Epoch 3. Loss 0.6922. accuracy 0.5106. Elapsed 17 seconds\n",
      "Epoch 3. Loss 0.6922. accuracy 0.5426. Elapsed 17 seconds\n",
      "Epoch 3. Loss 0.6927. accuracy 0.5957. Elapsed 17 seconds\n",
      "Epoch 3. Loss 0.6944. accuracy 0.6223. Elapsed 17 seconds\n",
      "Epoch 3. Loss 0.6939. accuracy 0.5585. Elapsed 17 seconds\n",
      "Epoch 3. Loss 0.6941. accuracy 0.6064. Elapsed 17 seconds\n",
      "Epoch 3. Loss 0.6940. accuracy 0.5851. Elapsed 17 seconds\n",
      "Epoch 3. Loss 0.6936. accuracy 0.5691. Elapsed 18 seconds\n",
      "Epoch 3. Loss 0.6934. accuracy 0.5691. Elapsed 18 seconds\n",
      "Epoch 3. Loss 0.6930. accuracy 0.5479. Elapsed 18 seconds\n",
      "Epoch 3. Loss 0.6928. accuracy 0.5745. Elapsed 18 seconds\n",
      "Epoch 3. Loss 0.6929. accuracy 0.6064. Elapsed 18 seconds\n",
      "Epoch 3. Loss 0.6926. accuracy 0.6064. Elapsed 18 seconds\n",
      "Epoch 3. Loss 0.6923. accuracy 0.6436. Elapsed 19 seconds\n",
      "ep acc 0.5378907276070396\n",
      "new best saving model\n",
      "saving model\n",
      "Epoch 4. Loss 0.6992. accuracy 0.5957. Elapsed 22 seconds\n",
      "Epoch 4. Loss 0.6913. accuracy 0.6117. Elapsed 22 seconds\n",
      "Epoch 4. Loss 0.6860. accuracy 0.5745. Elapsed 22 seconds\n",
      "Epoch 4. Loss 0.6808. accuracy 0.5638. Elapsed 22 seconds\n",
      "Epoch 4. Loss 0.6860. accuracy 0.6170. Elapsed 22 seconds\n",
      "Epoch 4. Loss 0.6914. accuracy 0.6011. Elapsed 23 seconds\n",
      "Epoch 4. Loss 0.6910. accuracy 0.5745. Elapsed 23 seconds\n",
      "Epoch 4. Loss 0.6890. accuracy 0.5691. Elapsed 23 seconds\n",
      "Epoch 4. Loss 0.6869. accuracy 0.5798. Elapsed 23 seconds\n",
      "Epoch 4. Loss 0.6853. accuracy 0.6117. Elapsed 23 seconds\n",
      "Epoch 4. Loss 0.6841. accuracy 0.6117. Elapsed 23 seconds\n",
      "Epoch 4. Loss 0.6859. accuracy 0.5957. Elapsed 24 seconds\n",
      "Epoch 4. Loss 0.6847. accuracy 0.5957. Elapsed 24 seconds\n",
      "Epoch 4. Loss 0.6849. accuracy 0.6117. Elapsed 24 seconds\n",
      "Epoch 4. Loss 0.6855. accuracy 0.5904. Elapsed 24 seconds\n",
      "Epoch 4. Loss 0.6857. accuracy 0.6064. Elapsed 24 seconds\n",
      "Epoch 4. Loss 0.6851. accuracy 0.5957. Elapsed 24 seconds\n",
      "Epoch 4. Loss 0.6846. accuracy 0.5904. Elapsed 24 seconds\n",
      "Epoch 4. Loss 0.6843. accuracy 0.6064. Elapsed 25 seconds\n",
      "Epoch 4. Loss 0.6850. accuracy 0.5904. Elapsed 25 seconds\n",
      "Epoch 4. Loss 0.6841. accuracy 0.5957. Elapsed 25 seconds\n",
      "Epoch 4. Loss 0.6835. accuracy 0.6064. Elapsed 25 seconds\n",
      "Epoch 4. Loss 0.6852. accuracy 0.6117. Elapsed 25 seconds\n",
      "Epoch 4. Loss 0.6859. accuracy 0.6170. Elapsed 25 seconds\n",
      "Epoch 4. Loss 0.6858. accuracy 0.6383. Elapsed 26 seconds\n",
      "Epoch 4. Loss 0.6853. accuracy 0.6064. Elapsed 26 seconds\n",
      "Epoch 4. Loss 0.6855. accuracy 0.5851. Elapsed 26 seconds\n",
      "ep acc 0.5529944838455476\n",
      "new best saving model\n",
      "saving model\n",
      "Epoch 5. Loss 0.6770. accuracy 0.5479. Elapsed 29 seconds\n",
      "Epoch 5. Loss 0.6658. accuracy 0.5851. Elapsed 29 seconds\n",
      "Epoch 5. Loss 0.6793. accuracy 0.5638. Elapsed 29 seconds\n",
      "Epoch 5. Loss 0.6767. accuracy 0.6543. Elapsed 29 seconds\n",
      "Epoch 5. Loss 0.6758. accuracy 0.6170. Elapsed 29 seconds\n",
      "Epoch 5. Loss 0.6784. accuracy 0.6170. Elapsed 30 seconds\n",
      "Epoch 5. Loss 0.6756. accuracy 0.5957. Elapsed 30 seconds\n",
      "Epoch 5. Loss 0.6762. accuracy 0.6011. Elapsed 30 seconds\n",
      "Epoch 5. Loss 0.6709. accuracy 0.5691. Elapsed 30 seconds\n",
      "Epoch 5. Loss 0.6726. accuracy 0.5904. Elapsed 30 seconds\n",
      "Epoch 5. Loss 0.6775. accuracy 0.6117. Elapsed 30 seconds\n",
      "Epoch 5. Loss 0.6771. accuracy 0.6277. Elapsed 31 seconds\n",
      "Epoch 5. Loss 0.6755. accuracy 0.5532. Elapsed 31 seconds\n",
      "Epoch 5. Loss 0.6787. accuracy 0.6170. Elapsed 31 seconds\n",
      "Epoch 5. Loss 0.6766. accuracy 0.6170. Elapsed 31 seconds\n",
      "Epoch 5. Loss 0.6766. accuracy 0.5904. Elapsed 31 seconds\n",
      "Epoch 5. Loss 0.6758. accuracy 0.6011. Elapsed 31 seconds\n",
      "Epoch 5. Loss 0.6754. accuracy 0.5957. Elapsed 32 seconds\n",
      "Epoch 5. Loss 0.6743. accuracy 0.6011. Elapsed 32 seconds\n",
      "Epoch 5. Loss 0.6748. accuracy 0.6170. Elapsed 32 seconds\n",
      "Epoch 5. Loss 0.6752. accuracy 0.6170. Elapsed 32 seconds\n",
      "Epoch 5. Loss 0.6744. accuracy 0.5957. Elapsed 32 seconds\n",
      "Epoch 5. Loss 0.6742. accuracy 0.6011. Elapsed 32 seconds\n",
      "Epoch 5. Loss 0.6738. accuracy 0.6170. Elapsed 32 seconds\n",
      "Epoch 5. Loss 0.6733. accuracy 0.5745. Elapsed 33 seconds\n",
      "Epoch 5. Loss 0.6745. accuracy 0.6223. Elapsed 33 seconds\n",
      "Epoch 5. Loss 0.6761. accuracy 0.6277. Elapsed 33 seconds\n",
      "ep acc 0.562608353033885\n",
      "new best saving model\n",
      "saving model\n",
      "Epoch 6. Loss 0.6356. accuracy 0.6223. Elapsed 36 seconds\n",
      "Epoch 6. Loss 0.6274. accuracy 0.6011. Elapsed 36 seconds\n",
      "Epoch 6. Loss 0.6273. accuracy 0.6330. Elapsed 36 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6. Loss 0.6394. accuracy 0.6330. Elapsed 36 seconds\n",
      "Epoch 6. Loss 0.6358. accuracy 0.5957. Elapsed 36 seconds\n",
      "Epoch 6. Loss 0.6421. accuracy 0.6223. Elapsed 37 seconds\n",
      "Epoch 6. Loss 0.6396. accuracy 0.6489. Elapsed 37 seconds\n",
      "Epoch 6. Loss 0.6403. accuracy 0.6489. Elapsed 37 seconds\n",
      "Epoch 6. Loss 0.6362. accuracy 0.6543. Elapsed 37 seconds\n",
      "Epoch 6. Loss 0.6478. accuracy 0.6489. Elapsed 37 seconds\n",
      "Epoch 6. Loss 0.6579. accuracy 0.6011. Elapsed 37 seconds\n",
      "Epoch 6. Loss 0.6592. accuracy 0.6277. Elapsed 38 seconds\n",
      "Epoch 6. Loss 0.6578. accuracy 0.6330. Elapsed 38 seconds\n",
      "Epoch 6. Loss 0.6582. accuracy 0.6277. Elapsed 38 seconds\n",
      "Epoch 6. Loss 0.6567. accuracy 0.6064. Elapsed 38 seconds\n",
      "Epoch 6. Loss 0.6560. accuracy 0.5798. Elapsed 38 seconds\n",
      "Epoch 6. Loss 0.6540. accuracy 0.6277. Elapsed 38 seconds\n",
      "Epoch 6. Loss 0.6534. accuracy 0.6011. Elapsed 39 seconds\n",
      "Epoch 6. Loss 0.6558. accuracy 0.6330. Elapsed 39 seconds\n",
      "Epoch 6. Loss 0.6552. accuracy 0.6489. Elapsed 39 seconds\n",
      "Epoch 6. Loss 0.6543. accuracy 0.6223. Elapsed 39 seconds\n",
      "Epoch 6. Loss 0.6553. accuracy 0.6223. Elapsed 39 seconds\n",
      "Epoch 6. Loss 0.6545. accuracy 0.6330. Elapsed 39 seconds\n",
      "Epoch 6. Loss 0.6545. accuracy 0.6383. Elapsed 39 seconds\n",
      "Epoch 6. Loss 0.6547. accuracy 0.5957. Elapsed 40 seconds\n",
      "Epoch 6. Loss 0.6547. accuracy 0.6436. Elapsed 40 seconds\n",
      "Epoch 6. Loss 0.6524. accuracy 0.6383. Elapsed 40 seconds\n",
      "ep acc 0.5730890464933018\n",
      "new best saving model\n",
      "saving model\n",
      "Epoch 7. Loss 0.6312. accuracy 0.6277. Elapsed 43 seconds\n",
      "Epoch 7. Loss 0.6260. accuracy 0.6383. Elapsed 43 seconds\n",
      "Epoch 7. Loss 0.6186. accuracy 0.5957. Elapsed 43 seconds\n",
      "Epoch 7. Loss 0.6700. accuracy 0.6117. Elapsed 43 seconds\n",
      "Epoch 7. Loss 0.6645. accuracy 0.6543. Elapsed 44 seconds\n",
      "Epoch 7. Loss 0.6619. accuracy 0.6330. Elapsed 44 seconds\n",
      "Epoch 7. Loss 0.6581. accuracy 0.6383. Elapsed 44 seconds\n",
      "Epoch 7. Loss 0.6517. accuracy 0.6383. Elapsed 44 seconds\n",
      "Epoch 7. Loss 0.6495. accuracy 0.6383. Elapsed 44 seconds\n",
      "Epoch 7. Loss 0.6511. accuracy 0.6170. Elapsed 44 seconds\n",
      "Epoch 7. Loss 0.6487. accuracy 0.6223. Elapsed 45 seconds\n",
      "Epoch 7. Loss 0.6469. accuracy 0.6330. Elapsed 45 seconds\n",
      "Epoch 7. Loss 0.6443. accuracy 0.5851. Elapsed 45 seconds\n",
      "Epoch 7. Loss 0.6476. accuracy 0.6543. Elapsed 45 seconds\n",
      "Epoch 7. Loss 0.6606. accuracy 0.6223. Elapsed 45 seconds\n",
      "Epoch 7. Loss 0.6593. accuracy 0.6117. Elapsed 45 seconds\n",
      "Epoch 7. Loss 0.6593. accuracy 0.6383. Elapsed 45 seconds\n",
      "Epoch 7. Loss 0.6582. accuracy 0.6436. Elapsed 46 seconds\n",
      "Epoch 7. Loss 0.6563. accuracy 0.6223. Elapsed 46 seconds\n",
      "Epoch 7. Loss 0.6568. accuracy 0.6383. Elapsed 46 seconds\n",
      "Epoch 7. Loss 0.6568. accuracy 0.6277. Elapsed 46 seconds\n",
      "Epoch 7. Loss 0.6549. accuracy 0.6596. Elapsed 46 seconds\n",
      "Epoch 7. Loss 0.6539. accuracy 0.6277. Elapsed 46 seconds\n",
      "Epoch 7. Loss 0.6522. accuracy 0.6436. Elapsed 47 seconds\n",
      "Epoch 7. Loss 0.6512. accuracy 0.6223. Elapsed 47 seconds\n",
      "Epoch 7. Loss 0.6494. accuracy 0.6489. Elapsed 47 seconds\n",
      "Epoch 7. Loss 0.6525. accuracy 0.6064. Elapsed 47 seconds\n",
      "ep acc 0.5811662726556344\n",
      "new best saving model\n",
      "saving model\n",
      "Epoch 8. Loss 0.6867. accuracy 0.6170. Elapsed 50 seconds\n",
      "Epoch 8. Loss 0.6313. accuracy 0.5904. Elapsed 50 seconds\n",
      "Epoch 8. Loss 0.6242. accuracy 0.6383. Elapsed 50 seconds\n",
      "Epoch 8. Loss 0.6282. accuracy 0.6117. Elapsed 51 seconds\n",
      "Epoch 8. Loss 0.6193. accuracy 0.6223. Elapsed 51 seconds\n",
      "Epoch 8. Loss 0.6124. accuracy 0.6223. Elapsed 51 seconds\n",
      "Epoch 8. Loss 0.6059. accuracy 0.6383. Elapsed 51 seconds\n",
      "Epoch 8. Loss 0.6034. accuracy 0.6064. Elapsed 51 seconds\n",
      "Epoch 8. Loss 0.6092. accuracy 0.6489. Elapsed 51 seconds\n",
      "Epoch 8. Loss 0.6162. accuracy 0.6330. Elapsed 52 seconds\n",
      "Epoch 8. Loss 0.6123. accuracy 0.6277. Elapsed 52 seconds\n",
      "Epoch 8. Loss 0.6096. accuracy 0.6223. Elapsed 52 seconds\n",
      "Epoch 8. Loss 0.6076. accuracy 0.6277. Elapsed 52 seconds\n",
      "Epoch 8. Loss 0.6060. accuracy 0.6596. Elapsed 52 seconds\n",
      "Epoch 8. Loss 0.6066. accuracy 0.6649. Elapsed 52 seconds\n",
      "Epoch 8. Loss 0.6074. accuracy 0.6330. Elapsed 53 seconds\n",
      "Epoch 8. Loss 0.6041. accuracy 0.6277. Elapsed 53 seconds\n",
      "Epoch 8. Loss 0.6030. accuracy 0.6330. Elapsed 53 seconds\n",
      "Epoch 8. Loss 0.6019. accuracy 0.6330. Elapsed 53 seconds\n",
      "Epoch 8. Loss 0.5999. accuracy 0.6277. Elapsed 53 seconds\n",
      "Epoch 8. Loss 0.5983. accuracy 0.6436. Elapsed 53 seconds\n",
      "Epoch 8. Loss 0.5981. accuracy 0.6223. Elapsed 53 seconds\n",
      "Epoch 8. Loss 0.6052. accuracy 0.6383. Elapsed 54 seconds\n",
      "Epoch 8. Loss 0.6050. accuracy 0.6330. Elapsed 54 seconds\n",
      "Epoch 8. Loss 0.6053. accuracy 0.6223. Elapsed 54 seconds\n",
      "Epoch 8. Loss 0.6045. accuracy 0.6170. Elapsed 54 seconds\n",
      "Epoch 8. Loss 0.6017. accuracy 0.5957. Elapsed 54 seconds\n",
      "ep acc 0.5870271867612292\n",
      "new best saving model\n",
      "saving model\n",
      "Epoch 9. Loss 0.5648. accuracy 0.5851. Elapsed 57 seconds\n",
      "Epoch 9. Loss 0.5418. accuracy 0.5957. Elapsed 57 seconds\n",
      "Epoch 9. Loss 0.5414. accuracy 0.5798. Elapsed 57 seconds\n",
      "Epoch 9. Loss 0.5583. accuracy 0.5745. Elapsed 58 seconds\n",
      "Epoch 9. Loss 0.5557. accuracy 0.5638. Elapsed 58 seconds\n",
      "Epoch 9. Loss 0.5457. accuracy 0.5904. Elapsed 58 seconds\n",
      "Epoch 9. Loss 0.5560. accuracy 0.5957. Elapsed 58 seconds\n",
      "Epoch 9. Loss 0.5692. accuracy 0.5957. Elapsed 58 seconds\n",
      "Epoch 9. Loss 0.5715. accuracy 0.5851. Elapsed 58 seconds\n",
      "Epoch 9. Loss 0.5654. accuracy 0.5957. Elapsed 59 seconds\n",
      "Epoch 9. Loss 0.5553. accuracy 0.5904. Elapsed 59 seconds\n",
      "Epoch 9. Loss 0.5477. accuracy 0.6064. Elapsed 59 seconds\n",
      "Epoch 9. Loss 0.5461. accuracy 0.5904. Elapsed 59 seconds\n",
      "Epoch 9. Loss 0.5396. accuracy 0.6064. Elapsed 59 seconds\n",
      "Epoch 9. Loss 0.5374. accuracy 0.6117. Elapsed 59 seconds\n",
      "Epoch 9. Loss 0.5349. accuracy 0.6011. Elapsed 60 seconds\n",
      "Epoch 9. Loss 0.5453. accuracy 0.6011. Elapsed 60 seconds\n",
      "Epoch 9. Loss 0.5458. accuracy 0.6064. Elapsed 60 seconds\n",
      "Epoch 9. Loss 0.5427. accuracy 0.6330. Elapsed 60 seconds\n",
      "Epoch 9. Loss 0.5428. accuracy 0.5904. Elapsed 60 seconds\n",
      "Epoch 9. Loss 0.5397. accuracy 0.5798. Elapsed 60 seconds\n",
      "Epoch 9. Loss 0.5358. accuracy 0.5798. Elapsed 60 seconds\n",
      "Epoch 9. Loss 0.5387. accuracy 0.5904. Elapsed 61 seconds\n",
      "Epoch 9. Loss 0.5375. accuracy 0.6011. Elapsed 61 seconds\n",
      "Epoch 9. Loss 0.5368. accuracy 0.6011. Elapsed 61 seconds\n",
      "Epoch 9. Loss 0.5381. accuracy 0.5957. Elapsed 61 seconds\n",
      "Epoch 9. Loss 0.5481. accuracy 0.6383. Elapsed 61 seconds\n",
      "ep acc 0.5879957972156554\n",
      "new best saving model\n",
      "saving model\n",
      "Epoch 10. Loss 0.6920. accuracy 0.5745. Elapsed 65 seconds\n",
      "Epoch 10. Loss 0.6216. accuracy 0.5798. Elapsed 65 seconds\n",
      "Epoch 10. Loss 0.5820. accuracy 0.5904. Elapsed 65 seconds\n",
      "Epoch 10. Loss 0.5596. accuracy 0.5957. Elapsed 65 seconds\n",
      "Epoch 10. Loss 0.5431. accuracy 0.5904. Elapsed 65 seconds\n",
      "Epoch 10. Loss 0.5327. accuracy 0.5585. Elapsed 65 seconds\n",
      "Epoch 10. Loss 0.5315. accuracy 0.6170. Elapsed 66 seconds\n",
      "Epoch 10. Loss 0.5271. accuracy 0.6330. Elapsed 66 seconds\n",
      "Epoch 10. Loss 0.5201. accuracy 0.5851. Elapsed 66 seconds\n",
      "Epoch 10. Loss 0.5053. accuracy 0.5957. Elapsed 66 seconds\n",
      "Epoch 10. Loss 0.5011. accuracy 0.5851. Elapsed 66 seconds\n",
      "Epoch 10. Loss 0.4972. accuracy 0.6064. Elapsed 66 seconds\n",
      "Epoch 10. Loss 0.4890. accuracy 0.6117. Elapsed 66 seconds\n",
      "Epoch 10. Loss 0.4824. accuracy 0.5585. Elapsed 67 seconds\n",
      "Epoch 10. Loss 0.4757. accuracy 0.5638. Elapsed 67 seconds\n",
      "Epoch 10. Loss 0.4943. accuracy 0.5957. Elapsed 67 seconds\n",
      "Epoch 10. Loss 0.5392. accuracy 0.5745. Elapsed 67 seconds\n",
      "Epoch 10. Loss 0.5417. accuracy 0.5691. Elapsed 67 seconds\n",
      "Epoch 10. Loss 0.5429. accuracy 0.5851. Elapsed 67 seconds\n",
      "Epoch 10. Loss 0.5443. accuracy 0.5798. Elapsed 68 seconds\n",
      "Epoch 10. Loss 0.5442. accuracy 0.5745. Elapsed 68 seconds\n",
      "Epoch 10. Loss 0.5437. accuracy 0.6170. Elapsed 68 seconds\n",
      "Epoch 10. Loss 0.5417. accuracy 0.5798. Elapsed 68 seconds\n",
      "Epoch 10. Loss 0.5373. accuracy 0.5691. Elapsed 68 seconds\n",
      "Epoch 10. Loss 0.5353. accuracy 0.5798. Elapsed 68 seconds\n",
      "Epoch 10. Loss 0.5362. accuracy 0.6064. Elapsed 69 seconds\n",
      "Epoch 10. Loss 0.5353. accuracy 0.5957. Elapsed 69 seconds\n",
      "ep acc 0.5879826635145784\n",
      "saving model\n",
      "Total time elapsed: 1 minutes\n"
     ]
    }
   ],
   "source": [
    "tick = time.time()\n",
    "name='LS_new'\n",
    "epoch_accs = []\n",
    "best_score= 0.5\n",
    "for epoch_num in range(1, num_epochs + 1):\n",
    "    batch_losses = []\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        #print(batch)\n",
    "        ### YOUR CODE BELOW ###\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Extract the inputs and the targets\n",
    "        inputs, targets = batch\n",
    "        # Transfer the inputs and the targets to GPUs, if available\n",
    "        if torch.cuda.is_available():\n",
    "            inputs =inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "        # Run the model\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs,targets)\n",
    "        \n",
    "        \n",
    "        # Backpropagate the error\n",
    "        loss.backward()\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Append the loss\n",
    "        batch_losses.append(float (loss))\n",
    "        ### YOUR CODE ABOVE ###\n",
    "        epoch_loss = np.mean(np.array(batch_losses))\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        \n",
    "        acc = compute_accuracy(val_data_set,model)\n",
    "        epoch_accs.append(acc)\n",
    "        \n",
    "\n",
    "        if epoch_num % report_every == 0:\n",
    "            tock = time.time()\n",
    "            print(\"Epoch {}. Loss {:.4f}. accuracy {:.4f}. Elapsed {:.0f} seconds\".format(epoch_num, epoch_loss,acc, tock-tick))\n",
    "    epoch_acc = np.mean(np.array(epoch_accs))\n",
    "    print('ep acc',epoch_acc)\n",
    "    if epoch_acc > best_score:\n",
    "            best_score = epoch_acc\n",
    "            print('new best saving model')\n",
    "            save('{}{}_best_acc_{:.2f}'.format(name,epoch_num,acc))\n",
    "     \n",
    "    print('saving model')\n",
    "    #save(name + str(epoch_num))\n",
    "\n",
    "print(\"Total time elapsed: {:.0f} minutes\".format((tock-tick)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len is 5226\n",
      "Warning: 47/930911 words are not in dictionary, thus set UNK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/cs/undergrad/2019/gnosborn/nlp/env/lib/python3.6/site-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 47/930911 words are not in dictionary, thus set UNK\n",
      "skip thought encoding dataset\n",
      "[============================================================] 99.9% ...\r"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data_set = None\n",
    "test_data_set = LastSentenceDataset(file='story_cloze_data/cloze_test_test__spring2016 - cloze_test_ALL_test.csv',vocab=voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(name):\n",
    "    l_model = LSModel()\n",
    "    l_model.load_state_dict(torch.load('saved_models/{}.save'.format(name)))\n",
    "    return l_model\n",
    "#model = load('LS_adagrad5')\n",
    "#print('model',model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5793693212185996"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy(test_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
