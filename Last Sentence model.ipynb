{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys\n",
    "import numpy\n",
    "sys.path.append('skip-thoughts.torch/pytorch')\n",
    "from skipthoughts import UniSkip,BiSkip\n",
    "import pandas as pd\n",
    "from Vocabulary import Vocabulary, preprocess\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab(tokens):\n",
    "    voc = Vocabulary(['<PAD>','<UNK>'])\n",
    "    voc.add_tokens(tokens)\n",
    "    print('vocab len is {}'.format(len(voc.w2idx)))\n",
    "    return voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file='story_cloze_data/cloze_test_val__spring2016 - cloze_test_ALL_val.csv'):\n",
    "    df= pd.read_csv(file)\n",
    "    df = df.drop('InputStoryid',axis=1)\n",
    "    targets = df['AnswerRightEnding']\n",
    "    df = df.drop('AnswerRightEnding',axis=1)\n",
    "    df = df.drop('InputSentence1',axis=1)\n",
    "    df = df.drop('InputSentence2',axis=1)\n",
    "    df = df.drop('InputSentence3',axis=1)\n",
    "    \n",
    "    voc_str= ''\n",
    "    for index, row in df.iterrows():\n",
    "        voc_str+=' '.join(list(row)) + ' '\n",
    "        \n",
    "    df['AnswerRightEnding'] = targets\n",
    "    return df,make_vocab(preprocess(voc_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_train(file='story_cloze_data/ROCStories__spring2016 - ROCStories_spring2016.csv',random_seed=None):\n",
    "    np.random.RandomState(random_seed)\n",
    "    df = pd.read_csv(file)\n",
    "    df = df.drop('storyid',axis=1)\n",
    "    df = df.drop('storytitle',axis=1)\n",
    "    df = df.drop('sentence1',axis=1)\n",
    "    df = df.drop('sentence2',axis=1)\n",
    "    df = df.drop('sentence3',axis=1)\n",
    "    target = df['sentence5']\n",
    "    df = df.drop('sentence5',axis=1)\n",
    "    \n",
    "    df = df.rename({'sentence4':'InputSentence4'},axis=1)\n",
    "    answer1=[]\n",
    "    answer2=[]\n",
    "    trueanswer=[]\n",
    "    for i in range(len(target)):\n",
    "        k= np.random.randint(0,len(target))\n",
    "        while k == i:\n",
    "            k= np.random.randint(0,len(target))\n",
    "        if np.random.random()<.5:\n",
    "            answer1.append(target[i])\n",
    "            answer2.append(target[k])\n",
    "            trueanswer.append(1)\n",
    "        else:\n",
    "            answer1.append(target[k])\n",
    "            answer2.append(target[i])\n",
    "            trueanswer.append(2)\n",
    "    \n",
    "    df1 = pd.DataFrame({'RandomFifthSentenceQuiz1': answer1,'RandomFifthSentenceQuiz2': answer2})\n",
    "    df = df.join(df1)\n",
    "    voc_str= ''\n",
    "    for index, row in df.iterrows():\n",
    "        voc_str+=' '.join(list(row)) + ' '\n",
    "    df['AnswerRightEnding'] = trueanswer\n",
    "    return df,make_vocab(preprocess(voc_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import OrderedDict \n",
    "class LastSentenceDataset(Dataset):\n",
    "    def __init__(self,file='story_cloze_data/cloze_test_val__spring2016 - cloze_test_ALL_val.csv',vocab=None,df=None,train=None):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        created_df, created_vocab = load_data(file)\n",
    "        if df is None:\n",
    "            df = created_df\n",
    "        if vocab:\n",
    "            self.vocab = vocab\n",
    "        else:\n",
    "            self.vocab = created_vocab\n",
    "      \n",
    "        \n",
    "        self.dir_st = 'data/skip-thoughts'\n",
    "        self.biskip = BiSkip(self.dir_st, self.vocab.convert_to_list()[1:])\n",
    "        \n",
    "        self.uniskip = UniSkip(self.dir_st, self.vocab.convert_to_list()[1:])\n",
    "        \n",
    "        \n",
    "        self.data = self.make_data(df)\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx\n",
    "        Returns: skip thought embedding of ending and 0/1 if it is the right ending \n",
    "\n",
    "        \"\"\"\n",
    "        return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns len of the dataset\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "       \n",
    "    def make_data(self, df):\n",
    "        data = []\n",
    "        total = df.index\n",
    "        print('skip thought encoding dataset')\n",
    "        for i in total:\n",
    "            #print(row['RandomFifthSentenceQuiz1'],row['RandomFifthSentenceQuiz2'])\n",
    "            progress(i,len(total))\n",
    "            endings =  self.gen_embbeding(df.at[i,'RandomFifthSentenceQuiz1'], \n",
    "                                          df.at[i,'RandomFifthSentenceQuiz2'],\n",
    "                                          df.at[i,'InputSentence4'])\n",
    "            if df.at[i,'AnswerRightEnding'] == 1:\n",
    "                data.append((endings[0].detach().numpy(),1))\n",
    "                data.append((endings[1].detach().numpy(),0))\n",
    "            else:\n",
    "                data.append((endings[0].detach().numpy(),0))\n",
    "                data.append((endings[1].detach().numpy(),1))\n",
    "        return data\n",
    "    \n",
    "\n",
    "    def zero_pad(self,l,n):\n",
    "        l = (l + n * [0])[:n]\n",
    "        return l\n",
    "    \n",
    "    def pad_input(self,d):\n",
    "        d = OrderedDict(sorted(d.items(), key=lambda s: len(s[1])))\n",
    "        for k,v in d.items():\n",
    "            d[k]= self.zero_pad(v,len(list(d.items())[-1][1]))\n",
    "        return d\n",
    "        \n",
    "    def gen_embbeding(self,sent1,sent2,last_sent):\n",
    "        d = dict()\n",
    "        sent1 = preprocess(sent1)\n",
    "        sent2 = preprocess(sent2)\n",
    "        ls = preprocess(last_sent)\n",
    "        #remove random n token that is in one sentence\n",
    "        if 'n' in sent2:\n",
    "            sent2.remove('n')\n",
    "        d['sent1'] = self.vocab.get_sentence(sent1)\n",
    "        d['sent2'] = self.vocab.get_sentence(sent2)\n",
    "        d['ls'] = self.vocab.get_sentence(ls)\n",
    "        d = self.pad_input(d)\n",
    "        \n",
    "        batch = torch.LongTensor([d['sent1'],d['sent2'], d['ls']]) \n",
    "        top_half = self.uniskip(batch)\n",
    "        bottom_half = self.biskip(batch)\n",
    "        combine_skip = torch.cat([top_half,bottom_half],dim=1)\n",
    "        end1 = combine_skip[0]\n",
    "        end2 = combine_skip[1]\n",
    "        ls = combine_skip[2]\n",
    "        \n",
    "        #print(end1[:20],end2[:20])\n",
    "        end1.add_(ls)\n",
    "        end2.add_(ls)\n",
    "        #print('ls',ls[:20])\n",
    "        #print('after',end1[:20])\n",
    "        return end1,end2    \n",
    "    \n",
    "def progress(count, total, status=''):\n",
    "    bar_len = 60\n",
    "    filled_len = int(round(bar_len * count / float(total)))\n",
    "\n",
    "    percents = round(100.0 * count / float(total), 1)\n",
    "    bar = '=' * filled_len + '-' * (bar_len - filled_len)\n",
    "\n",
    "    sys.stdout.write('[%s] %s%s ...%s\\r' % (bar, percents, '%', status))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hd1 = torch.nn.Linear(4800, 2400)\n",
    "        self.hd2 = torch.nn.Linear(2400, 1200)\n",
    "        self.hd3 = torch.nn.Linear(1200, 600)\n",
    "        self.output = torch.nn.Linear(600, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.hd1(x))\n",
    "        x = torch.nn.functional.relu(self.hd2(x))\n",
    "        x = torch.nn.functional.relu(self.hd3(x))\n",
    "        x = self.output(x)\n",
    "        #print('output',x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSShortModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input = torch.nn.Linear(4800,256)\n",
    "        self.hidden= torch.nn.Linear(256,64)\n",
    "        self.output = torch.nn.Linear(64,2)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        hidden = torch.nn.functional.relu(self.input(inputs))\n",
    "        hidden1 = torch.nn.functional.relu(self.hidden(hidden))\n",
    "        output = self.output(hidden1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(pair,model):\n",
    "    '''true if model predicts right'''\n",
    "    ending1, ending2 = pair\n",
    "    if ending1[1] == 1:\n",
    "        target = 1\n",
    "    else:\n",
    "        target =  2 \n",
    "        \n",
    "    ending1 = torch.tensor(ending1[0])\n",
    "    ending2 = torch.tensor(ending2[0])\n",
    "    res1 = model(ending1)\n",
    "    res2 = model(ending2)\n",
    "    softm = torch.nn.Softmax(dim=0)\n",
    "    prob_end1_right = softm(res1)[1].item() \n",
    "    prob_end2_right = softm(res2)[1].item()\n",
    "    \n",
    "    if prob_end1_right > prob_end2_right:\n",
    "        pred = 1\n",
    "    else:\n",
    "        pred = 2\n",
    "    \n",
    "    if pred == target:\n",
    "        return True\n",
    "    else: \n",
    "        #print(prob_end1_right,prob_end2_right,pred,target)\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(data_set,model):\n",
    "    num_right = 0\n",
    "    for i in range(0,len(data_set),2):\n",
    "        if score((data_set[i],data_set[i+1]),model):\n",
    "            num_right+=1\n",
    "    return num_right / (len(data_set)/2)\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(name):\n",
    "    torch.save(model.state_dict(), 'saved_models/{}.save'.format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len is 4851\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df, voc = load_data(file='story_cloze_data/cloze_test_val__winter2018-cloze_test_ALL_val - 1 - 1.csv')\n",
    "train, val = train_test_split(df, test_size=0.1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len is 22443\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df, voc = load_data_train()\n",
    "train, val = train_test_split(df, test_size=0.1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "lr = 0.01\n",
    "num_epochs = 20\n",
    "report_every = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len is 5303\n",
      "Warning: 841/930911 words are not in dictionary, thus set UNK\n",
      "Warning: 841/930911 words are not in dictionary, thus set UNK\n",
      "skip thought encoding dataset\n",
      "[============================================================] 100.0% ...\r"
     ]
    }
   ],
   "source": [
    "train_data_set = LastSentenceDataset(df=train, vocab=voc)\n",
    "data_loader = torch.utils.data.DataLoader(train_data_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1  out of 26004 ids generated were for unk(0.004 %)'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc.unk_ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len is 5303\n",
      "Warning: 841/930911 words are not in dictionary, thus set UNK\n",
      "Warning: 841/930911 words are not in dictionary, thus set UNK\n",
      "skip thought encoding dataset\n",
      "[========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] 999.9% ...\r"
     ]
    }
   ],
   "source": [
    "val_data_set = LastSentenceDataset(df=val,vocab=voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSModel()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Loss 0.6938. accuracy 0.4938. Elapsed 31 seconds\n",
      "Epoch 1. Loss 11.2370. accuracy 0.4938. Elapsed 62 seconds\n",
      "Epoch 1. Loss 67.9420. accuracy 0.5048. Elapsed 93 seconds\n",
      "Epoch 1. Loss 51.9454. accuracy 0.5130. Elapsed 123 seconds\n",
      "Epoch 1. Loss 42.3841. accuracy 0.5053. Elapsed 154 seconds\n",
      "Epoch 1. Loss 35.6061. accuracy 0.4916. Elapsed 184 seconds\n",
      "Epoch 1. Loss 30.6209. accuracy 0.5048. Elapsed 214 seconds\n",
      "Epoch 1. Loss 26.8877. accuracy 0.5165. Elapsed 245 seconds\n",
      "Epoch 1. Loss 24.0031. accuracy 0.5169. Elapsed 275 seconds\n",
      "Epoch 1. Loss 21.6983. accuracy 0.5358. Elapsed 305 seconds\n",
      "Epoch 1. Loss 19.7882. accuracy 0.5407. Elapsed 336 seconds\n",
      "Epoch 1. Loss 18.2169. accuracy 0.5433. Elapsed 366 seconds\n",
      "Epoch 1. Loss 16.8700. accuracy 0.5347. Elapsed 397 seconds\n",
      "Epoch 1. Loss 15.7164. accuracy 0.5290. Elapsed 427 seconds\n",
      "Epoch 1. Loss 14.7160. accuracy 0.5352. Elapsed 457 seconds\n",
      "Epoch 1. Loss 13.8391. accuracy 0.5475. Elapsed 488 seconds\n",
      "Epoch 1. Loss 13.0657. accuracy 0.5398. Elapsed 518 seconds\n",
      "Epoch 1. Loss 12.3797. accuracy 0.5341. Elapsed 548 seconds\n",
      "Epoch 1. Loss 11.7644. accuracy 0.5574. Elapsed 578 seconds\n",
      "Epoch 1. Loss 11.2113. accuracy 0.5459. Elapsed 609 seconds\n",
      "Epoch 1. Loss 10.7145. accuracy 0.5257. Elapsed 639 seconds\n",
      "Epoch 1. Loss 10.2587. accuracy 0.5345. Elapsed 669 seconds\n",
      "Epoch 1. Loss 9.8436. accuracy 0.5244. Elapsed 700 seconds\n",
      "Epoch 1. Loss 9.4625. accuracy 0.5486. Elapsed 730 seconds\n",
      "Epoch 1. Loss 9.1116. accuracy 0.5479. Elapsed 760 seconds\n",
      "Epoch 1. Loss 8.7955. accuracy 0.5501. Elapsed 791 seconds\n",
      "Epoch 1. Loss 8.4955. accuracy 0.5429. Elapsed 821 seconds\n",
      "Epoch 1. Loss 8.2168. accuracy 0.5332. Elapsed 852 seconds\n",
      "Epoch 1. Loss 7.9573. accuracy 0.5477. Elapsed 882 seconds\n",
      "Epoch 1. Loss 7.7152. accuracy 0.5407. Elapsed 912 seconds\n",
      "Epoch 1. Loss 7.4886. accuracy 0.5378. Elapsed 943 seconds\n",
      "Epoch 1. Loss 7.2762. accuracy 0.5292. Elapsed 975 seconds\n",
      "Epoch 1. Loss 7.0767. accuracy 0.5251. Elapsed 1006 seconds\n",
      "Epoch 1. Loss 6.8887. accuracy 0.5262. Elapsed 1037 seconds\n",
      "Epoch 1. Loss 6.7121. accuracy 0.5312. Elapsed 1068 seconds\n",
      "Epoch 1. Loss 6.5452. accuracy 0.5345. Elapsed 1098 seconds\n",
      "Epoch 1. Loss 6.3871. accuracy 0.5224. Elapsed 1128 seconds\n",
      "Epoch 1. Loss 6.2374. accuracy 0.5145. Elapsed 1158 seconds\n",
      "Epoch 1. Loss 6.0953. accuracy 0.5018. Elapsed 1188 seconds\n",
      "Epoch 1. Loss 5.9602. accuracy 0.5000. Elapsed 1219 seconds\n",
      "Epoch 1. Loss 5.8318. accuracy 0.4985. Elapsed 1251 seconds\n",
      "Epoch 1. Loss 5.7095. accuracy 0.5134. Elapsed 1282 seconds\n",
      "Epoch 1. Loss 5.5928. accuracy 0.5257. Elapsed 1313 seconds\n",
      "Epoch 1. Loss 5.4816. accuracy 0.5268. Elapsed 1345 seconds\n",
      "Epoch 1. Loss 5.3752. accuracy 0.5149. Elapsed 1377 seconds\n",
      "Epoch 1. Loss 5.2733. accuracy 0.5279. Elapsed 1409 seconds\n",
      "Epoch 1. Loss 5.1760. accuracy 0.5380. Elapsed 1440 seconds\n",
      "Epoch 1. Loss 5.0826. accuracy 0.5402. Elapsed 1471 seconds\n",
      "Epoch 1. Loss 4.9930. accuracy 0.5429. Elapsed 1503 seconds\n",
      "Epoch 1. Loss 4.9068. accuracy 0.5371. Elapsed 1535 seconds\n",
      "Epoch 1. Loss 4.8248. accuracy 0.5165. Elapsed 1567 seconds\n",
      "Epoch 1. Loss 4.7453. accuracy 0.4996. Elapsed 1600 seconds\n",
      "Epoch 1. Loss 4.6688. accuracy 0.5024. Elapsed 1633 seconds\n",
      "Epoch 1. Loss 4.5951. accuracy 0.5051. Elapsed 1666 seconds\n",
      "Epoch 1. Loss 4.5241. accuracy 0.5149. Elapsed 1698 seconds\n",
      "Epoch 1. Loss 4.4556. accuracy 0.5149. Elapsed 1732 seconds\n",
      "Epoch 1. Loss 4.3898. accuracy 0.5180. Elapsed 1763 seconds\n",
      "Epoch 1. Loss 4.3260. accuracy 0.5218. Elapsed 1795 seconds\n",
      "Epoch 1. Loss 4.2642. accuracy 0.5222. Elapsed 1828 seconds\n",
      "Epoch 1. Loss 4.2046. accuracy 0.5334. Elapsed 1862 seconds\n",
      "Epoch 1. Loss 4.1471. accuracy 0.5222. Elapsed 1893 seconds\n",
      "Epoch 1. Loss 4.0916. accuracy 0.5160. Elapsed 1923 seconds\n",
      "Epoch 1. Loss 4.0376. accuracy 0.5191. Elapsed 1954 seconds\n",
      "Epoch 1. Loss 3.9853. accuracy 0.5178. Elapsed 1984 seconds\n",
      "Epoch 1. Loss 3.9348. accuracy 0.5253. Elapsed 2014 seconds\n",
      "Epoch 1. Loss 3.8857. accuracy 0.5301. Elapsed 2046 seconds\n",
      "Epoch 1. Loss 3.8380. accuracy 0.5213. Elapsed 2077 seconds\n",
      "Epoch 1. Loss 3.7921. accuracy 0.5371. Elapsed 2110 seconds\n",
      "Epoch 1. Loss 3.7472. accuracy 0.5534. Elapsed 2140 seconds\n",
      "Epoch 1. Loss 3.7036. accuracy 0.5336. Elapsed 2170 seconds\n",
      "Epoch 1. Loss 3.6612. accuracy 0.5134. Elapsed 2200 seconds\n",
      "Epoch 1. Loss 3.6201. accuracy 0.5240. Elapsed 2230 seconds\n",
      "Epoch 1. Loss 3.5800. accuracy 0.5275. Elapsed 2262 seconds\n",
      "Epoch 1. Loss 3.5410. accuracy 0.5295. Elapsed 2292 seconds\n",
      "Epoch 1. Loss 3.5030. accuracy 0.5244. Elapsed 2322 seconds\n",
      "Epoch 1. Loss 3.4660. accuracy 0.5290. Elapsed 2352 seconds\n",
      "Epoch 1. Loss 3.4300. accuracy 0.5319. Elapsed 2383 seconds\n",
      "Epoch 1. Loss 3.3948. accuracy 0.5332. Elapsed 2418 seconds\n",
      "Epoch 1. Loss 3.3607. accuracy 0.5185. Elapsed 2456 seconds\n",
      "Epoch 1. Loss 3.3279. accuracy 0.5295. Elapsed 2486 seconds\n",
      "Epoch 1. Loss 3.2953. accuracy 0.5341. Elapsed 2519 seconds\n",
      "Epoch 1. Loss 3.2636. accuracy 0.5462. Elapsed 2550 seconds\n",
      "Epoch 1. Loss 3.2326. accuracy 0.5527. Elapsed 2583 seconds\n",
      "Epoch 1. Loss 3.2023. accuracy 0.5501. Elapsed 2618 seconds\n",
      "Epoch 1. Loss 3.1727. accuracy 0.5453. Elapsed 2650 seconds\n",
      "Epoch 1. Loss 3.1439. accuracy 0.5358. Elapsed 2680 seconds\n",
      "Epoch 1. Loss 3.1157. accuracy 0.5398. Elapsed 2715 seconds\n",
      "Epoch 1. Loss 3.0881. accuracy 0.5457. Elapsed 2750 seconds\n",
      "Epoch 1. Loss 3.0612. accuracy 0.5492. Elapsed 2784 seconds\n",
      "Epoch 1. Loss 3.0349. accuracy 0.5613. Elapsed 2816 seconds\n",
      "Epoch 1. Loss 3.0091. accuracy 0.5782. Elapsed 2847 seconds\n",
      "Epoch 1. Loss 2.9838. accuracy 0.5692. Elapsed 2878 seconds\n",
      "Epoch 1. Loss 2.9590. accuracy 0.5560. Elapsed 2909 seconds\n",
      "Epoch 1. Loss 2.9350. accuracy 0.5571. Elapsed 2944 seconds\n",
      "Epoch 1. Loss 2.9112. accuracy 0.5719. Elapsed 2978 seconds\n",
      "Epoch 1. Loss 2.8880. accuracy 0.5675. Elapsed 3011 seconds\n",
      "Epoch 1. Loss 2.8656. accuracy 0.5815. Elapsed 3043 seconds\n",
      "Epoch 1. Loss 2.8435. accuracy 0.5818. Elapsed 3076 seconds\n",
      "Epoch 1. Loss 2.8219. accuracy 0.5853. Elapsed 3111 seconds\n",
      "Epoch 1. Loss 2.8005. accuracy 0.5545. Elapsed 3146 seconds\n",
      "Epoch 1. Loss 2.7797. accuracy 0.5312. Elapsed 3178 seconds\n",
      "Epoch 1. Loss 2.7594. accuracy 0.5202. Elapsed 3210 seconds\n",
      "Epoch 1. Loss 2.7393. accuracy 0.5064. Elapsed 3241 seconds\n",
      "Epoch 1. Loss 2.7197. accuracy 0.5207. Elapsed 3272 seconds\n",
      "Epoch 1. Loss 2.7004. accuracy 0.5396. Elapsed 3303 seconds\n",
      "Epoch 1. Loss 2.6815. accuracy 0.5415. Elapsed 3334 seconds\n",
      "Epoch 1. Loss 2.6629. accuracy 0.5400. Elapsed 3367 seconds\n",
      "Epoch 1. Loss 2.6446. accuracy 0.5473. Elapsed 3399 seconds\n",
      "Epoch 1. Loss 2.6267. accuracy 0.5536. Elapsed 3431 seconds\n",
      "Epoch 1. Loss 2.6091. accuracy 0.5360. Elapsed 3463 seconds\n",
      "Epoch 1. Loss 2.5918. accuracy 0.5262. Elapsed 3495 seconds\n",
      "Epoch 1. Loss 2.5749. accuracy 0.5171. Elapsed 3526 seconds\n",
      "Epoch 1. Loss 2.5583. accuracy 0.5033. Elapsed 3558 seconds\n",
      "Epoch 1. Loss 2.5418. accuracy 0.4897. Elapsed 3590 seconds\n",
      "Epoch 1. Loss 2.5259. accuracy 0.4897. Elapsed 3622 seconds\n",
      "Epoch 1. Loss 2.5101. accuracy 0.5031. Elapsed 3655 seconds\n",
      "Epoch 1. Loss 2.4946. accuracy 0.5253. Elapsed 3689 seconds\n",
      "Epoch 1. Loss 2.4793. accuracy 0.5327. Elapsed 3723 seconds\n",
      "Epoch 1. Loss 2.4643. accuracy 0.5486. Elapsed 3757 seconds\n",
      "Epoch 1. Loss 2.4496. accuracy 0.5492. Elapsed 3793 seconds\n",
      "Epoch 1. Loss 2.4351. accuracy 0.5525. Elapsed 3826 seconds\n",
      "Epoch 1. Loss 2.4208. accuracy 0.5473. Elapsed 3864 seconds\n",
      "Epoch 1. Loss 2.4067. accuracy 0.5499. Elapsed 3905 seconds\n",
      "Epoch 1. Loss 2.3929. accuracy 0.5490. Elapsed 3939 seconds\n",
      "Epoch 1. Loss 2.3793. accuracy 0.5393. Elapsed 3973 seconds\n",
      "Epoch 1. Loss 2.3659. accuracy 0.5407. Elapsed 4007 seconds\n",
      "Epoch 1. Loss 2.3527. accuracy 0.5270. Elapsed 4041 seconds\n",
      "Epoch 1. Loss 2.3397. accuracy 0.5268. Elapsed 4074 seconds\n",
      "Epoch 1. Loss 2.3269. accuracy 0.5396. Elapsed 4107 seconds\n",
      "Epoch 1. Loss 2.3144. accuracy 0.5387. Elapsed 4140 seconds\n",
      "Epoch 1. Loss 2.3019. accuracy 0.5582. Elapsed 4173 seconds\n",
      "Epoch 1. Loss 2.2896. accuracy 0.5719. Elapsed 4205 seconds\n",
      "Epoch 1. Loss 2.2777. accuracy 0.5714. Elapsed 4238 seconds\n",
      "Epoch 1. Loss 2.2657. accuracy 0.5758. Elapsed 4270 seconds\n",
      "Epoch 1. Loss 2.2540. accuracy 0.5826. Elapsed 4303 seconds\n",
      "Epoch 1. Loss 2.2426. accuracy 0.5870. Elapsed 4336 seconds\n",
      "Epoch 1. Loss 2.2312. accuracy 0.5870. Elapsed 4368 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Loss 2.2201. accuracy 0.5840. Elapsed 4401 seconds\n",
      "Epoch 1. Loss 2.2091. accuracy 0.5736. Elapsed 4435 seconds\n",
      "Epoch 1. Loss 2.1982. accuracy 0.5714. Elapsed 4467 seconds\n",
      "Epoch 1. Loss 2.1875. accuracy 0.5730. Elapsed 4500 seconds\n",
      "Epoch 1. Loss 2.1770. accuracy 0.5778. Elapsed 4533 seconds\n",
      "Epoch 1. Loss 2.1666. accuracy 0.5771. Elapsed 4565 seconds\n",
      "Epoch 1. Loss 2.1562. accuracy 0.5776. Elapsed 4600 seconds\n",
      "Epoch 1. Loss 2.1461. accuracy 0.5776. Elapsed 4632 seconds\n",
      "Epoch 1. Loss 2.1361. accuracy 0.5813. Elapsed 4665 seconds\n",
      "Epoch 1. Loss 2.1263. accuracy 0.5818. Elapsed 4698 seconds\n",
      "Epoch 1. Loss 2.1166. accuracy 0.5851. Elapsed 4730 seconds\n",
      "Epoch 1. Loss 2.1070. accuracy 0.5714. Elapsed 4761 seconds\n",
      "Epoch 1. Loss 2.0976. accuracy 0.5697. Elapsed 4794 seconds\n",
      "Epoch 1. Loss 2.0881. accuracy 0.5668. Elapsed 4827 seconds\n",
      "Epoch 1. Loss 2.0788. accuracy 0.5800. Elapsed 4859 seconds\n",
      "Epoch 1. Loss 2.0696. accuracy 0.5789. Elapsed 4892 seconds\n",
      "Epoch 1. Loss 2.0606. accuracy 0.5752. Elapsed 4924 seconds\n",
      "Epoch 1. Loss 2.0519. accuracy 0.5710. Elapsed 4957 seconds\n",
      "Epoch 1. Loss 2.0432. accuracy 0.5695. Elapsed 4990 seconds\n",
      "Epoch 1. Loss 2.0347. accuracy 0.5848. Elapsed 5022 seconds\n",
      "Epoch 1. Loss 2.0262. accuracy 0.5820. Elapsed 5054 seconds\n",
      "Epoch 1. Loss 2.0178. accuracy 0.5765. Elapsed 5087 seconds\n",
      "Epoch 1. Loss 2.0095. accuracy 0.5664. Elapsed 5121 seconds\n",
      "Epoch 1. Loss 2.0012. accuracy 0.5631. Elapsed 5155 seconds\n",
      "Epoch 1. Loss 1.9932. accuracy 0.5589. Elapsed 5189 seconds\n",
      "Epoch 1. Loss 1.9851. accuracy 0.5521. Elapsed 5221 seconds\n",
      "Epoch 1. Loss 1.9773. accuracy 0.5468. Elapsed 5254 seconds\n",
      "Epoch 1. Loss 1.9695. accuracy 0.5451. Elapsed 5289 seconds\n",
      "Epoch 1. Loss 1.9618. accuracy 0.5497. Elapsed 5322 seconds\n",
      "Epoch 1. Loss 1.9542. accuracy 0.5525. Elapsed 5353 seconds\n",
      "Epoch 1. Loss 1.9467. accuracy 0.5545. Elapsed 5387 seconds\n",
      "Epoch 1. Loss 1.9393. accuracy 0.5510. Elapsed 5419 seconds\n",
      "Epoch 1. Loss 1.9318. accuracy 0.5477. Elapsed 5449 seconds\n",
      "Epoch 1. Loss 1.9245. accuracy 0.5565. Elapsed 5480 seconds\n",
      "Epoch 1. Loss 1.9173. accuracy 0.5538. Elapsed 5510 seconds\n",
      "Epoch 1. Loss 1.9103. accuracy 0.5607. Elapsed 5541 seconds\n",
      "Epoch 1. Loss 1.9033. accuracy 0.5549. Elapsed 5571 seconds\n",
      "Epoch 1. Loss 1.8963. accuracy 0.5521. Elapsed 5601 seconds\n",
      "Epoch 1. Loss 1.8894. accuracy 0.5556. Elapsed 5631 seconds\n",
      "Epoch 1. Loss 1.8826. accuracy 0.5545. Elapsed 5661 seconds\n",
      "Epoch 1. Loss 1.8760. accuracy 0.5536. Elapsed 5691 seconds\n",
      "Epoch 1. Loss 1.8693. accuracy 0.5431. Elapsed 5721 seconds\n",
      "Epoch 1. Loss 1.8628. accuracy 0.5385. Elapsed 5751 seconds\n",
      "Epoch 1. Loss 1.8563. accuracy 0.5422. Elapsed 5781 seconds\n",
      "Epoch 1. Loss 1.8499. accuracy 0.5435. Elapsed 5811 seconds\n",
      "Epoch 1. Loss 1.8436. accuracy 0.5607. Elapsed 5843 seconds\n",
      "Epoch 1. Loss 1.8373. accuracy 0.5615. Elapsed 5874 seconds\n",
      "Epoch 1. Loss 1.8311. accuracy 0.5598. Elapsed 5904 seconds\n",
      "Epoch 1. Loss 1.8249. accuracy 0.5648. Elapsed 5934 seconds\n",
      "Epoch 1. Loss 1.8189. accuracy 0.5646. Elapsed 5964 seconds\n",
      "Epoch 1. Loss 1.8129. accuracy 0.5659. Elapsed 5994 seconds\n",
      "Epoch 1. Loss 1.8069. accuracy 0.5545. Elapsed 6024 seconds\n",
      "Epoch 1. Loss 1.8010. accuracy 0.5554. Elapsed 6054 seconds\n",
      "Epoch 1. Loss 1.7952. accuracy 0.5710. Elapsed 6084 seconds\n",
      "Epoch 1. Loss 1.7894. accuracy 0.5756. Elapsed 6113 seconds\n",
      "Epoch 1. Loss 1.7837. accuracy 0.5705. Elapsed 6143 seconds\n",
      "Epoch 1. Loss 1.7778. accuracy 0.5701. Elapsed 6175 seconds\n",
      "Epoch 1. Loss 1.7721. accuracy 0.5708. Elapsed 6206 seconds\n",
      "Epoch 1. Loss 1.7666. accuracy 0.5747. Elapsed 6236 seconds\n",
      "Epoch 1. Loss 1.7611. accuracy 0.5782. Elapsed 6266 seconds\n",
      "Epoch 1. Loss 1.7557. accuracy 0.5787. Elapsed 6295 seconds\n",
      "Epoch 1. Loss 1.7504. accuracy 0.5833. Elapsed 6325 seconds\n",
      "Epoch 1. Loss 1.7451. accuracy 0.5848. Elapsed 6354 seconds\n",
      "Epoch 1. Loss 1.7399. accuracy 0.5862. Elapsed 6384 seconds\n",
      "Epoch 1. Loss 1.7346. accuracy 0.5785. Elapsed 6414 seconds\n",
      "Epoch 1. Loss 1.7295. accuracy 0.5796. Elapsed 6443 seconds\n",
      "Epoch 1. Loss 1.7244. accuracy 0.5813. Elapsed 6473 seconds\n",
      "Epoch 1. Loss 1.7193. accuracy 0.5824. Elapsed 6503 seconds\n",
      "Epoch 1. Loss 1.7143. accuracy 0.5807. Elapsed 6534 seconds\n",
      "Epoch 1. Loss 1.7093. accuracy 0.5714. Elapsed 6564 seconds\n",
      "Epoch 1. Loss 1.7044. accuracy 0.5677. Elapsed 6594 seconds\n",
      "Epoch 1. Loss 1.6995. accuracy 0.5708. Elapsed 6623 seconds\n",
      "Epoch 1. Loss 1.6946. accuracy 0.5747. Elapsed 6653 seconds\n",
      "Epoch 1. Loss 1.6898. accuracy 0.5813. Elapsed 6685 seconds\n",
      "Epoch 1. Loss 1.6851. accuracy 0.5826. Elapsed 6716 seconds\n",
      "Epoch 1. Loss 1.6804. accuracy 0.5829. Elapsed 6746 seconds\n",
      "Epoch 1. Loss 1.6758. accuracy 0.5809. Elapsed 6777 seconds\n",
      "Epoch 1. Loss 1.6711. accuracy 0.5842. Elapsed 6808 seconds\n",
      "Epoch 1. Loss 1.6666. accuracy 0.5767. Elapsed 6837 seconds\n",
      "Epoch 1. Loss 1.6620. accuracy 0.5780. Elapsed 6868 seconds\n",
      "Epoch 1. Loss 1.6575. accuracy 0.5837. Elapsed 6899 seconds\n",
      "Epoch 1. Loss 1.6529. accuracy 0.5877. Elapsed 6932 seconds\n",
      "Epoch 1. Loss 1.6485. accuracy 0.5873. Elapsed 6962 seconds\n",
      "Epoch 1. Loss 1.6443. accuracy 0.5831. Elapsed 6992 seconds\n",
      "Epoch 1. Loss 1.6410. accuracy 0.5807. Elapsed 7022 seconds\n",
      "Epoch 1. Loss 1.6367. accuracy 0.5725. Elapsed 7053 seconds\n",
      "Epoch 1. Loss 1.6323. accuracy 0.5723. Elapsed 7084 seconds\n",
      "Epoch 1. Loss 1.6281. accuracy 0.5565. Elapsed 7115 seconds\n",
      "Epoch 1. Loss 1.6240. accuracy 0.5437. Elapsed 7146 seconds\n",
      "Epoch 1. Loss 1.6199. accuracy 0.5527. Elapsed 7177 seconds\n",
      "Epoch 1. Loss 1.6158. accuracy 0.5716. Elapsed 7209 seconds\n",
      "Epoch 1. Loss 1.6117. accuracy 0.5716. Elapsed 7241 seconds\n",
      "Epoch 1. Loss 1.6077. accuracy 0.5705. Elapsed 7273 seconds\n",
      "Epoch 1. Loss 1.6038. accuracy 0.5695. Elapsed 7304 seconds\n",
      "Epoch 1. Loss 1.5997. accuracy 0.5705. Elapsed 7335 seconds\n",
      "Epoch 1. Loss 1.5958. accuracy 0.5578. Elapsed 7367 seconds\n",
      "Epoch 1. Loss 1.5919. accuracy 0.5712. Elapsed 7398 seconds\n",
      "Epoch 1. Loss 1.5881. accuracy 0.5754. Elapsed 7430 seconds\n",
      "Epoch 1. Loss 1.5842. accuracy 0.5767. Elapsed 7461 seconds\n",
      "Epoch 1. Loss 1.5804. accuracy 0.5732. Elapsed 7492 seconds\n",
      "Epoch 1. Loss 1.5765. accuracy 0.5741. Elapsed 7523 seconds\n",
      "Epoch 1. Loss 1.5728. accuracy 0.5727. Elapsed 7554 seconds\n",
      "Epoch 1. Loss 1.5691. accuracy 0.5723. Elapsed 7586 seconds\n",
      "Epoch 1. Loss 1.5654. accuracy 0.5778. Elapsed 7617 seconds\n",
      "Epoch 1. Loss 1.5617. accuracy 0.5916. Elapsed 7648 seconds\n",
      "Epoch 1. Loss 1.5581. accuracy 0.5903. Elapsed 7678 seconds\n",
      "Epoch 1. Loss 1.5546. accuracy 0.5985. Elapsed 7708 seconds\n",
      "Epoch 1. Loss 1.5509. accuracy 0.5945. Elapsed 7738 seconds\n",
      "Epoch 1. Loss 1.5473. accuracy 0.5949. Elapsed 7767 seconds\n",
      "Epoch 1. Loss 1.5438. accuracy 0.5952. Elapsed 7797 seconds\n",
      "Epoch 1. Loss 1.5404. accuracy 0.5969. Elapsed 7827 seconds\n",
      "Epoch 1. Loss 1.5369. accuracy 0.5982. Elapsed 7857 seconds\n",
      "Epoch 1. Loss 1.5335. accuracy 0.5945. Elapsed 7886 seconds\n",
      "Epoch 1. Loss 1.5301. accuracy 0.5989. Elapsed 7916 seconds\n",
      "Epoch 1. Loss 1.5267. accuracy 0.5934. Elapsed 7946 seconds\n",
      "Epoch 1. Loss 1.5234. accuracy 0.5914. Elapsed 7975 seconds\n",
      "Epoch 1. Loss 1.5201. accuracy 0.5936. Elapsed 8005 seconds\n",
      "Epoch 1. Loss 1.5168. accuracy 0.5943. Elapsed 8035 seconds\n",
      "Epoch 1. Loss 1.5136. accuracy 0.5963. Elapsed 8064 seconds\n",
      "Epoch 1. Loss 1.5105. accuracy 0.5892. Elapsed 8094 seconds\n",
      "Epoch 1. Loss 1.5073. accuracy 0.5892. Elapsed 8123 seconds\n",
      "Epoch 1. Loss 1.5041. accuracy 0.5862. Elapsed 8153 seconds\n",
      "Epoch 1. Loss 1.5009. accuracy 0.5888. Elapsed 8183 seconds\n",
      "Epoch 1. Loss 1.4978. accuracy 0.5870. Elapsed 8212 seconds\n",
      "Epoch 1. Loss 1.4947. accuracy 0.5862. Elapsed 8242 seconds\n",
      "Epoch 1. Loss 1.4916. accuracy 0.5862. Elapsed 8271 seconds\n",
      "Epoch 1. Loss 1.4885. accuracy 0.5864. Elapsed 8301 seconds\n",
      "Epoch 1. Loss 1.4855. accuracy 0.5862. Elapsed 8330 seconds\n",
      "Epoch 1. Loss 1.4824. accuracy 0.5897. Elapsed 8360 seconds\n",
      "Epoch 1. Loss 1.4794. accuracy 0.5923. Elapsed 8390 seconds\n",
      "Epoch 1. Loss 1.4766. accuracy 0.5864. Elapsed 8420 seconds\n",
      "Epoch 1. Loss 1.4737. accuracy 0.5873. Elapsed 8449 seconds\n",
      "Epoch 1. Loss 1.4707. accuracy 0.5848. Elapsed 8479 seconds\n",
      "Epoch 1. Loss 1.4679. accuracy 0.5831. Elapsed 8509 seconds\n",
      "Epoch 1. Loss 1.4650. accuracy 0.5780. Elapsed 8538 seconds\n",
      "Epoch 1. Loss 1.4621. accuracy 0.5776. Elapsed 8568 seconds\n",
      "Epoch 1. Loss 1.4593. accuracy 0.5774. Elapsed 8597 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Loss 1.4564. accuracy 0.5774. Elapsed 8627 seconds\n",
      "Epoch 1. Loss 1.4536. accuracy 0.5778. Elapsed 8657 seconds\n",
      "Epoch 1. Loss 1.4510. accuracy 0.5782. Elapsed 8686 seconds\n",
      "Epoch 1. Loss 1.4482. accuracy 0.5798. Elapsed 8716 seconds\n",
      "Epoch 1. Loss 1.4454. accuracy 0.5824. Elapsed 8746 seconds\n",
      "Epoch 1. Loss 1.4427. accuracy 0.5855. Elapsed 8775 seconds\n",
      "Epoch 1. Loss 1.4400. accuracy 0.5844. Elapsed 8805 seconds\n",
      "Epoch 1. Loss 1.4374. accuracy 0.5842. Elapsed 8835 seconds\n",
      "Epoch 1. Loss 1.4347. accuracy 0.5826. Elapsed 8864 seconds\n",
      "Epoch 1. Loss 1.4321. accuracy 0.5815. Elapsed 8894 seconds\n",
      "Epoch 1. Loss 1.4295. accuracy 0.5837. Elapsed 8923 seconds\n",
      "Epoch 1. Loss 1.4269. accuracy 0.5815. Elapsed 8953 seconds\n",
      "Epoch 1. Loss 1.4243. accuracy 0.5820. Elapsed 8983 seconds\n",
      "Epoch 1. Loss 1.4218. accuracy 0.5846. Elapsed 9013 seconds\n",
      "Epoch 1. Loss 1.4192. accuracy 0.5844. Elapsed 9042 seconds\n",
      "Epoch 1. Loss 1.4166. accuracy 0.5837. Elapsed 9072 seconds\n",
      "Epoch 1. Loss 1.4141. accuracy 0.5844. Elapsed 9102 seconds\n",
      "Epoch 1. Loss 1.4116. accuracy 0.5842. Elapsed 9131 seconds\n",
      "Epoch 1. Loss 1.4091. accuracy 0.5879. Elapsed 9161 seconds\n",
      "Epoch 1. Loss 1.4067. accuracy 0.5921. Elapsed 9192 seconds\n",
      "Epoch 1. Loss 1.4042. accuracy 0.5899. Elapsed 9223 seconds\n",
      "Epoch 1. Loss 1.4018. accuracy 0.5905. Elapsed 9256 seconds\n",
      "Epoch 1. Loss 1.3994. accuracy 0.5914. Elapsed 9289 seconds\n",
      "Epoch 1. Loss 1.3969. accuracy 0.5916. Elapsed 9321 seconds\n",
      "Epoch 1. Loss 1.3945. accuracy 0.5938. Elapsed 9353 seconds\n",
      "Epoch 1. Loss 1.3922. accuracy 0.5976. Elapsed 9384 seconds\n",
      "Epoch 1. Loss 1.3898. accuracy 0.5954. Elapsed 9416 seconds\n",
      "Epoch 1. Loss 1.3875. accuracy 0.5936. Elapsed 9447 seconds\n",
      "Epoch 1. Loss 1.3851. accuracy 0.5934. Elapsed 9477 seconds\n",
      "Epoch 1. Loss 1.3828. accuracy 0.5903. Elapsed 9507 seconds\n",
      "Epoch 1. Loss 1.3805. accuracy 0.5813. Elapsed 9538 seconds\n",
      "Epoch 1. Loss 1.3782. accuracy 0.5866. Elapsed 9568 seconds\n",
      "Epoch 1. Loss 1.3759. accuracy 0.5886. Elapsed 9601 seconds\n",
      "Epoch 1. Loss 1.3737. accuracy 0.5905. Elapsed 9634 seconds\n",
      "Epoch 1. Loss 1.3714. accuracy 0.5971. Elapsed 9666 seconds\n",
      "Epoch 1. Loss 1.3692. accuracy 0.5976. Elapsed 9698 seconds\n",
      "Epoch 1. Loss 1.3669. accuracy 0.5945. Elapsed 9730 seconds\n",
      "Epoch 1. Loss 1.3647. accuracy 0.6123. Elapsed 9761 seconds\n",
      "Epoch 1. Loss 1.3626. accuracy 0.6088. Elapsed 9793 seconds\n",
      "Epoch 1. Loss 1.3603. accuracy 0.6077. Elapsed 9824 seconds\n",
      "Epoch 1. Loss 1.3582. accuracy 0.6075. Elapsed 9855 seconds\n",
      "Epoch 1. Loss 1.3560. accuracy 0.6064. Elapsed 9886 seconds\n",
      "Epoch 1. Loss 1.3539. accuracy 0.6095. Elapsed 9917 seconds\n",
      "Epoch 1. Loss 1.3518. accuracy 0.6130. Elapsed 9948 seconds\n",
      "Epoch 1. Loss 1.3496. accuracy 0.6112. Elapsed 9979 seconds\n",
      "Epoch 1. Loss 1.3476. accuracy 0.6095. Elapsed 10011 seconds\n",
      "Epoch 1. Loss 1.3456. accuracy 0.6035. Elapsed 10042 seconds\n",
      "Epoch 1. Loss 1.3436. accuracy 0.6062. Elapsed 10073 seconds\n",
      "Epoch 1. Loss 1.3415. accuracy 0.6051. Elapsed 10103 seconds\n",
      "Epoch 1. Loss 1.3395. accuracy 0.6022. Elapsed 10134 seconds\n",
      "Epoch 1. Loss 1.3375. accuracy 0.6007. Elapsed 10164 seconds\n",
      "Epoch 1. Loss 1.3354. accuracy 0.6013. Elapsed 10195 seconds\n",
      "Epoch 1. Loss 1.3334. accuracy 0.5998. Elapsed 10226 seconds\n",
      "Epoch 1. Loss 1.3314. accuracy 0.6099. Elapsed 10257 seconds\n",
      "Epoch 1. Loss 1.3294. accuracy 0.6090. Elapsed 10287 seconds\n",
      "Epoch 1. Loss 1.3275. accuracy 0.6114. Elapsed 10318 seconds\n",
      "Epoch 1. Loss 1.3256. accuracy 0.6180. Elapsed 10349 seconds\n",
      "Epoch 1. Loss 1.3236. accuracy 0.6268. Elapsed 10379 seconds\n",
      "Epoch 1. Loss 1.3217. accuracy 0.6288. Elapsed 10410 seconds\n",
      "Epoch 1. Loss 1.3198. accuracy 0.6275. Elapsed 10441 seconds\n",
      "Epoch 1. Loss 1.3178. accuracy 0.6193. Elapsed 10471 seconds\n",
      "Epoch 1. Loss 1.3159. accuracy 0.6160. Elapsed 10502 seconds\n",
      "Epoch 1. Loss 1.3140. accuracy 0.6200. Elapsed 10532 seconds\n",
      "Epoch 1. Loss 1.3121. accuracy 0.6284. Elapsed 10563 seconds\n",
      "Epoch 1. Loss 1.3103. accuracy 0.6308. Elapsed 10594 seconds\n",
      "Epoch 1. Loss 1.3084. accuracy 0.6215. Elapsed 10624 seconds\n",
      "Epoch 1. Loss 1.3066. accuracy 0.6200. Elapsed 10655 seconds\n",
      "Epoch 1. Loss 1.3048. accuracy 0.6040. Elapsed 10686 seconds\n",
      "Epoch 1. Loss 1.3030. accuracy 0.5936. Elapsed 10716 seconds\n",
      "Epoch 1. Loss 1.3012. accuracy 0.5930. Elapsed 10747 seconds\n",
      "Epoch 1. Loss 1.2994. accuracy 0.5996. Elapsed 10778 seconds\n",
      "Epoch 1. Loss 1.2976. accuracy 0.6130. Elapsed 10809 seconds\n",
      "Epoch 1. Loss 1.2958. accuracy 0.6233. Elapsed 10839 seconds\n",
      "Epoch 1. Loss 1.2941. accuracy 0.6286. Elapsed 10870 seconds\n",
      "Epoch 1. Loss 1.2923. accuracy 0.6295. Elapsed 10901 seconds\n",
      "Epoch 1. Loss 1.2905. accuracy 0.6284. Elapsed 10931 seconds\n",
      "Epoch 1. Loss 1.2888. accuracy 0.6290. Elapsed 10962 seconds\n",
      "Epoch 1. Loss 1.2870. accuracy 0.6308. Elapsed 10993 seconds\n",
      "Epoch 1. Loss 1.2853. accuracy 0.6297. Elapsed 11023 seconds\n",
      "Epoch 1. Loss 1.2836. accuracy 0.6189. Elapsed 11054 seconds\n",
      "Epoch 1. Loss 1.2819. accuracy 0.6202. Elapsed 11085 seconds\n",
      "Epoch 1. Loss 1.2802. accuracy 0.6229. Elapsed 11115 seconds\n",
      "Epoch 1. Loss 1.2786. accuracy 0.6163. Elapsed 11146 seconds\n",
      "Epoch 1. Loss 1.2769. accuracy 0.6213. Elapsed 11177 seconds\n",
      "Epoch 1. Loss 1.2752. accuracy 0.6292. Elapsed 11209 seconds\n",
      "Epoch 1. Loss 1.2736. accuracy 0.6286. Elapsed 11240 seconds\n",
      "Epoch 1. Loss 1.2719. accuracy 0.6308. Elapsed 11270 seconds\n",
      "Epoch 1. Loss 1.2702. accuracy 0.6220. Elapsed 11304 seconds\n",
      "Epoch 1. Loss 1.2685. accuracy 0.6292. Elapsed 11335 seconds\n",
      "Epoch 1. Loss 1.2668. accuracy 0.6303. Elapsed 11369 seconds\n",
      "Epoch 1. Loss 1.2652. accuracy 0.6246. Elapsed 11400 seconds\n",
      "Epoch 1. Loss 1.2636. accuracy 0.6218. Elapsed 11430 seconds\n",
      "Epoch 1. Loss 1.2620. accuracy 0.6158. Elapsed 11460 seconds\n",
      "Epoch 1. Loss 1.2604. accuracy 0.6191. Elapsed 11493 seconds\n",
      "Epoch 1. Loss 1.2587. accuracy 0.6176. Elapsed 11523 seconds\n",
      "Epoch 1. Loss 1.2572. accuracy 0.6211. Elapsed 11555 seconds\n",
      "Epoch 1. Loss 1.2556. accuracy 0.6105. Elapsed 11585 seconds\n",
      "Epoch 1. Loss 1.2540. accuracy 0.6145. Elapsed 11617 seconds\n",
      "Epoch 1. Loss 1.2526. accuracy 0.6167. Elapsed 11648 seconds\n",
      "Epoch 1. Loss 1.2511. accuracy 0.6224. Elapsed 11680 seconds\n",
      "Epoch 1. Loss 1.2496. accuracy 0.6116. Elapsed 11711 seconds\n",
      "Epoch 1. Loss 1.2481. accuracy 0.5908. Elapsed 11742 seconds\n",
      "Epoch 1. Loss 1.2466. accuracy 0.5807. Elapsed 11774 seconds\n",
      "Epoch 1. Loss 1.2451. accuracy 0.5868. Elapsed 11805 seconds\n",
      "Epoch 1. Loss 1.2436. accuracy 0.5912. Elapsed 11836 seconds\n",
      "Epoch 1. Loss 1.2421. accuracy 0.6013. Elapsed 11867 seconds\n",
      "Epoch 1. Loss 1.2406. accuracy 0.6075. Elapsed 11898 seconds\n",
      "Epoch 1. Loss 1.2392. accuracy 0.6125. Elapsed 11930 seconds\n",
      "Epoch 1. Loss 1.2377. accuracy 0.6207. Elapsed 11961 seconds\n",
      "Epoch 1. Loss 1.2363. accuracy 0.6226. Elapsed 11991 seconds\n",
      "Epoch 1. Loss 1.2348. accuracy 0.6253. Elapsed 12023 seconds\n",
      "Epoch 1. Loss 1.2333. accuracy 0.6288. Elapsed 12054 seconds\n",
      "Epoch 1. Loss 1.2319. accuracy 0.6323. Elapsed 12084 seconds\n",
      "Epoch 1. Loss 1.2305. accuracy 0.6290. Elapsed 12115 seconds\n",
      "Epoch 1. Loss 1.2290. accuracy 0.6116. Elapsed 12146 seconds\n",
      "Epoch 1. Loss 1.2276. accuracy 0.6132. Elapsed 12176 seconds\n",
      "Epoch 1. Loss 1.2261. accuracy 0.6196. Elapsed 12207 seconds\n",
      "Epoch 1. Loss 1.2247. accuracy 0.6286. Elapsed 12238 seconds\n",
      "Epoch 1. Loss 1.2234. accuracy 0.6358. Elapsed 12269 seconds\n",
      "Epoch 1. Loss 1.2220. accuracy 0.6404. Elapsed 12299 seconds\n",
      "Epoch 1. Loss 1.2206. accuracy 0.6429. Elapsed 12330 seconds\n",
      "Epoch 1. Loss 1.2192. accuracy 0.6431. Elapsed 12361 seconds\n",
      "Epoch 1. Loss 1.2178. accuracy 0.6470. Elapsed 12392 seconds\n",
      "Epoch 1. Loss 1.2164. accuracy 0.6510. Elapsed 12422 seconds\n",
      "Epoch 1. Loss 1.2150. accuracy 0.6536. Elapsed 12453 seconds\n",
      "Epoch 1. Loss 1.2136. accuracy 0.6530. Elapsed 12484 seconds\n",
      "Epoch 1. Loss 1.2123. accuracy 0.6514. Elapsed 12515 seconds\n",
      "Epoch 1. Loss 1.2110. accuracy 0.6484. Elapsed 12546 seconds\n",
      "Epoch 1. Loss 1.2095. accuracy 0.6547. Elapsed 12576 seconds\n",
      "Epoch 1. Loss 1.2083. accuracy 0.6536. Elapsed 12607 seconds\n",
      "Epoch 1. Loss 1.2069. accuracy 0.6543. Elapsed 12638 seconds\n",
      "Epoch 1. Loss 1.2055. accuracy 0.6556. Elapsed 12669 seconds\n",
      "Epoch 1. Loss 1.2042. accuracy 0.6508. Elapsed 12700 seconds\n",
      "Epoch 1. Loss 1.2028. accuracy 0.6473. Elapsed 12731 seconds\n",
      "Epoch 1. Loss 1.2015. accuracy 0.6499. Elapsed 12761 seconds\n",
      "Epoch 1. Loss 1.2002. accuracy 0.6495. Elapsed 12792 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Loss 1.1989. accuracy 0.6488. Elapsed 12823 seconds\n",
      "Epoch 1. Loss 1.1977. accuracy 0.6418. Elapsed 12853 seconds\n",
      "Epoch 1. Loss 1.1963. accuracy 0.6433. Elapsed 12884 seconds\n",
      "Epoch 1. Loss 1.1950. accuracy 0.6380. Elapsed 12915 seconds\n",
      "Epoch 1. Loss 1.1937. accuracy 0.6380. Elapsed 12946 seconds\n",
      "Epoch 1. Loss 1.1925. accuracy 0.6367. Elapsed 12977 seconds\n",
      "Epoch 1. Loss 1.1912. accuracy 0.6402. Elapsed 13008 seconds\n",
      "Epoch 1. Loss 1.1900. accuracy 0.6446. Elapsed 13039 seconds\n",
      "Epoch 1. Loss 1.1888. accuracy 0.6426. Elapsed 13069 seconds\n",
      "Epoch 1. Loss 1.1876. accuracy 0.6440. Elapsed 13099 seconds\n",
      "Epoch 1. Loss 1.1864. accuracy 0.6457. Elapsed 13128 seconds\n",
      "Epoch 1. Loss 1.1851. accuracy 0.6488. Elapsed 13158 seconds\n",
      "Epoch 1. Loss 1.1839. accuracy 0.6538. Elapsed 13188 seconds\n",
      "Epoch 1. Loss 1.1827. accuracy 0.6567. Elapsed 13218 seconds\n",
      "Epoch 1. Loss 1.1816. accuracy 0.6565. Elapsed 13247 seconds\n",
      "Epoch 1. Loss 1.1804. accuracy 0.6563. Elapsed 13277 seconds\n",
      "Epoch 1. Loss 1.1792. accuracy 0.6554. Elapsed 13307 seconds\n",
      "Epoch 1. Loss 1.1779. accuracy 0.6567. Elapsed 13336 seconds\n",
      "Epoch 1. Loss 1.1767. accuracy 0.6585. Elapsed 13366 seconds\n",
      "Epoch 1. Loss 1.1755. accuracy 0.6578. Elapsed 13395 seconds\n",
      "Epoch 1. Loss 1.1744. accuracy 0.6596. Elapsed 13425 seconds\n",
      "Epoch 1. Loss 1.1732. accuracy 0.6536. Elapsed 13455 seconds\n",
      "Epoch 1. Loss 1.1721. accuracy 0.6543. Elapsed 13484 seconds\n",
      "Epoch 1. Loss 1.1709. accuracy 0.6514. Elapsed 13514 seconds\n",
      "Epoch 1. Loss 1.1698. accuracy 0.6497. Elapsed 13543 seconds\n",
      "Epoch 1. Loss 1.1686. accuracy 0.6479. Elapsed 13573 seconds\n",
      "Epoch 1. Loss 1.1675. accuracy 0.6486. Elapsed 13603 seconds\n",
      "Epoch 1. Loss 1.1663. accuracy 0.6475. Elapsed 13632 seconds\n",
      "Epoch 1. Loss 1.1651. accuracy 0.6431. Elapsed 13662 seconds\n",
      "Epoch 1. Loss 1.1640. accuracy 0.6448. Elapsed 13691 seconds\n",
      "Epoch 1. Loss 1.1629. accuracy 0.6488. Elapsed 13721 seconds\n",
      "Epoch 1. Loss 1.1617. accuracy 0.6486. Elapsed 13751 seconds\n",
      "Epoch 1. Loss 1.1606. accuracy 0.6510. Elapsed 13781 seconds\n",
      "Epoch 1. Loss 1.1595. accuracy 0.6530. Elapsed 13812 seconds\n",
      "Epoch 1. Loss 1.1584. accuracy 0.6552. Elapsed 13843 seconds\n",
      "Epoch 1. Loss 1.1573. accuracy 0.6558. Elapsed 13873 seconds\n",
      "Epoch 1. Loss 1.1562. accuracy 0.6589. Elapsed 13904 seconds\n",
      "Epoch 1. Loss 1.1552. accuracy 0.6593. Elapsed 13933 seconds\n",
      "Epoch 1. Loss 1.1541. accuracy 0.6587. Elapsed 13964 seconds\n",
      "Epoch 1. Loss 1.1531. accuracy 0.6560. Elapsed 13993 seconds\n",
      "Epoch 1. Loss 1.1520. accuracy 0.6565. Elapsed 14025 seconds\n",
      "Epoch 1. Loss 1.1510. accuracy 0.6547. Elapsed 14057 seconds\n",
      "Epoch 1. Loss 1.1499. accuracy 0.6565. Elapsed 14087 seconds\n",
      "Epoch 1. Loss 1.1489. accuracy 0.6549. Elapsed 14117 seconds\n",
      "Epoch 1. Loss 1.1478. accuracy 0.6538. Elapsed 14148 seconds\n",
      "Epoch 1. Loss 1.1467. accuracy 0.6514. Elapsed 14179 seconds\n",
      "Epoch 1. Loss 1.1457. accuracy 0.6534. Elapsed 14210 seconds\n",
      "Epoch 1. Loss 1.1446. accuracy 0.6549. Elapsed 14240 seconds\n",
      "Epoch 1. Loss 1.1436. accuracy 0.6609. Elapsed 14271 seconds\n",
      "Epoch 1. Loss 1.1426. accuracy 0.6651. Elapsed 14302 seconds\n",
      "Epoch 1. Loss 1.1415. accuracy 0.6613. Elapsed 14333 seconds\n",
      "Epoch 1. Loss 1.1405. accuracy 0.6569. Elapsed 14364 seconds\n",
      "Epoch 1. Loss 1.1395. accuracy 0.6497. Elapsed 14395 seconds\n",
      "Epoch 1. Loss 1.1385. accuracy 0.6492. Elapsed 14426 seconds\n",
      "Epoch 1. Loss 1.1374. accuracy 0.6492. Elapsed 14458 seconds\n",
      "Epoch 1. Loss 1.1368. accuracy 0.6420. Elapsed 14488 seconds\n",
      "Epoch 1. Loss 1.1357. accuracy 0.6411. Elapsed 14519 seconds\n",
      "Epoch 1. Loss 1.1348. accuracy 0.6440. Elapsed 14549 seconds\n",
      "Epoch 1. Loss 1.1338. accuracy 0.6455. Elapsed 14580 seconds\n",
      "Epoch 1. Loss 1.1328. accuracy 0.6468. Elapsed 14610 seconds\n",
      "Epoch 1. Loss 1.1319. accuracy 0.6510. Elapsed 14641 seconds\n",
      "Epoch 1. Loss 1.1309. accuracy 0.6541. Elapsed 14671 seconds\n",
      "Epoch 1. Loss 1.1300. accuracy 0.6532. Elapsed 14702 seconds\n",
      "Epoch 1. Loss 1.1291. accuracy 0.6527. Elapsed 14733 seconds\n",
      "Epoch 1. Loss 1.1281. accuracy 0.6490. Elapsed 14763 seconds\n",
      "Epoch 1. Loss 1.1272. accuracy 0.6466. Elapsed 14794 seconds\n",
      "Epoch 1. Loss 1.1262. accuracy 0.6440. Elapsed 14824 seconds\n",
      "Epoch 1. Loss 1.1253. accuracy 0.6407. Elapsed 14855 seconds\n",
      "Epoch 1. Loss 1.1244. accuracy 0.6409. Elapsed 14885 seconds\n",
      "Epoch 1. Loss 1.1235. accuracy 0.6424. Elapsed 14914 seconds\n",
      "Epoch 1. Loss 1.1226. accuracy 0.6451. Elapsed 14944 seconds\n",
      "Epoch 1. Loss 1.1216. accuracy 0.6488. Elapsed 14974 seconds\n",
      "Epoch 1. Loss 1.1207. accuracy 0.6541. Elapsed 15004 seconds\n",
      "Epoch 1. Loss 1.1197. accuracy 0.6582. Elapsed 15033 seconds\n",
      "Epoch 1. Loss 1.1188. accuracy 0.6622. Elapsed 15063 seconds\n",
      "Epoch 1. Loss 1.1178. accuracy 0.6655. Elapsed 15093 seconds\n",
      "Epoch 1. Loss 1.1169. accuracy 0.6688. Elapsed 15122 seconds\n",
      "Epoch 1. Loss 1.1160. accuracy 0.6686. Elapsed 15152 seconds\n",
      "Epoch 1. Loss 1.1150. accuracy 0.6701. Elapsed 15181 seconds\n",
      "Epoch 1. Loss 1.1141. accuracy 0.6684. Elapsed 15211 seconds\n",
      "Epoch 1. Loss 1.1132. accuracy 0.6686. Elapsed 15241 seconds\n",
      "Epoch 1. Loss 1.1122. accuracy 0.6703. Elapsed 15271 seconds\n",
      "Epoch 1. Loss 1.1113. accuracy 0.6745. Elapsed 15301 seconds\n",
      "Epoch 1. Loss 1.1103. accuracy 0.6809. Elapsed 15331 seconds\n",
      "Epoch 1. Loss 1.1094. accuracy 0.6767. Elapsed 15364 seconds\n",
      "Epoch 1. Loss 1.1085. accuracy 0.6765. Elapsed 15394 seconds\n",
      "Epoch 1. Loss 1.1076. accuracy 0.6743. Elapsed 15424 seconds\n",
      "Epoch 1. Loss 1.1068. accuracy 0.6736. Elapsed 15455 seconds\n",
      "Epoch 1. Loss 1.1059. accuracy 0.6756. Elapsed 15487 seconds\n",
      "Epoch 1. Loss 1.1050. accuracy 0.6774. Elapsed 15519 seconds\n",
      "Epoch 1. Loss 1.1041. accuracy 0.6769. Elapsed 15551 seconds\n",
      "Epoch 1. Loss 1.1033. accuracy 0.6752. Elapsed 15581 seconds\n",
      "Epoch 1. Loss 1.1024. accuracy 0.6760. Elapsed 15612 seconds\n",
      "Epoch 1. Loss 1.1015. accuracy 0.6796. Elapsed 15644 seconds\n",
      "Epoch 1. Loss 1.1006. accuracy 0.6859. Elapsed 15674 seconds\n",
      "Epoch 1. Loss 1.0997. accuracy 0.6912. Elapsed 15704 seconds\n",
      "Epoch 1. Loss 1.0988. accuracy 0.6895. Elapsed 15738 seconds\n",
      "Epoch 1. Loss 1.0979. accuracy 0.6965. Elapsed 15771 seconds\n",
      "Epoch 1. Loss 1.0971. accuracy 0.6873. Elapsed 15803 seconds\n",
      "Epoch 1. Loss 1.0961. accuracy 0.6813. Elapsed 15833 seconds\n",
      "Epoch 1. Loss 1.0953. accuracy 0.6785. Elapsed 15863 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-9b0fe448366d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mepoch_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mepoch_accs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-601a453a885e>\u001b[0m in \u001b[0;36mcompute_accuracy\u001b[0;34m(data_set, model)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mnum_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m             \u001b[0mnum_right\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnum_right\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-f35eb2f28757>\u001b[0m in \u001b[0;36mscore\u001b[0;34m(pair, model)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mending1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mending1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mending2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mending2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mres1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mending1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mres2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mending2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0msoftm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/school/nlp/env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-824342a9939f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhd1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhd2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhd3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/school/nlp/env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/school/nlp/env/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/school/nlp/env/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1354\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tick = time.time()\n",
    "name='LS_new_4_26_train_ep'\n",
    "epoch_losses = []\n",
    "epoch_accs = []\n",
    "best_score= 0.5\n",
    "for epoch_num in range(1, num_epochs + 1):\n",
    "    batch_losses = []\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        #print(batch)\n",
    "        ### YOUR CODE BELOW ###\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Extract the inputs and the targets\n",
    "        inputs, targets = batch\n",
    "        # Transfer the inputs and the targets to GPUs, if available\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "        # Run the model\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs,targets)\n",
    "        \n",
    "        \n",
    "        # Backpropagate the error\n",
    "        loss.backward()\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Append the loss\n",
    "        batch_losses.append(float (loss))\n",
    "        ### YOUR CODE ABOVE ###\n",
    "        epoch_loss = np.mean(np.array(batch_losses))\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        \n",
    "        acc = compute_accuracy(val_data_set,model)\n",
    "        epoch_accs.append(acc)\n",
    "        \n",
    "\n",
    "        if epoch_num % report_every == 0:\n",
    "            tock = time.time()\n",
    "            print(\"Epoch {}. Loss {:.4f}. accuracy {:.4f}. Elapsed {:.0f} seconds\".format(epoch_num, epoch_loss,acc, tock-tick))\n",
    "    epoch_acc = np.mean(np.array(epoch_accs))\n",
    "    print('ep acc',epoch_acc)\n",
    "    if epoch_acc > best_score:\n",
    "            best_score = epoch_acc\n",
    "            print('new best saving model')\n",
    "            save('{}{}_best_acc_{:.2f}'.format(name,epoch_num,epoch_acc))\n",
    "     \n",
    "    #print('saving model')\n",
    "    #save(name + str(epoch_num))\n",
    "\n",
    "print(\"Total time elapsed: {:.0f} minutes\".format((tock-tick)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len is 5226\n",
      "Warning: 21/930911 words are not in dictionary, thus set UNK\n",
      "Warning: 21/930911 words are not in dictionary, thus set UNK\n",
      "skip thought encoding dataset\n",
      "[============================================================] 99.9% ...\r"
     ]
    }
   ],
   "source": [
    "\n",
    "#train_data_set = None\n",
    "test_data_set = LastSentenceDataset(file='story_cloze_data/cloze_test_test__spring2016 - cloze_test_ALL_test.csv',vocab=voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(name):\n",
    "    l_model = LSModel()\n",
    "    l_model.load_state_dict(torch.load('saved_models/{}.save'.format(name)))\n",
    "    return l_model\n",
    "model = load('LS_new_4_25_voc_fix_2_ep7_best_acc_0.70')\n",
    "#print('model',model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_accuracy(train_data_set,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5344735435595938"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy(test_data_set,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
